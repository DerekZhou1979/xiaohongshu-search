#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å¼ºåˆ¶æœç´¢é¡µé¢ä¿æŒåŠŸèƒ½
éªŒè¯åçˆ¬è™«å¤„ç†åæ˜¯å¦èƒ½æ­£ç¡®ä¿æŒåœ¨æœç´¢é¡µé¢
"""

import sys
import os
import time
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from crawler.XHS_crawler import XiaoHongShuCrawler

def test_force_search_page():
    """æµ‹è¯•å¼ºåˆ¶æœç´¢é¡µé¢ä¿æŒåŠŸèƒ½"""
    
    print("=" * 80)
    print("ğŸ§ª æµ‹è¯•å¼ºåˆ¶æœç´¢é¡µé¢ä¿æŒåŠŸèƒ½")
    print("=" * 80)
    
    # æµ‹è¯•å…³é”®è¯åˆ—è¡¨
    test_keywords = [
        "ç¾é£Ÿæ¨è",
        "æ—…è¡Œæ”»ç•¥", 
        "æŠ¤è‚¤å¿ƒå¾—",
        "å‡è‚¥æ–¹æ³•",
        "æ‘„å½±æŠ€å·§"
    ]
    
    # åˆå§‹åŒ–çˆ¬è™«ï¼ˆä½¿ç”¨å¯è§æµè§ˆå™¨ä»¥ä¾¿è§‚å¯Ÿï¼‰
    crawler = XiaoHongShuCrawler(
        headless=False,  # ä½¿ç”¨å¯è§æµè§ˆå™¨
        cookies_file='cache/cookies/xiaohongshu_cookies.json'
    )
    
    results_summary = []
    
    try:
        for i, keyword in enumerate(test_keywords):
            print(f"\n{'='*60}")
            print(f"ğŸ” æµ‹è¯• {i+1}/{len(test_keywords)}: å…³é”®è¯ '{keyword}'")
            print(f"{'='*60}")
            
            test_start_time = datetime.now()
            
            # æ‰§è¡Œæœç´¢
            results = crawler.search(keyword, max_results=5, use_cache=False)
            
            test_end_time = datetime.now()
            test_duration = (test_end_time - test_start_time).total_seconds()
            
            # æ£€æŸ¥ç»“æœ
            if results and len(results) > 0:
                print(f"âœ… æµ‹è¯•æˆåŠŸï¼æ‰¾åˆ° {len(results)} æ¡ç›¸å…³ç»“æœ")
                print(f"â±ï¸  è€—æ—¶: {test_duration:.2f} ç§’")
                
                # æ˜¾ç¤ºå‰3ä¸ªç»“æœçš„æ ‡é¢˜
                print("ğŸ“ ç»“æœé¢„è§ˆ:")
                for j, result in enumerate(results[:3]):
                    title = result.get('title', 'æ— æ ‡é¢˜')[:50]
                    print(f"   {j+1}. {title}...")
                
                results_summary.append({
                    'keyword': keyword,
                    'status': 'SUCCESS',
                    'count': len(results),
                    'duration': test_duration
                })
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥ï¼æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                print(f"â±ï¸  è€—æ—¶: {test_duration:.2f} ç§’")
                
                results_summary.append({
                    'keyword': keyword,
                    'status': 'FAILED',
                    'count': 0,
                    'duration': test_duration
                })
            
            # æ£€æŸ¥å½“å‰URLçŠ¶æ€
            try:
                current_url = crawler.driver.current_url
                if 'search_result' in current_url or 'keyword=' in current_url:
                    print(f"âœ… URLçŠ¶æ€: æ­£ç¡®çš„æœç´¢é¡µé¢")
                    print(f"ğŸ“ å½“å‰URL: {current_url[:70]}...")
                else:
                    print(f"âš ï¸  URLçŠ¶æ€: å¯èƒ½ä¸åœ¨æœç´¢é¡µé¢")
                    print(f"ğŸ“ å½“å‰URL: {current_url[:70]}...")
            except Exception as e:
                print(f"âŒ æ— æ³•æ£€æŸ¥URLçŠ¶æ€: {str(e)}")
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•
            if i < len(test_keywords) - 1:
                print(f"â³ ç­‰å¾…5ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
                time.sleep(5)
    
    except KeyboardInterrupt:
        print("\nğŸ›‘ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
    finally:
        # å…³é—­æµè§ˆå™¨
        try:
            crawler.close()
            print("\nğŸ”’ æµè§ˆå™¨å·²å…³é—­")
        except:
            pass
    
    # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
    print(f"\n{'='*80}")
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print(f"{'='*80}")
    
    total_tests = len(results_summary)
    successful_tests = len([r for r in results_summary if r['status'] == 'SUCCESS'])
    failed_tests = total_tests - successful_tests
    
    print(f"ğŸ“ˆ æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"âœ… æˆåŠŸ: {successful_tests}")
    print(f"âŒ å¤±è´¥: {failed_tests}")
    print(f"ğŸ“Š æˆåŠŸç‡: {(successful_tests/total_tests*100):.1f}%")
    
    if successful_tests > 0:
        avg_duration = sum([r['duration'] for r in results_summary if r['status'] == 'SUCCESS']) / successful_tests
        avg_results = sum([r['count'] for r in results_summary if r['status'] == 'SUCCESS']) / successful_tests
        print(f"â±ï¸  å¹³å‡è€—æ—¶: {avg_duration:.2f} ç§’")
        print(f"ğŸ“ å¹³å‡ç»“æœæ•°: {avg_results:.1f} æ¡")
    
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for result in results_summary:
        status_icon = "âœ…" if result['status'] == 'SUCCESS' else "âŒ"
        print(f"   {status_icon} {result['keyword']}: {result['count']}æ¡ç»“æœ, {result['duration']:.2f}s")
    
    print(f"\nğŸ¯ æµ‹è¯•å®Œæˆï¼")
    
    # æ ¹æ®ç»“æœè¿”å›é€€å‡ºç 
    if successful_tests == total_tests:
        print("ğŸŒŸ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼å¼ºåˆ¶æœç´¢é¡µé¢ä¿æŒåŠŸèƒ½å·¥ä½œæ­£å¸¸ã€‚")
        return 0
    elif successful_tests > 0:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•é€šè¿‡ã€‚å¼ºåˆ¶æœç´¢é¡µé¢ä¿æŒåŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
        return 1
    else:
        print("ğŸš¨ æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥äº†ã€‚å¼ºåˆ¶æœç´¢é¡µé¢ä¿æŒåŠŸèƒ½éœ€è¦ä¿®å¤ã€‚")
        return 2

def test_redirect_detection():
    """æµ‹è¯•é‡å®šå‘æ£€æµ‹åŠŸèƒ½"""
    
    print("\n" + "="*80)
    print("ğŸ§ª æµ‹è¯•é‡å®šå‘æ£€æµ‹åŠŸèƒ½")
    print("="*80)
    
    crawler = XiaoHongShuCrawler(headless=False, cookies_file='cache/cookies.json')
    
    try:
        # æµ‹è¯•é‡å®šå‘æ£€æµ‹
        original_search_url = "https://www.xiaohongshu.com/search_result?keyword=%E7%BE%8E%E9%A3%9F&source=web_search&type=comprehensive"
        non_search_url = "https://www.xiaohongshu.com"
        
        # æµ‹è¯•ä»æœç´¢é¡µé¢åˆ°éæœç´¢é¡µé¢çš„é‡å®šå‘æ£€æµ‹
        redirected = crawler._check_redirected_from_search(original_search_url, non_search_url)
        
        if redirected:
            print("âœ… é‡å®šå‘æ£€æµ‹åŠŸèƒ½æ­£å¸¸ï¼šæ­£ç¡®è¯†åˆ«äº†ä»æœç´¢é¡µé¢åˆ°é¦–é¡µçš„é‡å®šå‘")
        else:
            print("âŒ é‡å®šå‘æ£€æµ‹åŠŸèƒ½å¼‚å¸¸ï¼šæœªèƒ½è¯†åˆ«é‡å®šå‘")
        
        # æµ‹è¯•ä»æœç´¢é¡µé¢åˆ°æœç´¢é¡µé¢çš„æƒ…å†µï¼ˆä¸åº”è¢«åˆ¤å®šä¸ºé‡å®šå‘ï¼‰
        same_search_url = "https://www.xiaohongshu.com/search_result?keyword=%E6%97%85%E8%A1%8C&source=web_search"
        not_redirected = crawler._check_redirected_from_search(original_search_url, same_search_url)
        
        if not not_redirected:
            print("âœ… é‡å®šå‘æ£€æµ‹åŠŸèƒ½æ­£å¸¸ï¼šæ­£ç¡®è¯†åˆ«äº†æœç´¢é¡µé¢é—´çš„è·³è½¬ä¸æ˜¯é‡å®šå‘")
        else:
            print("âŒ é‡å®šå‘æ£€æµ‹åŠŸèƒ½å¼‚å¸¸ï¼šé”™è¯¯åœ°å°†æœç´¢é¡µé¢é—´è·³è½¬åˆ¤å®šä¸ºé‡å®šå‘")
            
    except Exception as e:
        print(f"âŒ é‡å®šå‘æ£€æµ‹æµ‹è¯•å¤±è´¥: {str(e)}")
    finally:
        try:
            crawler.close()
        except:
            pass

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨å¼ºåˆ¶æœç´¢é¡µé¢ä¿æŒåŠŸèƒ½æµ‹è¯•")
    print("ğŸ“ è¿™ä¸ªæµ‹è¯•å°†éªŒè¯åçˆ¬è™«å¤„ç†åæ˜¯å¦èƒ½æ­£ç¡®ä¿æŒåœ¨æœç´¢é¡µé¢")
    print("âš ï¸  è¯·ç¡®ä¿å·²ç»è¿è¡Œäº† simple_refresh_cookies.py åˆ·æ–°cookies")
    
    input("\næŒ‰Enteré”®å¼€å§‹æµ‹è¯•...")
    
    # è¿è¡Œä¸»è¦æµ‹è¯•
    exit_code = test_force_search_page()
    
    # è¿è¡Œé‡å®šå‘æ£€æµ‹æµ‹è¯•
    test_redirect_detection()
    
    sys.exit(exit_code) 