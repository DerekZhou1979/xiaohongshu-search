#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å°çº¢ä¹¦çˆ¬è™«æ¨¡å—
ä½¿ç”¨å¤šç§ç­–ç•¥æå–å°çº¢ä¹¦ç¬”è®°æ•°æ®
æ³¨æ„ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ ç ”ç©¶ä½¿ç”¨ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€éµå®ˆå°çº¢ä¹¦çš„ä½¿ç”¨æ¡æ¬¾å’Œç›¸å…³æ³•å¾‹æ³•è§„
"""

import json
import random
import time
import logging
import hashlib
import os
import sys
import urllib.parse
from urllib.parse import quote
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

# å¯¼å…¥é…ç½®ä¿¡æ¯
try:
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    from app import SEARCH_CONFIG, CRAWLER_CONFIG, DIRECTORIES, FILE_PATHS, URLS, HOT_KEYWORDS, get_crawl_config
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    SEARCH_CONFIG = {'DEFAULT_MAX_RESULTS': 30, 'MAX_RESULTS_LIMIT': 100, 'USE_CACHE': True, 'CACHE_EXPIRE_TIME': 3600}
    CRAWLER_CONFIG = {'USE_SELENIUM': True, 'HEADLESS': True, 'WINDOW_SIZE': (1920, 1080), 'CHROME_OPTIONS': ['--headless']}
    DIRECTORIES = {'CACHE_DIR': os.path.join(PROJECT_ROOT, 'cache'), 'TEMP_DIR': os.path.join(PROJECT_ROOT, 'cache', 'temp')}
    FILE_PATHS = {'CHROMEDRIVER_PATH': os.path.join(PROJECT_ROOT, 'drivers', 'chromedriver-mac-arm64', 'chromedriver'), 'COOKIES_FILE': os.path.join(PROJECT_ROOT, 'cache', 'cookies', 'xiaohongshu_cookies.json')}
    URLS = {'XIAOHONGSHU_BASE': 'https://www.xiaohongshu.com'}
    HOT_KEYWORDS = ["æµ·é¸¥æ‰‹è¡¨", "ç¾é£Ÿ", "æŠ¤è‚¤"]
    
    def get_crawl_config():
        import os
        import json
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        config_str = os.environ.get('CRAWL_CONFIG')
        if config_str:
            try:
                return json.loads(config_str)
            except:
                pass
        
        return {
            'enable_debug_screenshots': False,
            'enable_strategy_1': True,
            'enable_strategy_2': True,
            'enable_strategy_3': True,
            'validation_strict_level': 'medium',
            'enable_detailed_logs': True,
            'screenshot_interval': 0,
        }

# å¯¼å…¥Seleniumç›¸å…³åº“
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class XiaoHongShuCrawler:
    """å°çº¢ä¹¦çˆ¬è™«ç±» - ä½¿ç”¨å…¨å±€é…ç½®å’Œä¸‰ç§æå–ç­–ç•¥"""
    
    def __init__(self, use_selenium=True, headless=None, proxy=None, cookies_file=None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        å‚æ•°:
            use_selenium (bool): æ˜¯å¦ä½¿ç”¨Selenium
            headless (bool): æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®
            proxy (str): ä»£ç†æœåŠ¡å™¨åœ°å€
            cookies_file (str): cookieæ–‡ä»¶è·¯å¾„
        """
        # ä½¿ç”¨é…ç½®
        self.search_config = SEARCH_CONFIG
        self.crawler_config = CRAWLER_CONFIG
        self.crawl_config = get_crawl_config()  # æ–°å¢çˆ¬è™«é…ç½®
        
        # åˆå§‹åŒ–å‚æ•°
        self.use_selenium = use_selenium if use_selenium is not None else self.crawler_config['USE_SELENIUM']
        # ä¸ºäº†æ”¯æŒäººå·¥éªŒè¯ï¼Œä¼˜å…ˆä½¿ç”¨å¯è§æ¨¡å¼
        self.headless = headless if headless is not None else False  # æ”¹ä¸ºé»˜è®¤éæ— å¤´æ¨¡å¼
        self.original_headless = headless if headless is not None else self.crawler_config['HEADLESS']
        self.proxy = proxy
        self.cookies_file = cookies_file or FILE_PATHS['COOKIES_FILE']
        
        # WebDriverç›¸å…³
        self.driver = None
        
        # ç¼“å­˜é…ç½®
        self.cache_dir = DIRECTORIES['TEMP_DIR']
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # å›è°ƒå‡½æ•°
        self.html_callback = None  # HTMLç»“æœå›è°ƒå‡½æ•°
        
        # éªŒè¯å¤„ç†çŠ¶æ€
        self.verification_in_progress = False
        self.verification_completed = False
        
        # åŠ è½½cookie
        self.cookies = self._load_cookies()
        
        logger.info("å°çº¢ä¹¦çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼ˆæ”¯æŒäººå·¥éªŒè¯æ¨¡å¼ï¼‰")
    
    def set_html_callback(self, callback_func):
        """è®¾ç½®HTMLå­˜å‚¨å›è°ƒå‡½æ•°"""
        self.html_callback = callback_func
        logger.info("HTMLå­˜å‚¨å›è°ƒå‡½æ•°å·²è®¾ç½®")
    
    def set_debug_callback(self, callback_func):
        """è®¾ç½®Debugä¿¡æ¯å›è°ƒå‡½æ•°"""
        self.debug_callback = callback_func
        logger.info("Debugä¿¡æ¯å›è°ƒå‡½æ•°å·²è®¾ç½®")
    
    def _debug_log(self, message, level="INFO"):
        """å‘é€debugä¿¡æ¯åˆ°å›è°ƒå‡½æ•°å’Œæ—¥å¿—"""
        # å‘é€åˆ°å›è°ƒå‡½æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if hasattr(self, 'debug_callback') and self.debug_callback:
            try:
                self.debug_callback(message, level)
            except Exception as e:
                logger.error(f"è°ƒç”¨debugå›è°ƒå‡½æ•°å¤±è´¥: {str(e)}")
        
        # åŒæ—¶å†™å…¥æ—¥å¿—
        if level == "ERROR":
            logger.error(message)
        elif level == "WARNING":
            logger.warning(message)
        else:
            logger.info(message)
    
    def _ensure_driver_initialized(self):
        """ç¡®ä¿WebDriverå·²åˆå§‹åŒ–"""
        if self.driver is None:
            return self._init_selenium()
        return True
    
    def _load_cookies(self):
        """åŠ è½½cookie"""
        if not os.path.exists(self.cookies_file):
            logger.warning(f"Cookieæ–‡ä»¶ä¸å­˜åœ¨: {self.cookies_file}")
            return []
        
        try:
            with open(self.cookies_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            logger.info(f"æˆåŠŸåŠ è½½Cookieæ–‡ä»¶: {self.cookies_file}, åŒ…å« {len(cookies)} ä¸ªcookie")
            return cookies
        except Exception as e:
            logger.error(f"åŠ è½½Cookieæ–‡ä»¶å¤±è´¥: {str(e)}")
            return []
    
    def save_cookies(self, cookies_file=None):
        """ä¿å­˜å½“å‰æµè§ˆå™¨çš„cookie"""
        if not self._ensure_driver_initialized():
            logger.error("WebDriveråˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ä¿å­˜cookie")
            return False
        
        file_path = cookies_file or self.cookies_file
        cookies_dir = os.path.dirname(file_path)
        os.makedirs(cookies_dir, exist_ok=True)
        
        try:
            cookies = self.driver.get_cookies()
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, ensure_ascii=False, indent=2)
            logger.info(f"æˆåŠŸä¿å­˜ {len(cookies)} ä¸ªcookieåˆ°: {file_path}")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜cookieå¤±è´¥: {str(e)}")
            return False
    
    def _init_selenium(self):
        """åˆå§‹åŒ–Selenium WebDriver"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–Selenium...")
            
            # é…ç½®Chromeé€‰é¡¹
            chrome_options = Options()
            
            # æ·»åŠ é…ç½®æ–‡ä»¶ä¸­çš„Chromeé€‰é¡¹ï¼Œä½†è·³è¿‡æ— å¤´æ¨¡å¼ç›¸å…³
            for option in self.crawler_config['CHROME_OPTIONS']:
                if '--headless' not in option:  # è·³è¿‡æ— å¤´æ¨¡å¼ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ”¯æŒäººå·¥éªŒè¯
                    chrome_options.add_argument(option)
            
            # æ ¹æ®å®é™…éœ€è¦å†³å®šæ˜¯å¦å¯ç”¨æ— å¤´æ¨¡å¼
            if self.headless:
                chrome_options.add_argument('--headless')
            else:
                logger.info("ğŸ–¥ï¸  æµè§ˆå™¨å¯åŠ¨ä¸ºå¯è§æ¨¡å¼ï¼ˆæ”¯æŒäººå·¥éªŒè¯ï¼‰")
            
            # è®¾ç½®çª—å£å¤§å°
            width, height = self.crawler_config['WINDOW_SIZE']
            chrome_options.add_argument(f'--window-size={width},{height}')
            
            # è®¾ç½®ä»£ç†
            if self.proxy:
                chrome_options.add_argument(f'--proxy-server={self.proxy}')
            
            # åçˆ¬è™«é…ç½®
            chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ä½¿ç”¨æœ¬åœ°ChromeDriver
            chromedriver_path = FILE_PATHS['CHROMEDRIVER_PATH']
            if os.path.exists(chromedriver_path):
                logger.info(f"ä½¿ç”¨æœ¬åœ°ChromeDriver: {chromedriver_path}")
                os.chmod(chromedriver_path, 0o755)
                service = Service(chromedriver_path)
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            else:
                logger.error(f"ChromeDriverä¸å­˜åœ¨: {chromedriver_path}")
                return False
            
            # éšè—WebDriverç‰¹å¾
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            logger.info("Chromeæµè§ˆå™¨å·²æˆåŠŸå¯åŠ¨")
            
            # æ·»åŠ cookie
            if self.cookies:
                self._add_cookies()
            
            logger.info("Seleniumåˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"Seleniumåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return False
    
    def _add_cookies(self):
        """æ·»åŠ cookieåˆ°æµè§ˆå™¨"""
        try:
            logger.info("å°è¯•æ·»åŠ cookie...")
            self.driver.get("https://www.xiaohongshu.com")
            time.sleep(3)
            
            for cookie in self.cookies:
                try:
                    # ç§»é™¤å¯èƒ½å¯¼è‡´é—®é¢˜çš„å­—æ®µ
                    cookie_clean = {k: v for k, v in cookie.items() 
                                  if k in ['name', 'value', 'domain', 'path', 'secure']}
                    self.driver.add_cookie(cookie_clean)
                except Exception as e:
                    logger.warning(f"æ·»åŠ cookieå¤±è´¥: {cookie.get('name', 'æœªçŸ¥')} - {str(e)}")
            
            # åˆ·æ–°é¡µé¢ä½¿cookieç”Ÿæ•ˆ
            self.driver.refresh()
            time.sleep(5)
            logger.info("å·²æ·»åŠ cookieå¹¶åˆ·æ–°é¡µé¢")
            
        except Exception as e:
            logger.error(f"æ·»åŠ cookieè¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    def _get_cache_path(self, keyword):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_filename = f"search_{hashlib.md5(keyword.encode()).hexdigest()}.json"
        return os.path.join(self.cache_dir, cache_filename)
    
    def _save_to_cache(self, keyword, data):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆï¼Œåªæœ‰éç©ºæ•°æ®æ‰ç¼“å­˜
            if not data or len(data) == 0:
                self._debug_log("âš ï¸ æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ç¼“å­˜ä¿å­˜", "WARNING")
                return
            
            cache_path = self._get_cache_path(keyword)
            cache_data = {
                'timestamp': time.time(),
                'keyword': keyword,
                'data': data
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            logger.info(f"æ•°æ®å·²ç¼“å­˜: {cache_path}")
            
            # ğŸ”§ ä¿®å¤ï¼šåªæœ‰æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰ç”ŸæˆHTMLé¡µé¢
            self._debug_log("ğŸ“„ ç”ŸæˆHTMLç»“æœé¡µé¢...")
            self._generate_result_html(keyword, data)
            
        except Exception as e:
            logger.error(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")
    
    def _generate_result_html(self, keyword, data):
        """ç”ŸæˆHTMLç»“æœé¡µé¢"""
        try:
            # åˆ›å»ºresultsç›®å½•
            results_dir = os.path.join(DIRECTORIES['CACHE_DIR'], 'results')
            os.makedirs(results_dir, exist_ok=True)
            
            # ç”ŸæˆHTMLæ–‡ä»¶å
            html_filename = f"search_{hashlib.md5(keyword.encode()).hexdigest()}.html"
            html_path = os.path.join(results_dir, html_filename)
            
            # ç”ŸæˆHTMLå†…å®¹
            html_content = self._create_html_template(keyword, data)
            
            # ä¿å­˜HTMLæ–‡ä»¶
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"HTMLç»“æœé¡µé¢å·²ç”Ÿæˆ: {html_path}")
            
            # å¦‚æœè®¾ç½®äº†å›è°ƒå‡½æ•°ï¼Œå°†HTMLå†…å®¹ä¼ é€’ç»™æœåŠ¡å™¨
            if self.html_callback:
                html_hash = hashlib.md5(keyword.encode()).hexdigest()
                self.html_callback(html_hash, html_content)
                logger.info(f"HTMLå†…å®¹å·²é€šè¿‡å›è°ƒå‡½æ•°ä¼ é€’: {html_hash}")
            
        except Exception as e:
            logger.error(f"ç”ŸæˆHTMLç»“æœé¡µé¢å¤±è´¥: {str(e)}")
    
    def _build_enhanced_url(self, original_url, xsec_token):
        """æ„å»ºåŒ…å«xsec_tokenå’Œxsec_sourceçš„å¢å¼ºURL"""
        try:
            from urllib.parse import urlparse, parse_qs, urlencode, urlunparse
            
            # è§£æåŸå§‹URL
            parsed = urlparse(original_url)
            query_params = parse_qs(parsed.query)
            
            # æ·»åŠ xsec_sourceå‚æ•°
            query_params['xsec_source'] = ['pc_feed']
            
            # å¦‚æœæœ‰xsec_tokenï¼Œæ·»åŠ æˆ–æ›´æ–°
            if xsec_token:
                query_params['xsec_token'] = [xsec_token]
            
            # é‡æ–°æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
            new_query = urlencode(query_params, doseq=True)
            
            # æ„å»ºæ–°çš„URL
            enhanced_url = urlunparse((
                parsed.scheme,
                parsed.netloc,
                parsed.path,
                parsed.params,
                new_query,
                parsed.fragment
            ))
            
            logger.debug(f"å¢å¼ºURL: {original_url} -> {enhanced_url}")
            return enhanced_url
            
        except Exception as e:
            logger.error(f"æ„å»ºå¢å¼ºURLå¤±è´¥: {str(e)}")
            return original_url
    
    def _create_html_template(self, keyword, data):
        """åˆ›å»ºHTMLæ¨¡æ¿"""
        import urllib.parse
        import json
        current_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
        
        # åŠ è½½cookiesæ•°æ®ç”¨äºJavaScript
        cookies_json = "[]"
        try:
            cookies_data = self._load_cookies()
            if cookies_data:
                cookies_json = json.dumps(cookies_data, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"åŠ è½½cookiesç”¨äºJavaScriptå¤±è´¥: {str(e)}")
        
        # ç”Ÿæˆç¬”è®°å¡ç‰‡HTML
        notes_html = ""
        for i, note in enumerate(data, 1):
            # å®‰å…¨åœ°è·å–ç¬”è®°ä¿¡æ¯
            title = note.get('title', 'æ— æ ‡é¢˜').replace('\n', '<br>')
            desc = note.get('desc', 'æ— æè¿°').replace('\n', '<br>')
            author = note.get('author', 'æœªçŸ¥ä½œè€…')
            cover = note.get('cover', '')
            url = note.get('url', '#')
            xsec_token = note.get('xsec_token', '')
            likes = note.get('likes', 0)
            comments = note.get('comments', 0)
            collects = note.get('collects', 0)
            
            # æ ¼å¼åŒ–æ•°å­—æ˜¾ç¤º
            def format_number(num):
                if num >= 10000:
                    return f"{num//10000}ä¸‡+"
                elif num >= 1000:
                    return f"{num//1000}k+"
                else:
                    return str(num)
            
            likes_str = format_number(likes)
            comments_str = format_number(comments)
            collects_str = format_number(collects)
            
            # æ„å»ºå®Œæ•´çš„URLï¼Œæ·»åŠ xsec_tokenå’Œxsec_sourceå‚æ•°
            enhanced_url = self._build_enhanced_url(url, xsec_token)
            
            # ç›´æ¥ä½¿ç”¨åŸå§‹å›¾ç‰‡URLï¼Œé€šè¿‡JavaScriptå¤„ç†åŠ è½½å¤±è´¥
            note_html = f'''
            <div class="note-card" data-note-id="{note.get('id', '')}">
                <div class="note-image">
                    <img src="{cover}" alt="{title}" loading="lazy" 
                         data-original="{cover}"
                         onerror="handleImageError(this)">
                    <div class="note-rank">#{i}</div>
                </div>
                <div class="note-content">
                    <h3 class="note-title">{title}</h3>
                    <p class="note-desc">{desc}</p>
                    <div class="note-author">@{author}</div>
                    <div class="note-stats">
                        <span class="stat-item">
                            <i class="fas fa-heart"></i> {likes_str}
                        </span>
                        <span class="stat-item">
                            <i class="fas fa-comment"></i> {comments_str}
                        </span>
                        <span class="stat-item">
                            <i class="fas fa-star"></i> {collects_str}
                        </span>
                    </div>
                    <div class="note-links">
                        <a href="javascript:void(0)" onclick="directAccess('{enhanced_url}')" class="note-link direct-link">ç›´æ¥è®¿é—®</a>
                        <a href="javascript:void(0)" onclick="createSimilarNote('{note.get("id", "")}')" class="note-link create-link">æ–°å¢åŒç±»ç¬”è®°</a>
                    </div>
                </div>
            </div>
            '''
            notes_html += note_html
        
        # ç”Ÿæˆå®Œæ•´çš„HTMLé¡µé¢
        html_template = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="no-referrer">
    <meta http-equiv="Content-Security-Policy" content="img-src * data: blob: 'unsafe-inline'; default-src 'self' 'unsafe-inline' 'unsafe-eval' *;">
    <title>"{keyword}"çš„æœç´¢ç»“æœ - å°çº¢ä¹¦çƒ­é—¨ç¬”è®°</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            background: linear-gradient(135deg, #ff6b6b, #ff8e8e, #ffa8a8);
            min-height: 100vh;
            color: #333;
        }}
        
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }}
        
        .header {{
            text-align: center;
            margin-bottom: 40px;
            background: rgba(255, 255, 255, 0.95);
            padding: 30px 20px;
            border-radius: 20px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }}
        
        .header h1 {{
            font-size: 2.5em;
            color: #ff6b6b;
            margin-bottom: 10px;
        }}
        
        .search-info {{
            font-size: 1.2em;
            color: #666;
            margin-bottom: 10px;
        }}
        
        .update-time {{
            font-size: 0.9em;
            color: #999;
        }}
        
        .proxy-notice {{
            background: linear-gradient(135deg, #4CAF50, #45a049);
            color: white;
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 20px;
            text-align: center;
            font-size: 0.95em;
            box-shadow: 0 4px 15px rgba(76, 175, 80, 0.3);
        }}
        
        .proxy-notice .icon {{
            font-size: 1.2em;
            margin-right: 8px;
        }}
        
        .results-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
            gap: 25px;
            margin-bottom: 40px;
        }}
        
        .note-card {{
            background: white;
            border-radius: 15px;
            overflow: hidden;
            box-shadow: 0 8px 25px rgba(0,0,0,0.1);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            position: relative;
        }}
        
        .note-card:hover {{
            transform: translateY(-5px);
            box-shadow: 0 15px 40px rgba(0,0,0,0.2);
        }}
        
        .note-image {{
            position: relative;
            height: 200px;
            overflow: hidden;
        }}
        
        .note-image img {{
            width: 100%;
            height: 100%;
            object-fit: cover;
            transition: transform 0.3s ease;
            background-color: #f5f5f5;
            display: block;
        }}
        
        .note-image img.loading {{
            background: #f5f5f5 url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 24 24"><path fill="%23ccc" d="M12,1A11,11,0,1,0,23,12,11,11,0,0,0,12,1Zm0,19a8,8,0,1,1,8-8A8,8,0,0,1,12,20Z" opacity=".25"/><path fill="%23666" d="M12,4a8,8,0,0,1,7.89,6.7A1.53,1.53,0,0,0,21.38,12h0a1.5,1.5,0,0,0,1.48-1.75,11,11,0,0,0-21.72,0A1.5,1.5,0,0,0,2.62,12h0a1.53,1.53,0,0,0,1.49-1.3A8,8,0,0,1,12,4Z"><animateTransform attributeName="transform" dur="0.75s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></svg>') center no-repeat;
            background-size: 40px 40px;
        }}
        
        .note-image img.error {{
            background: #f5f5f5 url('data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 24 24"><path fill="%23ccc" d="M21,19V5c0-1.1-0.9-2-2-2H5C3.9,3,3,3.9,3,5v14c0,1.1,0.9,2,2,2h14C20.1,21,21,20.1,21,19z M8.5,13.5l2.5,3.01L14.5,12l4.5,6H5L8.5,13.5z"/></svg>') center no-repeat;
            background-size: 40px 40px;
        }}
        
        .note-card:hover .note-image img {{
            transform: scale(1.05);
        }}
        
        .note-rank {{
            position: absolute;
            top: 10px;
            left: 10px;
            background: rgba(255, 107, 107, 0.9);
            color: white;
            padding: 5px 10px;
            border-radius: 15px;
            font-weight: bold;
            font-size: 0.9em;
        }}
        
        .note-content {{
            padding: 20px;
        }}
        
        .note-title {{
            font-size: 1.1em;
            font-weight: bold;
            margin-bottom: 10px;
            color: #333;
            line-height: 1.4;
            display: -webkit-box;
            -webkit-line-clamp: 2;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }}
        
        .note-desc {{
            font-size: 0.9em;
            color: #666;
            margin-bottom: 15px;
            line-height: 1.5;
            display: -webkit-box;
            -webkit-line-clamp: 3;
            -webkit-box-orient: vertical;
            overflow: hidden;
        }}
        
        .note-author {{
            font-size: 0.85em;
            color: #ff6b6b;
            margin-bottom: 15px;
            font-weight: 500;
        }}
        
        .note-stats {{
            display: flex;
            justify-content: space-between;
            margin-bottom: 15px;
            font-size: 0.85em;
        }}
        
        .stat-item {{
            color: #999;
        }}
        
        .stat-item i {{
            margin-right: 4px;
        }}
        
        .note-links {{
            display: flex;
            gap: 8px;
            margin-top: 15px;
        }}
        
        .note-link {{
            flex: 1;
            display: inline-block;
            color: white;
            padding: 8px 12px;
            border-radius: 18px;
            text-decoration: none;
            font-size: 0.8em;
            font-weight: 500;
            transition: all 0.3s ease;
            text-align: center;
        }}
        
        .direct-link {{
            background: linear-gradient(45deg, #4CAF50, #45a049);
        }}
        
        .direct-link:hover {{
            background: linear-gradient(45deg, #45a049, #3d8b40);
            transform: translateY(-1px);
            box-shadow: 0 3px 10px rgba(76, 175, 80, 0.3);
        }}
        
        .create-link {{
            background: linear-gradient(45deg, #FF9800, #F57C00);
        }}
        
        .create-link:hover {{
            background: linear-gradient(45deg, #F57C00, #E65100);
            transform: translateY(-1px);
            box-shadow: 0 3px 10px rgba(255, 152, 0, 0.3);
        }}
        
        .back-button {{
            position: fixed;
            top: 20px;
            left: 20px;
            background: rgba(255, 255, 255, 0.9);
            color: #ff6b6b;
            padding: 10px 20px;
            border: none;
            border-radius: 25px;
            cursor: pointer;
            font-size: 1em;
            font-weight: 500;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 8px;
        }}
        
        .back-button:hover {{
            background: white;
            transform: translateY(-2px);
            box-shadow: 0 8px 25px rgba(0,0,0,0.15);
        }}
        
        .footer {{
            text-align: center;
            padding: 40px 20px;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            margin-top: 40px;
        }}
        
        .footer p {{
            margin-bottom: 10px;
            color: #666;
        }}
        
        .disclaimer {{
            font-size: 0.8em;
            color: #999;
        }}
        
        @media (max-width: 768px) {{
            .results-grid {{
                grid-template-columns: 1fr;
                gap: 20px;
            }}
            
            .header h1 {{
                font-size: 2em;
            }}
            
            .back-button {{
                position: static;
                margin-bottom: 20px;
            }}
        }}
    </style>
</head>
<body>
    <a href="/" class="back-button">
        <i class="fas fa-arrow-left"></i>
        è¿”å›æœç´¢
    </a>
    
    <div class="container">
        <div class="header">
            <h1>"{keyword}"çš„çƒ­é—¨ç¬”è®°</h1>
            <div class="search-info">å…±æ‰¾åˆ° {len(data)} æ¡ç›¸å…³ç¬”è®°</div>
            <div class="update-time">æ›´æ–°æ—¶é—´ï¼š{current_time}</div>
        </div>
        

        
        <div class="results-grid">
            {notes_html}
        </div>
        
        <div class="footer">
            <p>Â© 2023 å°çº¢ä¹¦çƒ­é—¨ç¬”è®°æŸ¥è¯¢ - ä»…ä¾›å­¦ä¹ ç ”ç©¶ä½¿ç”¨</p>
            <p class="disclaimer">æœ¬å·¥å…·ä¸éš¶å±äºå°çº¢ä¹¦å®˜æ–¹ï¼Œæ•°æ®ä»…ä¾›å‚è€ƒ</p>
        </div>
    </div>
    
    <script>
        // å°çº¢ä¹¦cookiesé…ç½®
        const xiaohongShuCookies = {cookies_json};
        
        // ç›´æ¥è®¿é—®å‡½æ•°
        function directAccess(url) {{
            try {{
                // è®¾ç½®å°çº¢ä¹¦cookies
                setCookiesForXiaohongshu();
                
                // å»¶è¿Ÿè·³è½¬ï¼Œç¡®ä¿cookiesè®¾ç½®å®Œæˆ
                setTimeout(() => {{
                    window.open(url, '_blank');
                }}, 500);
                
            }} catch (error) {{
                console.error('è®¾ç½®cookieså¤±è´¥:', error);
                // å¦‚æœè®¾ç½®cookieså¤±è´¥ï¼Œä»ç„¶å°è¯•ç›´æ¥è®¿é—®
                window.open(url, '_blank');
            }}
        }}
        
        // æ–°å¢åŒç±»ç¬”è®°å‡½æ•°
        function createSimilarNote(noteId) {{
            try {{
                // æ˜¾ç¤ºåŠ è½½æç¤º
                const loadingModal = showLoadingModal('æ­£åœ¨åˆ†æç¬”è®°å†…å®¹ï¼Œè¯·ç¨å€™...');
                
                // è°ƒç”¨åç«¯APIåˆ†æç¬”è®°å†…å®¹
                fetch(`/api/create-similar-note/${{noteId}}`, {{
                    method: 'POST',
                    headers: {{
                        'Content-Type': 'application/json',
                    }},
                }})
                .then(response => response.json())
                .then(data => {{
                    hideLoadingModal(loadingModal);
                    
                    if (data.success) {{
                        // æ˜¾ç¤ºç”Ÿæˆçš„ç¬”è®°å†…å®¹é¢„è§ˆ
                        showNotePreview(data.generated_note, noteId);
                    }} else {{
                        alert('ç”Ÿæˆç¬”è®°å¤±è´¥: ' + (data.message || 'æœªçŸ¥é”™è¯¯'));
                    }}
                }})
                .catch(error => {{
                    hideLoadingModal(loadingModal);
                    console.error('åˆ›å»ºåŒç±»ç¬”è®°å¤±è´¥:', error);
                    alert('åˆ›å»ºåŒç±»ç¬”è®°å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•');
                }});
                
            }} catch (error) {{
                console.error('åˆ›å»ºåŒç±»ç¬”è®°å¤±è´¥:', error);
                alert('åˆ›å»ºåŒç±»ç¬”è®°å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•');
            }}
        }}
        
        // è®¾ç½®å°çº¢ä¹¦cookies
        function setCookiesForXiaohongshu() {{
            xiaohongShuCookies.forEach(cookie => {{
                try {{
                    // åªè®¾ç½®éhttpOnlyçš„cookiesï¼ˆæµè§ˆå™¨é™åˆ¶ï¼‰
                    if (!cookie.httpOnly) {{
                        let cookieString = `${{cookie.name}}=${{cookie.value}}`;
                        
                        // æ·»åŠ domain
                        if (cookie.domain) {{
                            cookieString += `; domain=${{cookie.domain}}`;
                        }}
                        
                        // æ·»åŠ path
                        if (cookie.path) {{
                            cookieString += `; path=${{cookie.path}}`;
                        }}
                        
                        // æ·»åŠ secure
                        if (cookie.secure) {{
                            cookieString += `; secure`;
                        }}
                        
                        // æ·»åŠ sameSite
                        if (cookie.sameSite) {{
                            cookieString += `; samesite=${{cookie.sameSite}}`;
                        }}
                        
                        // æ·»åŠ expires
                        if (cookie.expiry) {{
                            const expireDate = new Date(cookie.expiry * 1000);
                            cookieString += `; expires=${{expireDate.toUTCString()}}`;
                        }}
                        
                        document.cookie = cookieString;
                        console.log('è®¾ç½®cookie:', cookie.name);
                    }}
                }} catch (error) {{
                    console.warn('è®¾ç½®cookieå¤±è´¥:', cookie.name, error);
                }}
            }});
        }}
        
        // å›¾ç‰‡é”™è¯¯å¤„ç†å‡½æ•°
        function handleImageError(img) {{
            if (img.dataset.retryCount) {{
                img.dataset.retryCount = parseInt(img.dataset.retryCount) + 1;
            }} else {{
                img.dataset.retryCount = '1';
            }}
            
            const retryCount = parseInt(img.dataset.retryCount);
            const originalUrl = img.dataset.original;
            
            if (retryCount === 1) {{
                // ç¬¬ä¸€æ¬¡å¤±è´¥ï¼šå°è¯•ç§»é™¤URLå‚æ•°
                const cleanUrl = originalUrl.split('!')[0];
                console.log('å›¾ç‰‡åŠ è½½å¤±è´¥ï¼Œå°è¯•æ¸…ç†URL:', cleanUrl);
                img.src = cleanUrl;
            }} else {{
                // æœ€ç»ˆå¤±è´¥ï¼šæ˜¾ç¤ºå ä½ç¬¦
                console.log('å›¾ç‰‡åŠ è½½å¤±è´¥ï¼Œæ˜¾ç¤ºå ä½ç¬¦');
                img.src = 'data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjAwIiBoZWlnaHQ9IjIwMCIgdmlld0JveD0iMCAwIDIwMCAyMDAiIGZpbGw9Im5vbmUiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CjxyZWN0IHdpZHRoPSIyMDAiIGhlaWdodD0iMjAwIiBmaWxsPSIjRjVGNUY1Ii8+Cjx0ZXh0IHg9IjEwMCIgeT0iMTAwIiB0ZXh0LWFuY2hvcj0ibWlkZGxlIiBkb21pbmFudC1iYXNlbGluZT0iY2VudHJhbCIgZmlsbD0iIzk5OTk5OSIgZm9udC1zaXplPSIxNCIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIj7lsI/nuqLkuaZDRE48L3RleHQ+Cjwvc3ZnPg==';
                img.onerror = null; // é˜²æ­¢æ— é™å¾ªç¯
            }}
        }}
        
        // æ˜¾ç¤ºåŠ è½½æ¨¡æ€æ¡†
        function showLoadingModal(message) {{
            const modal = document.createElement('div');
            modal.style.cssText = `
                position: fixed; top: 0; left: 0; right: 0; bottom: 0;
                background: rgba(0,0,0,0.7); z-index: 10000;
                display: flex; align-items: center; justify-content: center;
            `;
            
            const content = document.createElement('div');
            content.style.cssText = `
                background: white; padding: 30px; border-radius: 15px;
                text-align: center; min-width: 300px; box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            `;
            
            content.innerHTML = `
                <div style="font-size: 18px; margin-bottom: 20px;">${{message}}</div>
                <div style="display: inline-block; width: 40px; height: 40px; border: 4px solid #f3f3f3;
                     border-top: 4px solid #ff6b6b; border-radius: 50%; animation: spin 1s linear infinite;"></div>
                <style>
                    @keyframes spin {{ 0% {{ transform: rotate(0deg); }} 100% {{ transform: rotate(360deg); }} }}
                </style>
            `;
            
            modal.appendChild(content);
            document.body.appendChild(modal);
            return modal;
        }}
        
        // éšè—åŠ è½½æ¨¡æ€æ¡†
        function hideLoadingModal(modal) {{
            if (modal && modal.parentNode) {{
                modal.parentNode.removeChild(modal);
            }}
        }}
        
        // æ˜¾ç¤ºç¬”è®°é¢„è§ˆæ¨¡æ€æ¡†
        function showNotePreview(generatedNote, originalNoteId) {{
            const modal = document.createElement('div');
            modal.style.cssText = `
                position: fixed; top: 0; left: 0; right: 0; bottom: 0;
                background: rgba(0,0,0,0.8); z-index: 10000;
                display: flex; align-items: center; justify-content: center;
                overflow-y: auto; padding: 20px;
            `;
            
            const content = document.createElement('div');
            content.style.cssText = `
                background: white; padding: 30px; border-radius: 15px;
                max-width: 600px; width: 90%; max-height: 80vh; overflow-y: auto;
                box-shadow: 0 10px 30px rgba(0,0,0,0.3);
            `;
            
            // æ„å»ºåŸç¬”è®°ä¿¡æ¯HTML
            let originalNoteHtml = '';
            if (generatedNote.original_note_detail) {{
                const original = generatedNote.original_note_detail;
                let imagesHtml = '';
                
                if (original.images && original.images.length > 0) {{
                    imagesHtml = `
                        <div style="margin-top: 15px;">
                            <h4 style="color: #666; margin-bottom: 10px;">ğŸ“¸ åŸç¬”è®°å›¾ç‰‡</h4>
                            <div style="display: flex; flex-wrap: wrap; gap: 10px;">
                                ${{original.images.map(img => `
                                    <img src="${{img.web_path}}" alt="åŸç¬”è®°å›¾ç‰‡" 
                                         style="width: 80px; height: 80px; object-fit: cover; border-radius: 8px; border: 2px solid #ddd;"
                                         onclick="window.open('${{img.original_url}}', '_blank')" 
                                         title="ç‚¹å‡»æŸ¥çœ‹åŸå›¾">
                                `).join('')}}
                            </div>
                        </div>
                    `;
                }}
                
                originalNoteHtml = `
                    <div style="background: #e8f4fd; padding: 20px; border-radius: 10px; margin-bottom: 20px; border-left: 4px solid #2196F3;">
                        <h3 style="color: #1976D2; margin-bottom: 15px;">ğŸ“– åŸç¬”è®°å†…å®¹å‚è€ƒ</h3>
                        
                        <div style="margin-bottom: 15px;">
                            <h4 style="color: #333; margin-bottom: 8px;">ğŸ“ åŸæ ‡é¢˜</h4>
                            <p style="font-size: 14px; line-height: 1.5; color: #555; background: white; padding: 10px; border-radius: 6px;">${{original.title}}</p>
                        </div>
                        
                        <div style="margin-bottom: 15px;">
                            <h4 style="color: #333; margin-bottom: 8px;">ğŸ“„ åŸå†…å®¹</h4>
                            <p style="font-size: 13px; line-height: 1.6; color: #555; background: white; padding: 10px; border-radius: 6px; white-space: pre-wrap; max-height: 120px; overflow-y: auto;">${{original.content}}</p>
                        </div>
                        
                        <div style="margin-bottom: 15px;">
                            <h4 style="color: #333; margin-bottom: 8px;">ğŸ·ï¸ åŸæ ‡ç­¾</h4>
                            <div style="display: flex; flex-wrap: wrap; gap: 6px;">
                                ${{original.tags.map(tag => `<span style="background: #2196F3; color: white; padding: 3px 8px; border-radius: 12px; font-size: 11px;">${{tag}}</span>`).join('')}}
                            </div>
                        </div>
                        
                        <div style="margin-bottom: 10px;">
                            <h4 style="color: #333; margin-bottom: 8px;">ğŸ‘¤ åŸä½œè€…</h4>
                            <span style="color: #666; font-size: 13px;">${{original.author}}</span>
                        </div>
                        
                        ${{imagesHtml}}
                    </div>
                `;
            }}
            
            content.innerHTML = `
                <h2 style="color: #ff6b6b; margin-bottom: 20px; text-align: center;">
                    ğŸ¨ AIç”Ÿæˆçš„åŒç±»ç¬”è®°é¢„è§ˆ
                </h2>
                
                ${{originalNoteHtml}}
                
                <div style="background: #f8f9fa; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                    <h3 style="color: #333; margin-bottom: 10px;">ğŸ“ ç”Ÿæˆæ ‡é¢˜</h3>
                    <p style="font-size: 16px; line-height: 1.5; margin-bottom: 15px;">${{generatedNote.title}}</p>
                    
                    <h3 style="color: #333; margin-bottom: 10px;">ğŸ“„ ç”Ÿæˆå†…å®¹</h3>
                    <p style="font-size: 14px; line-height: 1.6; white-space: pre-wrap;">${{generatedNote.content}}</p>
                </div>
                
                <div style="background: #e8f5e8; padding: 15px; border-radius: 10px; margin-bottom: 20px;">
                    <h3 style="color: #2e7d32; margin-bottom: 10px;">ğŸ·ï¸ æ ‡ç­¾å»ºè®®</h3>
                    <div style="display: flex; flex-wrap: wrap; gap: 8px;">
                        ${{generatedNote.tags.map(tag => `<span style="background: #4caf50; color: white; padding: 4px 8px; border-radius: 12px; font-size: 12px;">${{tag}}</span>`).join('')}}
                    </div>
                </div>
                
                <div style="background: #fff3e0; padding: 15px; border-radius: 10px; margin-bottom: 25px;">
                    <h3 style="color: #f57c00; margin-bottom: 10px;">ğŸ’¡ åˆ›ä½œå»ºè®®</h3>
                    <p style="font-size: 13px; line-height: 1.5; color: #666;">${{generatedNote.suggestions}}</p>
                </div>
                
                <div style="display: flex; gap: 10px; justify-content: center;">
                    <button onclick="openXhsCreatePage()" style="
                        background: linear-gradient(45deg, #ff6b6b, #ff8e8e); color: white;
                        border: none; padding: 12px 24px; border-radius: 25px;
                        font-size: 14px; font-weight: 500; cursor: pointer;
                        transition: all 0.3s ease;
                    " onmouseover="this.style.transform='translateY(-2px)'" 
                       onmouseout="this.style.transform='translateY(0)'">
                        ğŸš€ å»å°çº¢ä¹¦åˆ›å»ºç¬”è®°
                    </button>
                    
                    <button onclick="copyNoteContent('${{originalNoteId}}')" style="
                        background: linear-gradient(45deg, #4caf50, #66bb6a); color: white;
                        border: none; padding: 12px 24px; border-radius: 25px;
                        font-size: 14px; font-weight: 500; cursor: pointer;
                        transition: all 0.3s ease;
                    " onmouseover="this.style.transform='translateY(-2px)'" 
                       onmouseout="this.style.transform='translateY(0)'">
                        ğŸ“‹ å¤åˆ¶å†…å®¹
                    </button>
                    
                    <button onclick="closeNotePreview()" style="
                        background: #999; color: white;
                        border: none; padding: 12px 24px; border-radius: 25px;
                        font-size: 14px; font-weight: 500; cursor: pointer;
                        transition: all 0.3s ease;
                    " onmouseover="this.style.transform='translateY(-2px)'" 
                       onmouseout="this.style.transform='translateY(0)'">
                        âŒ å…³é—­
                    </button>
                </div>
            `;
            
            modal.appendChild(content);
            document.body.appendChild(modal);
            
            // ç‚¹å‡»èƒŒæ™¯å…³é—­
            modal.addEventListener('click', function(e) {{
                if (e.target === modal) {{
                    closeNotePreview();
                }}
            }});
            
            // è®¾ç½®å…¨å±€å¼•ç”¨ä»¥ä¾¿å…³é—­
            window.currentNotePreviewModal = modal;
            window.currentGeneratedNote = generatedNote;
        }}
        
        // å…³é—­ç¬”è®°é¢„è§ˆ
        function closeNotePreview() {{
            if (window.currentNotePreviewModal) {{
                document.body.removeChild(window.currentNotePreviewModal);
                window.currentNotePreviewModal = null;
                window.currentGeneratedNote = null;
            }}
        }}
        
        // æ‰“å¼€å°çº¢ä¹¦åˆ›å»ºé¡µé¢
        function openXhsCreatePage() {{
            // è®¾ç½®å°çº¢ä¹¦cookies
            setCookiesForXiaohongshu();
            
            // å»¶è¿Ÿè·³è½¬ï¼Œç¡®ä¿cookiesè®¾ç½®å®Œæˆ
            setTimeout(() => {{
                window.open('https://creator.xiaohongshu.com/publish/publish?source=official&from=menu&target=image', '_blank');
            }}, 500);
            
            closeNotePreview();
        }}
        
        // å¤åˆ¶ç¬”è®°å†…å®¹
        function copyNoteContent(originalNoteId) {{
            if (window.currentGeneratedNote) {{
                const content = `æ ‡é¢˜ï¼š${{window.currentGeneratedNote.title}}

å†…å®¹ï¼š
${{window.currentGeneratedNote.content}}

æ ‡ç­¾ï¼š${{window.currentGeneratedNote.tags.join(' ')}}

åˆ›ä½œå»ºè®®ï¼š
${{window.currentGeneratedNote.suggestions}}`;
                
                navigator.clipboard.writeText(content).then(() => {{
                    alert('ğŸ“‹ ç¬”è®°å†…å®¹å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼');
                }}).catch(err => {{
                    console.error('å¤åˆ¶å¤±è´¥:', err);
                    // å¤‡ç”¨æ–¹æ¡ˆï¼šåˆ›å»ºä¸´æ—¶textarea
                    const textarea = document.createElement('textarea');
                    textarea.value = content;
                    document.body.appendChild(textarea);
                    textarea.select();
                    document.execCommand('copy');
                    document.body.removeChild(textarea);
                    alert('ğŸ“‹ ç¬”è®°å†…å®¹å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼');
                }});
            }}
        }}
        
        // æ·»åŠ ä¸€äº›äº¤äº’æ•ˆæœå’Œå›¾ç‰‡åŠ è½½å¤„ç†
        document.addEventListener('DOMContentLoaded', function() {{
            const cards = document.querySelectorAll('.note-card');
            const images = document.querySelectorAll('.note-image img');
            
            // å¡ç‰‡äº¤äº’æ•ˆæœ
            cards.forEach(card => {{
                card.addEventListener('mouseenter', function() {{
                    this.style.transform = 'translateY(-5px) scale(1.02)';
                }});
                
                card.addEventListener('mouseleave', function() {{
                    this.style.transform = 'translateY(0) scale(1)';
                }});
            }});
            
            // å›¾ç‰‡åŠ è½½å¤„ç†
            images.forEach(img => {{
                img.addEventListener('load', function() {{
                    this.classList.remove('loading');
                    this.classList.add('loaded');
                    console.log('å›¾ç‰‡åŠ è½½æˆåŠŸ:', this.src);
                }});
                
                // åˆå§‹çŠ¶æ€
                img.classList.add('loading');
            }});
        }});
    </script>
</body>
</html>'''
        
        # æ›¿æ¢cookieså ä½ç¬¦
        html_template = html_template.replace('{cookies_json}', cookies_json)
        
        return html_template
    
    def _load_from_cache(self, keyword, max_age=None):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        cache_path = self._get_cache_path(keyword)
        if not os.path.exists(cache_path):
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            max_age = max_age or self.search_config['CACHE_EXPIRE_TIME']
            if time.time() - cache['timestamp'] > max_age:
                logger.info(f"ç¼“å­˜å·²è¿‡æœŸ: {cache_path}")
                return None
            
            # ğŸ”§ ä¿®å¤ï¼šéªŒè¯ç¼“å­˜æ•°æ®æ˜¯å¦æœ‰æ•ˆ
            cached_data = cache.get('data', [])
            if not cached_data or len(cached_data) == 0:
                self._debug_log(f"âš ï¸ ç¼“å­˜æ–‡ä»¶å­˜åœ¨ä½†æ•°æ®ä¸ºç©ºï¼Œæ¸…ç†æ— æ•ˆç¼“å­˜: {cache_path}", "WARNING")
                self._remove_empty_cache(keyword)
                return None
            
            logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {cache_path}")
            return cached_data
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {str(e)}")
            return None
    
    def _handle_anti_bot(self):
        """å¤„ç†åçˆ¬è™«æœºåˆ¶ - æ”¹è¿›ç‰ˆæœ¬ï¼Œå¢å¼ºæœç´¢é¡µé¢ä¿æŒåŠŸèƒ½"""
        try:
            original_url = self.driver.current_url
            self._debug_log(f"ğŸ” åçˆ¬è™«å¤„ç†å‰URL: {original_url}")
            
            # ç­‰å¾…é¡µé¢ç¨³å®š
            time.sleep(8)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æç¤ºæˆ–éªŒè¯ç 
            page_text = self.driver.page_source.lower()
            anti_bot_keywords = ['ç™»å½•', 'login', 'éªŒè¯', 'captcha', 'æ»‘åŠ¨', 'slider', 'ç‚¹å‡»', 'click', 'å®‰å…¨']
            has_anti_bot = any(keyword in page_text for keyword in anti_bot_keywords)
            
            if has_anti_bot:
                self._debug_log("âš ï¸ æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶æˆ–ç™»å½•è¦æ±‚ï¼Œå°è¯•å¤„ç†...")
            
            # å°è¯•å…³é—­å„ç§å¯èƒ½çš„å¼¹çª—å’Œé®ç½©
            close_strategies = [
                # é€šç”¨å…³é—­æŒ‰é’®
                ("xpath", "//div[contains(@class, 'close')]"),
                ("xpath", "//button[contains(@class, 'close')]"), 
                ("xpath", "//span[contains(@class, 'close')]"),
                ("xpath", "//i[contains(@class, 'close')]"),
                
                # æ–‡å­—å…³é—­æŒ‰é’®
                ("xpath", "//div[contains(text(), 'å…³é—­')]"),
                ("xpath", "//button[contains(text(), 'å…³é—­')]"),
                ("xpath", "//span[contains(text(), 'Ã—')]"),
                ("xpath", "//div[contains(text(), 'Ã—')]"),
                
                # ç™»å½•å¼¹çª—å…³é—­
                ("xpath", "//div[contains(@class, 'modal')]//div[contains(@class, 'close')]"),
                ("xpath", "//div[contains(@class, 'dialog')]//div[contains(@class, 'close')]"),
                ("xpath", "//div[contains(@class, 'popup')]//div[contains(@class, 'close')]"),
                
                # CSSé€‰æ‹©å™¨
                ("css", ".close"),
                ("css", "[data-testid*='close']"),
                ("css", "[aria-label*='å…³é—­']"),
                ("css", "[aria-label*='close']"),
                
                # å…¶ä»–å¯èƒ½çš„å…³é—­å…ƒç´ 
                ("xpath", "//div[@role='button' and contains(text(), 'è·³è¿‡')]"),
                ("xpath", "//button[contains(text(), 'è·³è¿‡')]"),
                ("xpath", "//div[contains(@class, 'skip')]"),
            ]
            
            closed_elements = 0
            for method, selector in close_strategies:
                try:
                    if method == "xpath":
                        elements = self.driver.find_elements(By.XPATH, selector)
                    else:  # css
                        elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    
                    if elements:
                        for element in elements[:3]:  # æœ€å¤šç‚¹å‡»3ä¸ªåŒç±»å…ƒç´ 
                            try:
                                if element.is_displayed() and element.is_enabled():
                                    element.click()
                                    self._debug_log(f"âœ… æˆåŠŸç‚¹å‡»å…³é—­æŒ‰é’®: {selector}")
                                    closed_elements += 1
                                    time.sleep(2)
                            except Exception:
                                continue
                    
                    if closed_elements >= 3:  # å¦‚æœå·²ç»å…³é—­è¶³å¤Ÿå¤šçš„å…ƒç´ ï¼Œåœæ­¢
                        break
                        
                except Exception:
                    continue
            
            if closed_elements > 0:
                self._debug_log(f"âœ… å…±å…³é—­äº† {closed_elements} ä¸ªå¼¹çª—/é®ç½©")
                time.sleep(5)  # ç­‰å¾…é¡µé¢é‡æ–°åŠ è½½
            
            # å°è¯•æŒ‰ESCé”®å…³é—­å¼¹çª—
            try:
                from selenium.webdriver.common.keys import Keys
                self.driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)
                time.sleep(2)
                self._debug_log("âœ… å·²å‘é€ESCé”®")
            except Exception:
                pass
            
            # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘ï¼Œå¦‚æœæ˜¯åˆ™å¼ºåˆ¶è¿”å›æœç´¢é¡µé¢
            current_url = self.driver.current_url
            self._debug_log(f"ğŸ” åçˆ¬è™«å¤„ç†åURL: {current_url}")
            
            # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°éæœç´¢é¡µé¢
            redirected_to_non_search = self._check_redirected_from_search(original_url, current_url)
            
            if redirected_to_non_search:
                self._debug_log("âš ï¸ æ£€æµ‹åˆ°è¢«é‡å®šå‘åˆ°éæœç´¢é¡µé¢ï¼Œå°è¯•å¼ºåˆ¶è¿”å›æœç´¢")
                success = self._force_return_to_search(original_url)
                if not success:
                    self._debug_log("âŒ å¼ºåˆ¶è¿”å›æœç´¢é¡µé¢å¤±è´¥", "WARNING")
                    return False
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦ä»ç„¶æœ‰é˜»æŒ¡å…ƒç´ 
            try:
                if 'login' in current_url.lower() or 'auth' in current_url.lower() or 'captcha' in current_url.lower():
                    self._debug_log("ğŸ” æ£€æµ‹åˆ°éªŒè¯ç é¡µé¢ï¼Œå¯åŠ¨äººå·¥è¾…åŠ©éªŒè¯...")
                    return self._handle_captcha_verification()
                    
                # æ£€æŸ¥é¡µé¢å†…å®¹é•¿åº¦
                page_content_length = len(self.driver.page_source)
                if page_content_length < 5000:
                    self._debug_log(f"âš ï¸ é¡µé¢å†…å®¹è¾ƒå°‘({page_content_length}å­—ç¬¦)ï¼Œå¯èƒ½ä»è¢«åçˆ¬è™«é˜»æŒ¡")
                else:
                    self._debug_log(f"âœ… é¡µé¢å†…å®¹æ­£å¸¸({page_content_length}å­—ç¬¦)")
                    
            except Exception as e:
                self._debug_log(f"âš ï¸ æ£€æŸ¥é¡µé¢çŠ¶æ€æ—¶å‡ºé”™: {str(e)}", "WARNING")
            
            self._debug_log("âœ… åçˆ¬è™«å¤„ç†å®Œæˆ")
            return True
            
        except Exception as e:
            self._debug_log(f"âŒ å¤„ç†åçˆ¬è™«æœºåˆ¶æ—¶å‡ºé”™: {str(e)}", "WARNING")
            return True  # å³ä½¿å¤„ç†å¤±è´¥ä¹Ÿç»§ç»­æ‰§è¡Œ

    def _check_redirected_from_search(self, original_url, current_url):
        """æ£€æŸ¥æ˜¯å¦ä»æœç´¢é¡µé¢è¢«é‡å®šå‘åˆ°å…¶ä»–é¡µé¢"""
        try:
            # æ£€æŸ¥åŸå§‹URLæ˜¯å¦ä¸ºæœç´¢URL
            search_indicators_original = [
                'search_result' in original_url,
                'keyword=' in original_url,
                'search' in original_url.lower()
            ]
            
            # æ£€æŸ¥å½“å‰URLæ˜¯å¦ä¸ºéæœç´¢é¡µé¢
            non_search_indicators_current = [
                'search_result' not in current_url,
                'keyword=' not in current_url,
                'homefeed' in current_url,
                'explore' in current_url,
                'recommend' in current_url,
                current_url.count('/') <= 3,  # å¯èƒ½æ˜¯é¦–é¡µ
                current_url.endswith('xiaohongshu.com') or current_url.endswith('xiaohongshu.com/')
            ]
            
            was_search = any(search_indicators_original)
            is_non_search = any(non_search_indicators_current)
            
            if was_search and is_non_search:
                self._debug_log(f"ğŸš¨ æ£€æµ‹åˆ°é‡å®šå‘ï¼š{original_url[:50]}... -> {current_url[:50]}...")
                return True
            
            return False
            
        except Exception as e:
            self._debug_log(f"âš ï¸ æ£€æŸ¥é‡å®šå‘çŠ¶æ€å‡ºé”™: {str(e)}", "WARNING")
            return False

    def _is_recommendation_page(self, page_source, current_url):
        """æ£€æµ‹æ˜¯å¦ä¸ºæ¨èé¡µé¢"""
        try:
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åœ¨æœç´¢ç»“æœé¡µé¢ - å¦‚æœåœ¨æœç´¢é¡µé¢ï¼Œåˆ™ä¸æ˜¯æ¨èé¡µé¢
            if ('search_result' in current_url or 
                'keyword=' in current_url or 
                '/search/' in current_url):
                self._debug_log(f"âœ… ç¡®è®¤åœ¨æœç´¢é¡µé¢ï¼ŒURL: {current_url}")
                return False
            
            # æ£€æµ‹æ¨èé¡µé¢çš„å¤šç§æ ‡è¯†
            recommendation_indicators = [
                "homefeed_recommend" in page_source,
                "é¦–é¡µæ¨è" in page_source,
                ("æ¨è" in page_source and "æœç´¢ç»“æœ" not in page_source and "search" not in page_source),
                "recommend" in current_url.lower(),
                "explore" in current_url.lower(),
                (current_url.endswith("xiaohongshu.com") or current_url.endswith("xiaohongshu.com/")),
                (current_url.count('/') <= 3 and 'search' not in current_url)  # å¯èƒ½æ˜¯é¦–é¡µä¸”ä¸åŒ…å«search
            ]
            
            is_recommendation = any(recommendation_indicators)
            
            if is_recommendation:
                self._debug_log(f"ğŸ” æ¨èé¡µé¢æ£€æµ‹ç»“æœ: {recommendation_indicators}")
                self._debug_log(f"ğŸ“ å½“å‰URL: {current_url}")
                return True
            
            self._debug_log(f"âœ… ä¸æ˜¯æ¨èé¡µé¢ï¼ŒURL: {current_url}")
            return False
            
        except Exception as e:
            self._debug_log(f"âš ï¸ æ¨èé¡µé¢æ£€æµ‹å‡ºé”™: {str(e)}")
            return False

    def _force_return_to_search(self, original_search_url, max_attempts=3):
        """å¼ºåˆ¶è¿”å›æœç´¢é¡µé¢"""
        try:
            self._debug_log(f"ğŸ”„ å¼€å§‹å¼ºåˆ¶è¿”å›æœç´¢é¡µé¢ï¼Œæœ€å¤§å°è¯•æ¬¡æ•°: {max_attempts}")
            
            for attempt in range(max_attempts):
                self._debug_log(f"ğŸ”„ å°è¯• {attempt + 1}/{max_attempts}: è¿”å›æœç´¢é¡µé¢")
                
                try:
                    # æ–¹æ³•1ï¼šç›´æ¥å¯¼èˆªåˆ°åŸå§‹æœç´¢URL
                    self.driver.get(original_search_url)
                    time.sleep(5)
                    
                    # æ£€æŸ¥æ˜¯å¦æˆåŠŸè¿”å›æœç´¢é¡µé¢
                    current_url = self.driver.current_url
                    page_source = self.driver.page_source
                    
                    if 'search_result' in current_url or 'keyword=' in current_url:
                        self._debug_log(f"âœ… æ–¹æ³•1æˆåŠŸï¼šç›´æ¥å¯¼èˆªè¿”å›æœç´¢é¡µé¢")
                        return True
                    
                    # æ–¹æ³•2ï¼šå¦‚æœç›´æ¥å¯¼èˆªå¤±è´¥ï¼Œå°è¯•é€šè¿‡æœç´¢æ¡†æœç´¢
                    if attempt == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶ä½¿ç”¨æœç´¢æ¡†
                        keyword = self._extract_keyword_from_url(original_search_url)
                        if keyword and self._try_search_via_search_box(keyword):
                            self._debug_log(f"âœ… æ–¹æ³•2æˆåŠŸï¼šé€šè¿‡æœç´¢æ¡†è¿”å›æœç´¢é¡µé¢")
                            return True
                    
                    # æ–¹æ³•3ï¼šæ„é€ æ–°çš„æœç´¢URL
                    if attempt == 1:  # åœ¨ç¬¬äºŒæ¬¡å°è¯•æ—¶ä½¿ç”¨
                        keyword = self._extract_keyword_from_url(original_search_url)
                        if keyword and self._try_construct_new_search_url(keyword):
                            self._debug_log(f"âœ… æ–¹æ³•3æˆåŠŸï¼šæ„é€ æ–°æœç´¢URLè¿”å›æœç´¢é¡µé¢")
                            return True
                    
                    self._debug_log(f"âŒ å°è¯• {attempt + 1} å¤±è´¥ï¼Œå½“å‰URL: {current_url[:50]}...")
                    time.sleep(3)
                    
                except Exception as e:
                    self._debug_log(f"âŒ å°è¯• {attempt + 1} å‡ºé”™: {str(e)}")
                    time.sleep(3)
                    continue
            
            self._debug_log(f"âŒ æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œæ— æ³•å¼ºåˆ¶è¿”å›æœç´¢é¡µé¢")
            return False
            
        except Exception as e:
            self._debug_log(f"âŒ å¼ºåˆ¶è¿”å›æœç´¢é¡µé¢æ—¶å‡ºé”™: {str(e)}", "ERROR")
            return False

    def _extract_keyword_from_url(self, url):
        """ä»URLä¸­æå–å…³é”®è¯"""
        try:
            from urllib.parse import urlparse, parse_qs, unquote
            
            parsed = urlparse(url)
            query_params = parse_qs(parsed.query)
            
            # å°è¯•ä»keywordå‚æ•°è·å–
            if 'keyword' in query_params:
                keyword = unquote(query_params['keyword'][0])
                self._debug_log(f"ğŸ” ä»URLæå–å…³é”®è¯: {keyword}")
                return keyword
            
            return None
            
        except Exception as e:
            self._debug_log(f"âš ï¸ æå–å…³é”®è¯å¤±è´¥: {str(e)}")
            return None

    def _try_search_via_search_box(self, keyword):
        """å°è¯•é€šè¿‡æœç´¢æ¡†è¿›è¡Œæœç´¢"""
        try:
            self._debug_log(f"ğŸ” å°è¯•é€šè¿‡æœç´¢æ¡†æœç´¢: {keyword}")
            
            # é¦–å…ˆå¯¼èˆªåˆ°ä¸»é¡µ
            self.driver.get("https://www.xiaohongshu.com")
            time.sleep(3)
            
            # å°è¯•æ‰¾åˆ°æœç´¢æ¡†
            search_box_selectors = [
                "input[placeholder*='æœç´¢']",
                "input[placeholder*='search']",
                ".search-input",
                "#search-input",
                "input[type='search']",
                ".searchInput",
                "[data-testid*='search']"
            ]
            
            search_box = None
            for selector in search_box_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements and elements[0].is_displayed():
                        search_box = elements[0]
                        self._debug_log(f"âœ… æ‰¾åˆ°æœç´¢æ¡†: {selector}")
                        break
                except:
                    continue
            
            if not search_box:
                self._debug_log("âŒ æœªæ‰¾åˆ°æœç´¢æ¡†")
                return False
            
            # æ¸…ç©ºå¹¶è¾“å…¥å…³é”®è¯
            search_box.clear()
            search_box.send_keys(keyword)
            time.sleep(2)
            
            # å°è¯•æäº¤æœç´¢
            from selenium.webdriver.common.keys import Keys
            search_box.send_keys(Keys.ENTER)
            time.sleep(5)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·³è½¬åˆ°æœç´¢ç»“æœé¡µé¢
            current_url = self.driver.current_url
            if 'search_result' in current_url or 'keyword=' in current_url:
                self._debug_log("âœ… æœç´¢æ¡†æœç´¢æˆåŠŸ")
                return True
            else:
                self._debug_log(f"âŒ æœç´¢æ¡†æœç´¢å¤±è´¥ï¼Œå½“å‰URL: {current_url}")
                return False
                
        except Exception as e:
            self._debug_log(f"âŒ æœç´¢æ¡†æœç´¢å‡ºé”™: {str(e)}")
            return False

    def _try_construct_new_search_url(self, keyword):
        """å°è¯•æ„é€ æ–°çš„æœç´¢URL"""
        try:
            from urllib.parse import quote
            
            self._debug_log(f"ğŸ”§ å°è¯•æ„é€ æ–°æœç´¢URL: {keyword}")
            
            encoded_keyword = quote(keyword)
            new_search_urls = [
                f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}&source=web_search&type=comprehensive",
                f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}&source=web_search",
                f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}"
            ]
            
            for url in new_search_urls:
                try:
                    self._debug_log(f"ğŸ”— å°è¯•URL: {url}")
                    self.driver.get(url)
                    time.sleep(5)
                    
                    current_url = self.driver.current_url
                    if 'search_result' in current_url or 'keyword=' in current_url:
                        self._debug_log(f"âœ… æ–°URLæ„é€ æˆåŠŸ")
                        return True
                        
                except Exception as e:
                    self._debug_log(f"âŒ URL {url} å¤±è´¥: {str(e)}")
                    continue
            
            return False
            
        except Exception as e:
            self._debug_log(f"âŒ æ„é€ æ–°æœç´¢URLå‡ºé”™: {str(e)}")
            return False

    def _try_final_search_recovery(self, keyword):
        """æœ€ç»ˆæœç´¢é¡µé¢æ¢å¤å°è¯• - å½“æ‰€æœ‰å…¶ä»–æ–¹æ³•éƒ½å¤±è´¥æ—¶ä½¿ç”¨"""
        try:
            self._debug_log(f"ğŸš¨ å¯åŠ¨æœ€ç»ˆæœç´¢æ¢å¤ç¨‹åºï¼š{keyword}")
            
            # å°è¯•å¤šç§æ¢å¤ç­–ç•¥
            recovery_strategies = [
                self._recovery_direct_navigation,
                self._recovery_via_homepage_search,
                self._recovery_via_explore_page,
                self._recovery_refresh_and_retry
            ]
            
            for i, strategy in enumerate(recovery_strategies):
                try:
                    self._debug_log(f"ğŸ”„ æ‰§è¡Œæ¢å¤ç­–ç•¥ {i+1}: {strategy.__name__}")
                    if strategy(keyword):
                        self._debug_log(f"âœ… æ¢å¤ç­–ç•¥ {i+1} æˆåŠŸ")
                        return True
                    else:
                        self._debug_log(f"âŒ æ¢å¤ç­–ç•¥ {i+1} å¤±è´¥")
                except Exception as e:
                    self._debug_log(f"âŒ æ¢å¤ç­–ç•¥ {i+1} å‡ºé”™: {str(e)}")
                    continue
            
            self._debug_log("âŒ æ‰€æœ‰æ¢å¤ç­–ç•¥éƒ½å¤±è´¥")
            return False
            
        except Exception as e:
            self._debug_log(f"âŒ æœ€ç»ˆæœç´¢æ¢å¤å‡ºé”™: {str(e)}")
            return False

    def _recovery_direct_navigation(self, keyword):
        """æ¢å¤ç­–ç•¥1ï¼šç›´æ¥å¯¼èˆªåˆ°æœç´¢URL"""
        try:
            from urllib.parse import quote
            encoded_keyword = quote(keyword)
            direct_url = f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}&source=web_search&type=comprehensive"
            
            self.driver.get(direct_url)
            time.sleep(6)
            
            current_url = self.driver.current_url
            page_source = self.driver.page_source
            
            return ('search_result' in current_url or 'keyword=' in current_url) and self._verify_search_page_strict(page_source, keyword)
            
        except Exception as e:
            self._debug_log(f"ç›´æ¥å¯¼èˆªæ¢å¤å¤±è´¥: {str(e)}")
            return False

    def _recovery_via_homepage_search(self, keyword):
        """æ¢å¤ç­–ç•¥2ï¼šé€šè¿‡é¦–é¡µæœç´¢æ¡†"""
        try:
            # å¯¼èˆªåˆ°é¦–é¡µ
            self.driver.get("https://www.xiaohongshu.com")
            time.sleep(4)
            
            # æŸ¥æ‰¾å¹¶ä½¿ç”¨æœç´¢æ¡†
            search_selectors = [
                "input[placeholder*='æœç´¢']",
                "input[placeholder*='å‘ç°å¥½ç”Ÿæ´»']", 
                ".search-input",
                "#search-input",
                "input[type='search']",
                "[data-testid*='search']"
            ]
            
            for selector in search_selectors:
                try:
                    search_box = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if search_box.is_displayed():
                        search_box.clear()
                        search_box.send_keys(keyword)
                        
                        from selenium.webdriver.common.keys import Keys
                        search_box.send_keys(Keys.ENTER)
                        time.sleep(6)
                        
                        current_url = self.driver.current_url
                        page_source = self.driver.page_source
                        
                        if ('search_result' in current_url or 'keyword=' in current_url) and self._verify_search_page_strict(page_source, keyword):
                            return True
                        break
                except:
                    continue
            
            return False
            
        except Exception as e:
            self._debug_log(f"é¦–é¡µæœç´¢æ¢å¤å¤±è´¥: {str(e)}")
            return False

    def _recovery_via_explore_page(self, keyword):
        """æ¢å¤ç­–ç•¥3ï¼šé€šè¿‡æ¢ç´¢é¡µé¢"""
        try:
            # å°è¯•è®¿é—®æ¢ç´¢é¡µé¢
            explore_urls = [
                "https://www.xiaohongshu.com/explore",
                "https://www.xiaohongshu.com/discovery"
            ]
            
            for explore_url in explore_urls:
                try:
                    self.driver.get(explore_url)
                    time.sleep(4)
                    
                    # åœ¨æ¢ç´¢é¡µé¢æŸ¥æ‰¾æœç´¢åŠŸèƒ½
                    search_elements = self.driver.find_elements(By.CSS_SELECTOR, "input[placeholder*='æœç´¢']")
                    if search_elements:
                        search_box = search_elements[0]
                        search_box.clear()
                        search_box.send_keys(keyword)
                        
                        from selenium.webdriver.common.keys import Keys
                        search_box.send_keys(Keys.ENTER)
                        time.sleep(6)
                        
                        current_url = self.driver.current_url
                        page_source = self.driver.page_source
                        
                        if ('search_result' in current_url or 'keyword=' in current_url) and self._verify_search_page_strict(page_source, keyword):
                            return True
                except:
                    continue
            
            return False
            
        except Exception as e:
            self._debug_log(f"æ¢ç´¢é¡µé¢æ¢å¤å¤±è´¥: {str(e)}")
            return False

    def _recovery_refresh_and_retry(self, keyword):
        """æ¢å¤ç­–ç•¥4ï¼šåˆ·æ–°é¡µé¢å¹¶é‡è¯•"""
        try:
            self._debug_log("ğŸ”„ æ‰§è¡Œé¡µé¢åˆ·æ–°é‡è¯•ç­–ç•¥")
            
            # åˆ·æ–°å½“å‰é¡µé¢
            self.driver.refresh()
            time.sleep(5)
            
            # æ£€æŸ¥åˆ·æ–°åæ˜¯å¦å›åˆ°æœç´¢é¡µé¢
            current_url = self.driver.current_url
            page_source = self.driver.page_source
            
            if ('search_result' in current_url or 'keyword=' in current_url) and self._verify_search_page_strict(page_source, keyword):
                return True
            
            # å¦‚æœåˆ·æ–°æ— æ•ˆï¼Œé‡æ–°æ„é€ æœç´¢URL
            from urllib.parse import quote
            encoded_keyword = quote(keyword)
            retry_url = f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}"
            
            self.driver.get(retry_url)
            time.sleep(6)
            
            final_url = self.driver.current_url
            final_page_source = self.driver.page_source
            
            return ('search_result' in final_url or 'keyword=' in final_url) and self._verify_search_page_strict(final_page_source, keyword)
            
        except Exception as e:
            self._debug_log(f"åˆ·æ–°é‡è¯•æ¢å¤å¤±è´¥: {str(e)}")
            return False

    def _handle_captcha_verification(self):
        """å¤„ç†éªŒè¯ç  - äººå·¥è¾…åŠ©éªŒè¯ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        try:
            logger.info("ğŸ” æ£€æµ‹åˆ°æ‹¼å›¾éªŒè¯ç ï¼Œå¯åŠ¨äººå·¥è¾…åŠ©æ¨¡å¼...")
            
            # 1. è·å–å½“å‰é¡µé¢ä¿¡æ¯
            current_url = self.driver.current_url
            page_title = self.driver.title
            page_source = self.driver.page_source
            logger.info(f"ğŸ“‹ éªŒè¯é¡µé¢URL: {current_url}")
            logger.info(f"ğŸ“‹ é¡µé¢æ ‡é¢˜: {page_title}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯"éªŒè¯è¿‡äºé¢‘ç¹"çš„æƒ…å†µ
            frequent_check_indicators = ["éªŒè¯è¿‡äºé¢‘ç¹", "è¯·ç¨åé‡è¯•", "too frequent", "try again later"]
            is_frequent_error = any(indicator in page_source for indicator in frequent_check_indicators)
            
            if is_frequent_error:
                logger.warning("âš ï¸  æ£€æµ‹åˆ°'éªŒè¯è¿‡äºé¢‘ç¹'æç¤º")
                logger.info("ğŸ• å»ºè®®ç­‰å¾…10åˆ†é’Ÿåå†æ¬¡å°è¯•")
                logger.info("ğŸ“ æˆ–è€…æ‚¨å¯ä»¥:")
                logger.info("   1. æ¸…é™¤æµè§ˆå™¨ç¼“å­˜å’ŒCookie")  
                logger.info("   2. æ›´æ¢ç½‘ç»œç¯å¢ƒ")
                logger.info("   3. ç¨åé‡æ–°è¿è¡Œç¨‹åº")
                
                # ç­‰å¾…ä¸€æ®µæ—¶é—´ç„¶åç»§ç»­
                logger.info("â³ ç­‰å¾…60ç§’åç»§ç»­å°è¯•...")
                time.sleep(60)
                return True
            
            # 2. å¼ºåˆ¶æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            try:
                # å°è¯•é‡æ–°åˆ›å»ºå¯è§æµè§ˆå™¨
                if self.headless:
                    logger.info("ğŸ”„ å½“å‰ä¸ºæ— å¤´æ¨¡å¼ï¼Œé‡æ–°åˆ›å»ºå¯è§æµè§ˆå™¨...")
                    self._recreate_visible_browser()
                
                # æ¿€æ´»çª—å£
                self.driver.switch_to.window(self.driver.current_window_handle)
                self.driver.maximize_window()
                
                # macOSç‰¹å®šï¼šå°è¯•æ¿€æ´»Chrome
                try:
                    import subprocess
                    subprocess.run(['osascript', '-e', 'tell application "Google Chrome" to activate'], 
                                 check=False, timeout=5)
                    logger.info("ğŸ–¥ï¸  å·²å°è¯•æ¿€æ´»Chromeæµè§ˆå™¨")
                except Exception:
                    pass
                
                logger.info("ğŸ–¥ï¸  æµè§ˆå™¨çª—å£å·²æ¿€æ´»å¹¶æœ€å¤§åŒ–")
            except Exception as e:
                logger.warning(f"æ¿€æ´»æµè§ˆå™¨çª—å£å¤±è´¥: {str(e)}")
                logger.info("ğŸ’¡ è¯·æ‰‹åŠ¨æŸ¥çœ‹æ¡Œé¢ä¸Šçš„Chromeæµè§ˆå™¨çª—å£")
            
            # 3. åˆ›å»ºéªŒè¯æœŸé—´çš„æˆªå›¾ç›®å½•
            import os
            import hashlib
            from datetime import datetime
            
            debug_dir = os.path.join(self.cache_dir, 'debug_screenshots')
            os.makedirs(debug_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            session_id = hashlib.md5(f"captcha_{timestamp}_{random.randint(1000,9999)}".encode()).hexdigest()[:8]
            
            def take_verification_screenshot(step_name):
                """éªŒè¯æœŸé—´æˆªå›¾"""
                try:
                    screenshot_path = os.path.join(debug_dir, f"{timestamp}_{session_id}_verification_{step_name}.png")
                    self.driver.save_screenshot(screenshot_path)
                    logger.info(f"ğŸ” éªŒè¯æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
                    return screenshot_path
                except Exception as e:
                    logger.warning(f"éªŒè¯æˆªå›¾å¤±è´¥: {str(e)}")
                    return None
            
            # åˆå§‹éªŒè¯æˆªå›¾
            take_verification_screenshot("00_initial")
            
            # 4. æ˜¾ç¤ºç”¨æˆ·æç¤º
            logger.info("=" * 80)
            logger.info("ğŸš¨ ã€éœ€è¦äººå·¥éªŒè¯ã€‘")
            logger.info("ğŸ“± å°çº¢ä¹¦æ‹¼å›¾éªŒè¯ç å·²å‡ºç°ï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
            logger.info("")
            logger.info("âœ… æ­¥éª¤ï¼š")
            logger.info("   1ï¸âƒ£  åœ¨æ¡Œé¢ä¸Šæ‰¾åˆ°Chromeæµè§ˆå™¨çª—å£")
            logger.info("   2ï¸âƒ£  å®Œæˆæ‹¼å›¾éªŒè¯ç ï¼ˆæ‹–åŠ¨æ»‘å—åˆ°æ­£ç¡®ä½ç½®ï¼‰")
            logger.info("   3ï¸âƒ£  ç­‰å¾…é¡µé¢è‡ªåŠ¨è·³è½¬åˆ°æœç´¢ç»“æœ")
            logger.info("   4ï¸âƒ£  ç¨‹åºå°†è‡ªåŠ¨æ£€æµ‹éªŒè¯å®ŒæˆçŠ¶æ€")
            logger.info("")
            logger.info("â° è¶…æ—¶è®¾ç½®ï¼šæœ€å¤šç­‰å¾…8åˆ†é’Ÿ")
            logger.info("ğŸ“¸ è°ƒè¯•ï¼šç¨‹åºå°†æ¯1ç§’æˆªå›¾è®°å½•éªŒè¯è¿‡ç¨‹")
            logger.info("ğŸ’¡ æç¤ºï¼šå¦‚æœçœ‹ä¸åˆ°æµè§ˆå™¨ï¼Œè¯·æ£€æŸ¥Dockæˆ–çª—å£ç®¡ç†å™¨")
            logger.info("=" * 80)
            
            # 5. éªŒè¯ç­‰å¾…å¾ªç¯ï¼ˆæ¯1ç§’æˆªå›¾ï¼‰
            max_wait_time = 480  # å¢åŠ åˆ°8åˆ†é’Ÿ
            check_interval = 1    # æ¯1ç§’æ£€æŸ¥ä¸€æ¬¡
            waited_time = 0
            screenshot_count = 0
            
            logger.info(f"â³ å¼€å§‹ç­‰å¾…äººå·¥éªŒè¯ï¼ˆæœ€å¤š{max_wait_time}ç§’ï¼Œæ¯1ç§’æˆªå›¾ï¼‰...")
            
            while waited_time < max_wait_time:
                try:
                    # æ¯1ç§’æˆªå›¾
                    screenshot_count += 1
                    take_verification_screenshot(f"sec_{screenshot_count:03d}")
                    
                    # è·å–å½“å‰é¡µé¢çŠ¶æ€
                    current_url = self.driver.current_url
                    page_title = self.driver.title
                    page_source = self.driver.page_source
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»è·³è½¬å‡ºéªŒè¯é¡µé¢
                    verification_indicators = ["captcha", "login", "verify", "éªŒè¯"]
                    is_still_verifying = any(indicator in current_url.lower() for indicator in verification_indicators)
                    
                    if not is_still_verifying:
                        # è¿›ä¸€æ­¥æ£€æŸ¥é¡µé¢å†…å®¹
                        success_indicators = [
                            "search_result" in current_url,
                            "explore" in current_url,
                            "æœç´¢ç»“æœ" in page_source,
                            len(page_source) > 20000,  # æ­£å¸¸é¡µé¢é€šå¸¸å†…å®¹è¾ƒå¤š
                            "å°çº¢ä¹¦" in page_title and "éªŒè¯" not in page_title
                        ]
                        
                        if any(success_indicators):
                            logger.info("âœ… éªŒè¯æˆåŠŸï¼é¡µé¢å·²è·³è½¬åˆ°æ­£å¸¸å†…å®¹ï¼Œç»§ç»­æœç´¢æµç¨‹...")
                            logger.info(f"ğŸ“ æ–°URL: {current_url}")
                            logger.info(f"ğŸ“ æ–°æ ‡é¢˜: {page_title}")
                            take_verification_screenshot("success_final")
                            time.sleep(3)  # ç­‰å¾…é¡µé¢å®Œå…¨ç¨³å®š
                            return True
                    
                    # è¿›åº¦æç¤ºï¼ˆæ¯30ç§’ï¼‰
                    if waited_time % 30 == 0 and waited_time > 0:
                        remaining_time = max_wait_time - waited_time
                        logger.info(f"â³ ä»åœ¨ç­‰å¾…éªŒè¯å®Œæˆ... (å‰©ä½™{remaining_time}ç§’ï¼Œå·²æˆªå›¾{screenshot_count}å¼ )")
                        logger.info(f"ğŸ“ å½“å‰çŠ¶æ€: {page_title}")
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„é”™è¯¯æç¤º
                        error_indicators = ["å¤±è´¥", "é”™è¯¯", "éªŒè¯è¿‡äºé¢‘ç¹", "invalid", "failed"]
                        if any(indicator in page_source.lower() for indicator in error_indicators):
                            logger.warning("âš ï¸  æ£€æµ‹åˆ°å¯èƒ½çš„éªŒè¯é—®é¢˜ï¼Œè¯·é‡æ–°å°è¯•æˆ–åˆ·æ–°é¡µé¢")
                    
                    time.sleep(check_interval)
                    waited_time += check_interval
                    
                except Exception as e:
                    logger.warning(f"æ£€æŸ¥éªŒè¯çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
                    time.sleep(check_interval)
                    waited_time += check_interval
            
            # è¶…æ—¶å¤„ç†
            logger.warning("â° äººå·¥éªŒè¯è¶…æ—¶ï¼")
            logger.info(f"ğŸ“¸ å…±æˆªå–äº† {screenshot_count} å¼ éªŒè¯è¿‡ç¨‹æˆªå›¾")
            logger.info("ğŸ’¡ å¯èƒ½çš„åŸå› ï¼š")
            logger.info("   - éªŒè¯ç æœªå®Œæˆ")
            logger.info("   - æµè§ˆå™¨çª—å£æœªæ­£ç¡®æ˜¾ç¤º")
            logger.info("   - ç½‘ç»œè¿æ¥é—®é¢˜")
            logger.info("   - éªŒè¯è¿‡äºé¢‘ç¹å¯¼è‡´æš‚æ—¶å°ç¦")
            logger.info("ğŸ“ å»ºè®®ï¼šæ£€æŸ¥æˆªå›¾äº†è§£å…·ä½“æƒ…å†µï¼Œæˆ–ç¨åé‡è¯•")
            
            take_verification_screenshot("timeout_final")
            
            # å³ä½¿è¶…æ—¶ä¹Ÿå°è¯•ç»§ç»­æ‰§è¡Œ
            logger.info("ğŸ”„ å°½ç®¡è¶…æ—¶ï¼Œç¨‹åºå°†ç»§ç»­å°è¯•æå–æ•°æ®...")
            return True
            
        except Exception as e:
            logger.error(f"äººå·¥éªŒè¯å¤„ç†å¤±è´¥: {str(e)}")
            logger.info("ğŸ”„ å°½ç®¡å¤„ç†å¤±è´¥ï¼Œç¨‹åºå°†ç»§ç»­å°è¯•æå–æ•°æ®...")
            return True
    
    def _recreate_visible_browser(self):
        """é‡æ–°åˆ›å»ºå¯è§çš„æµè§ˆå™¨å®ä¾‹"""
        try:
            logger.info("ğŸ”„ é‡æ–°åˆ›å»ºå¯è§æµè§ˆå™¨...")
            
            # ä¿å­˜å½“å‰URL
            current_url = self.driver.current_url if self.driver else None
            
            # å…³é—­å½“å‰æµè§ˆå™¨
            if self.driver:
                try:
                    self.driver.quit()
                except Exception:
                    pass
                self.driver = None
            
            # ä¸´æ—¶è®¾ç½®ä¸ºéæ— å¤´æ¨¡å¼
            original_headless = self.headless
            self.headless = False
            
            # é‡æ–°åˆå§‹åŒ–æµè§ˆå™¨
            success = self._init_selenium()
            
            if success and current_url:
                # é‡æ–°è®¿é—®ä¹‹å‰çš„URL
                try:
                    self.driver.get(current_url)
                    time.sleep(3)
                    logger.info("âœ… å¯è§æµè§ˆå™¨åˆ›å»ºæˆåŠŸï¼Œå·²é‡æ–°è½½å…¥é¡µé¢")
                except Exception as e:
                    logger.warning(f"é‡æ–°è½½å…¥é¡µé¢å¤±è´¥: {str(e)}")
                
                # æ¢å¤åŸå§‹è®¾ç½®
                self.headless = original_headless
                return True
            else:
                logger.error("âŒ å¯è§æµè§ˆå™¨åˆ›å»ºå¤±è´¥")
                self.headless = original_headless
                return False
                
        except Exception as e:
            logger.error(f"é‡æ–°åˆ›å»ºæµè§ˆå™¨å¤±è´¥: {str(e)}")
            return False

    def _save_page_source(self, page_source, keyword):
        """ä¿å­˜é¡µé¢æºç """
        try:
            filename = f"page_source_{hashlib.md5(keyword.encode()).hexdigest()}.html"
            filepath = os.path.join(self.cache_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(page_source)
            
            return filepath
            
        except Exception as e:
            logger.warning(f"ä¿å­˜é¡µé¢æºç å¤±è´¥: {str(e)}")
            return ""

    def wait_for_content_load(self):
        """ç­‰å¾…å†…å®¹å®Œå…¨åŠ è½½ - å¯é…ç½®çš„è°ƒè¯•ç‰ˆæœ¬"""
        try:
            enable_screenshots = self.crawl_config.get('enable_debug_screenshots', False)
            screenshot_interval = self.crawl_config.get('screenshot_interval', 0)
            
            if enable_screenshots:
                import os
                import hashlib
                from datetime import datetime
                
                # åˆ›å»ºdebugæˆªå›¾ç›®å½•
                debug_dir = os.path.join(self.cache_dir, 'debug_screenshots')
                os.makedirs(debug_dir, exist_ok=True)
                
                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                session_id = hashlib.md5(f"{timestamp}_{random.randint(1000,9999)}".encode()).hexdigest()[:8]
                
                def take_debug_screenshot(step_name):
                    """æˆªå–è°ƒè¯•æˆªå›¾"""
                    if not enable_screenshots:
                        return None
                    try:
                        screenshot_path = os.path.join(debug_dir, f"{timestamp}_{session_id}_{step_name}.png")
                        self.driver.save_screenshot(screenshot_path)
                        logger.info(f"ğŸ” Debugæˆªå›¾å·²ä¿å­˜: {screenshot_path}")
                        return screenshot_path
                    except Exception as e:
                        logger.warning(f"æˆªå›¾å¤±è´¥: {str(e)}")
                        return None
                
                logger.info(f"ğŸ” å¼€å§‹é¡µé¢åŠ è½½è°ƒè¯•ï¼Œä¼šè¯ID: {session_id}")
                take_debug_screenshot("00_initial")
            else:
                def take_debug_screenshot(step_name):
                    return None
                logger.info("ğŸ” å¼€å§‹é¡µé¢åŠ è½½æ£€æµ‹")
            
            wait = WebDriverWait(self.driver, 12)
            
            # 1. ç­‰å¾…é¡µé¢åŸºæœ¬ç»“æ„åŠ è½½
            try:
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                logger.info("é¡µé¢åŸºæœ¬ç»“æ„åŠ è½½å®Œæˆ")
                take_debug_screenshot("01_body_loaded")
            except Exception as e:
                logger.warning(f"ç­‰å¾…é¡µé¢åŸºæœ¬ç»“æ„è¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œ: {str(e)}")
                take_debug_screenshot("01_body_timeout")
            
            # 2. ç­‰å¾…ä»»æ„å†…å®¹åŒºåŸŸï¼ˆæ›´å®½æ¾çš„æ¡ä»¶ï¼‰
            try:
                wait_short = WebDriverWait(self.driver, 8)
                wait_short.until(
                    lambda driver: driver.find_elements(By.CSS_SELECTOR, 
                        "div, section, article, main, [class*='content'], [class*='list'], [class*='item']")
                )
                logger.info("å†…å®¹åŒºåŸŸåŠ è½½å®Œæˆ")
                take_debug_screenshot("02_content_loaded")
            except Exception as e:
                logger.warning(f"ç­‰å¾…å†…å®¹åŒºåŸŸè¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œ: {str(e)}")
                take_debug_screenshot("02_content_timeout")
            
            # 3. å°è¯•æ£€æµ‹å°çº¢ä¹¦ç‰¹å®šå…ƒç´ ï¼Œä½†ä¸å¼ºåˆ¶è¦æ±‚
            try:
                elements_found = False
                selectors_to_try = [
                    'a[href*="/explore/"]',  # æ¢ç´¢é“¾æ¥
                    '[class*="note"]',       # ç¬”è®°ç›¸å…³ç±»å
                    '[class*="card"]',       # å¡ç‰‡ç›¸å…³ç±»å
                    '[class*="item"]',       # é¡¹ç›®ç›¸å…³ç±»å
                    '[class*="feed"]',       # åŠ¨æ€ç›¸å…³ç±»å
                    'img',                   # å›¾ç‰‡å…ƒç´ 
                    '[data-v-]',            # Vueç»„ä»¶
                ]
                
                for selector in selectors_to_try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements and len(elements) > 3:
                        logger.info(f"æ‰¾åˆ°è¶³å¤Ÿçš„å…ƒç´ ({len(elements)}ä¸ª): {selector}")
                        elements_found = True
                        break
                
                if not elements_found:
                    logger.warning("æœªæ‰¾åˆ°è¶³å¤Ÿçš„é¡µé¢å…ƒç´ ï¼Œä½†ç»§ç»­æ‰§è¡Œä¸‰ç§ç­–ç•¥")
                
                take_debug_screenshot("03_elements_check")
                    
            except Exception as e:
                logger.warning(f"æ£€æµ‹é¡µé¢å…ƒç´ æ—¶å‡ºé”™ï¼Œç»§ç»­æ‰§è¡Œ: {str(e)}")
                take_debug_screenshot("03_elements_error")
            
            # 4. ç»™JavaScriptæ¸²æŸ“æ—¶é—´ï¼Œæ ¹æ®é…ç½®å†³å®šæ˜¯å¦é€ç§’æˆªå›¾
            if screenshot_interval > 0 and enable_screenshots:
                logger.info(f"ğŸ” å¼€å§‹JavaScriptæ¸²æŸ“ç­‰å¾…æœŸé—´çš„æˆªå›¾ï¼ˆé—´éš”{screenshot_interval}ç§’ï¼‰...")
                render_time = 5  # æ€»ç­‰å¾…æ—¶é—´
                shots_count = 0
                for i in range(render_time):
                    time.sleep(1)
                    if (i + 1) % screenshot_interval == 0:
                        shots_count += 1
                        take_debug_screenshot(f"04_js_render_sec_{i+1}")
                        
                        # æ£€æŸ¥é¡µé¢çŠ¶æ€
                        try:
                            current_url = self.driver.current_url
                            page_title = self.driver.title
                            logger.info(f"ç¬¬{i+1}ç§’ - URL: {current_url}, æ ‡é¢˜: {page_title}")
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯ç™»å½•é¡µé¢
                            login_indicators = ["ç™»å½•", "login", "signin", "éªŒè¯", "captcha"]
                            page_source_lower = self.driver.page_source.lower()
                            is_login_page = any(indicator in page_source_lower for indicator in login_indicators)
                            
                            if is_login_page:
                                logger.warning(f"ç¬¬{i+1}ç§’æ£€æµ‹åˆ°ç™»å½•é¡µé¢ç‰¹å¾")
                                
                        except Exception as e:
                            logger.warning(f"ç¬¬{i+1}ç§’é¡µé¢çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}")
            else:
                logger.info("JavaScriptæ¸²æŸ“ç­‰å¾…ä¸­...")
                time.sleep(3)  # ç®€å•ç­‰å¾…3ç§’
            
            # 5. æ»šåŠ¨é¡µé¢è§¦å‘æ‡’åŠ è½½
            try:
                if self.crawl_config.get('enable_detailed_logs', True):
                    logger.info("ğŸ” æ‰§è¡Œé¡µé¢æ»šåŠ¨...")
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
                take_debug_screenshot("05_scroll_middle")
                time.sleep(1)
                self.driver.execute_script("window.scrollTo(0, 0);")
                take_debug_screenshot("05_scroll_top")
                time.sleep(1)
            except Exception as e:
                logger.warning(f"é¡µé¢æ»šåŠ¨å¤±è´¥: {str(e)}")
                take_debug_screenshot("05_scroll_error")
            
            # 6. æœ€ç»ˆéªŒè¯é¡µé¢çŠ¶æ€ - ä½†ä¸ç®¡ç»“æœå¦‚ä½•éƒ½è¿”å›True
            try:
                page_text = self.driver.page_source
                take_debug_screenshot("06_final_state")
                
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ä¿å­˜é¡µé¢æºç 
                if enable_screenshots:
                    final_html_path = os.path.join(debug_dir, f"{timestamp}_{session_id}_final_source.html")
                    with open(final_html_path, 'w', encoding='utf-8') as f:
                        f.write(page_text)
                    logger.info(f"ğŸ” æœ€ç»ˆé¡µé¢æºç å·²ä¿å­˜: {final_html_path}")
                
                if len(page_text) > 10000:
                    logger.info("é¡µé¢å†…å®¹åŠ è½½å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œä¸‰ç§æå–ç­–ç•¥")
                    return True
                elif len(page_text) > 5000:
                    logger.warning(f"é¡µé¢å†…å®¹è¾ƒå°‘({len(page_text)}å­—ç¬¦)ï¼Œä½†ç»§ç»­æ‰§è¡Œä¸‰ç§ç­–ç•¥")
                    return True
                else:
                    logger.warning(f"é¡µé¢å†…å®¹å¾ˆå°‘({len(page_text)}å­—ç¬¦)ï¼Œä½†ä»å°è¯•æ‰§è¡Œä¸‰ç§ç­–ç•¥")
                    return True
            except Exception as e:
                logger.warning(f"æ£€æŸ¥é¡µé¢å†…å®¹æ—¶å‡ºé”™ï¼Œç»§ç»­æ‰§è¡Œä¸‰ç§ç­–ç•¥: {str(e)}")
                take_debug_screenshot("06_final_error")
                return True
            
        except Exception as e:
            logger.warning(f"ç­‰å¾…å†…å®¹åŠ è½½å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œä¸‰ç§ç­–ç•¥: {e}")
            return True  # å…³é”®ï¼šå³ä½¿å®Œå…¨è¶…æ—¶ä¹Ÿç»§ç»­æ‰§è¡Œï¼Œç¡®ä¿ä¸‰ç§ç­–ç•¥èƒ½å¤Ÿè¿è¡Œ

    def search(self, keyword, max_results=10, use_cache=True):
        """æœç´¢å°çº¢ä¹¦å†…å®¹ - æ”¹è¿›ç‰ˆæœ¬"""
        self._debug_log(f"ğŸ” å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
        
        if use_cache:
            self._debug_log("ğŸ“‚ æ£€æŸ¥ç¼“å­˜...")
            cached_result = self._load_from_cache(keyword)
            if cached_result:
                self._debug_log(f"âœ… ä»ç¼“å­˜è·å–åˆ° {len(cached_result)} æ¡ç»“æœ")
                return cached_result[:max_results]
            else:
                self._debug_log("â„¹ï¸ ç¼“å­˜ä¸­æ— æ•°æ®ï¼Œè¿›è¡Œå®æ—¶æœç´¢")

        try:
            # ğŸ”§ ä¿®å¤ï¼šå…³é”®è¯URLç¼–ç å’Œæœç´¢URLæ ¼å¼
            encoded_keyword = quote(keyword)  # æ­£ç¡®ç¼–ç å…³é”®è¯
            
            # å°è¯•å¤šç§æœç´¢URLæ ¼å¼ - ä¿®æ­£typeå‚æ•°
            search_urls = [
                f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}&source=web_search&type=comprehensive",
                f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}&source=web_search&type=note",
                f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}&source=web_search"
            ]
            
            self._debug_log(f"ğŸŒ å‡†å¤‡äº† {len(search_urls)} ä¸ªæœç´¢URL")
            self._debug_log(f"ğŸ” åŸå§‹å…³é”®è¯: '{keyword}' -> ç¼–ç å: '{encoded_keyword}'")
            
            # ç¡®ä¿WebDriverå·²åˆå§‹åŒ–
            self._debug_log("ğŸš€ åˆå§‹åŒ–æµè§ˆå™¨...")
            if not self._ensure_driver_initialized():
                self._debug_log("âŒ WebDriveråˆå§‹åŒ–å¤±è´¥", "ERROR")
                return []
            
            self._debug_log("âœ… æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # å°è¯•ä¸åŒçš„æœç´¢URL
            search_success = False
            for i, search_url in enumerate(search_urls):
                try:
                    self._debug_log(f"ğŸ”— å°è¯•æœç´¢URL {i+1}/{len(search_urls)}: {search_url}")
                    
                    # è®¿é—®æœç´¢é¡µé¢
                    self.driver.get(search_url)
                    
                    # ç­‰å¾…é¡µé¢åŠ è½½
                    self._debug_log("â³ ç­‰å¾…é¡µé¢åŠ è½½...")
                    time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
                    
                    # ğŸ”§ ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„é¡µé¢éªŒè¯
                    page_source = self.driver.page_source
                    if self._verify_search_page_strict(page_source, keyword):
                        self._debug_log(f"âœ… æœç´¢URLéªŒè¯æˆåŠŸ: {search_url[:70]}...")
                        search_success = True
                        break
                    else:
                        self._debug_log(f"âš ï¸ æœç´¢URLéªŒè¯å¤±è´¥ï¼Œå¯èƒ½ä¸æ˜¯æœç´¢ç»“æœé¡µé¢")
                        
                        # ğŸ”§ æ–°å¢ï¼šå¦‚æœæ£€æµ‹åˆ°æ¨èé¡µé¢ï¼Œç«‹å³å°è¯•å¼ºåˆ¶è¿”å›æœç´¢
                        current_url = self.driver.current_url
                        if self._is_recommendation_page(page_source, current_url):
                            self._debug_log(f"ğŸš¨ æ£€æµ‹åˆ°æ¨èé¡µé¢ï¼Œç«‹å³å°è¯•å¼ºåˆ¶è¿”å›æœç´¢é¡µé¢")
                            if self._force_return_to_search(search_url):
                                self._debug_log(f"âœ… æˆåŠŸå¼ºåˆ¶è¿”å›æœç´¢é¡µé¢")
                                # é‡æ–°éªŒè¯é¡µé¢
                                final_page_source = self.driver.page_source
                                if self._verify_search_page_strict(final_page_source, keyword):
                                    self._debug_log(f"âœ… å¼ºåˆ¶è¿”å›åéªŒè¯æˆåŠŸ")
                                    search_success = True
                                    break
                                else:
                                    self._debug_log(f"âŒ å¼ºåˆ¶è¿”å›åä»ç„¶éªŒè¯å¤±è´¥")
                            else:
                                self._debug_log(f"âŒ å¼ºåˆ¶è¿”å›æœç´¢é¡µé¢å¤±è´¥")
                        
                        # ä¿å­˜å¤±è´¥é¡µé¢ç”¨äºè°ƒè¯•
                        if self.crawl_config.get('enable_detailed_logs', True):
                            failed_page_path = self._save_page_source(page_source, f"failed_{keyword}_url{i+1}")
                            self._debug_log(f"ğŸ“ å¤±è´¥é¡µé¢å·²ä¿å­˜: {failed_page_path}")
                        continue
                        
                except Exception as e:
                    self._debug_log(f"âŒ æœç´¢URLå¤±è´¥: {str(e)}", "WARNING")
                    continue
            
            if not search_success:
                self._debug_log("âŒ æ‰€æœ‰æœç´¢URLéƒ½å¤±è´¥ï¼Œå¯èƒ½é‡åˆ°åçˆ¬è™«æœºåˆ¶", "ERROR")
                return []
            
            # ç­‰å¾…å¹¶æ£€æµ‹åçˆ¬è™«
            self._debug_log("ğŸ›¡ï¸ æ£€æµ‹åçˆ¬è™«æœºåˆ¶...")
            if not self._handle_anti_bot():
                self._debug_log("âŒ åçˆ¬è™«æ£€æµ‹å¤„ç†å¤±è´¥", "ERROR") 
                return []
            
            self._debug_log("âœ… åçˆ¬è™«æ£€æµ‹é€šè¿‡")
            
            # ğŸ”§ æ–°å¢ï¼šæœ€ç»ˆç¡®è®¤æ˜¯å¦ä»åœ¨æœç´¢é¡µé¢
            final_url = self.driver.current_url
            final_page_source = self.driver.page_source
            
            if not self._verify_search_page_strict(final_page_source, keyword):
                self._debug_log("âš ï¸ åçˆ¬è™«å¤„ç†åä»æœªåœ¨æ­£ç¡®çš„æœç´¢é¡µé¢ï¼Œå°è¯•æœ€åä¸€æ¬¡å¼ºåˆ¶è·³è½¬")
                # æœ€åä¸€æ¬¡å°è¯•å¼ºåˆ¶è¿”å›æœç´¢
                if self._try_final_search_recovery(keyword):
                    self._debug_log("âœ… æœ€ç»ˆæœç´¢é¡µé¢æ¢å¤æˆåŠŸ")
                else:
                    self._debug_log("âŒ æœ€ç»ˆæœç´¢é¡µé¢æ¢å¤å¤±è´¥ï¼Œä½†ç»§ç»­å°è¯•æå–", "WARNING")
            
            # ç­‰å¾…å†…å®¹å®Œå…¨åŠ è½½ - ä½†ä¸ç®¡ç»“æœå¦‚ä½•éƒ½ç»§ç»­æ‰§è¡Œ
            self._debug_log("ğŸ“„ ç­‰å¾…é¡µé¢å†…å®¹å®Œå…¨åŠ è½½...")
            content_loaded = self.wait_for_content_load()
            if not content_loaded:
                self._debug_log("âš ï¸ é¡µé¢å†…å®¹åŠ è½½è¶…æ—¶ï¼Œä½†ç»§ç»­å°è¯•æå–", "WARNING")
            else:
                self._debug_log("âœ… é¡µé¢å†…å®¹åŠ è½½å®Œæˆ")
            
            # ä¿å­˜é¡µé¢æºç ç”¨äºè°ƒè¯•
            self._debug_log("ğŸ’¾ ä¿å­˜é¡µé¢æºç ...")
            page_source_path = self._save_page_source(self.driver.page_source, keyword)
            self._debug_log(f"ğŸ“ é¡µé¢æºç å·²ä¿å­˜: {page_source_path[:50]}...")
            
            # ä½¿ç”¨æ”¹è¿›çš„å¤šç­–ç•¥æå–
            self._debug_log("ğŸ”§ å¼€å§‹æ‰§è¡Œä¸‰ç§æå–ç­–ç•¥...")
            results = self.extract_notes_advanced(keyword, max_results)
            
            if results:
                self._debug_log(f"ğŸ“Š åˆæ­¥æå–åˆ° {len(results)} æ¡ç»“æœ")
                
                # éªŒè¯ç»“æœæ˜¯å¦ä¸å…³é”®è¯ç›¸å…³
                self._debug_log("ğŸ” éªŒè¯ç»“æœä¸å…³é”®è¯çš„ç›¸å…³æ€§...")
                validated_results = self._validate_search_results(results, keyword)
                
                # ğŸ”§ ä¿®å¤ï¼šåªæœ‰å½“çœŸæ­£æœ‰ç»“æœæ—¶æ‰ç¼“å­˜å’Œç”ŸæˆHTML
                if validated_results and len(validated_results) > 0:
                    # ç¼“å­˜ç»“æœ
                    if use_cache:
                        self._debug_log("ğŸ’¾ ç¼“å­˜æœç´¢ç»“æœ...")
                        self._save_to_cache(keyword, validated_results)
                    
                    self._debug_log(f"ğŸ‰ æœç´¢å®Œæˆï¼æ‰¾åˆ° {len(validated_results)} æ¡ç›¸å…³ç»“æœ")
                    return validated_results[:max_results]
                else:
                    # ğŸ”§ ä¿®å¤ï¼šåˆ é™¤å¯èƒ½å­˜åœ¨çš„ç©ºç¼“å­˜æ–‡ä»¶
                    self._debug_log(f"âš ï¸ æœªæ‰¾åˆ°ä¸å…³é”®è¯ '{keyword}' ç›¸å…³çš„æœç´¢ç»“æœï¼Œæ¸…ç†ç©ºç¼“å­˜", "WARNING")
                    self._remove_empty_cache(keyword)
                    return []
            else:
                self._debug_log("âŒ æœªæ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœ", "WARNING")
                return []
                
        except Exception as e:
            self._debug_log(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", "ERROR")
            return []

    def _remove_empty_cache(self, keyword):
        """
        ğŸ”§ ä¿®å¤æ–¹æ³•ï¼šåˆ é™¤ç©ºç¼“å­˜æ–‡ä»¶
        å½“æœç´¢ç»“æœéªŒè¯åå‘ç°æ²¡æœ‰æœ‰æ•ˆå†…å®¹æ—¶ï¼Œæ¸…ç†ç©ºç¼“å­˜æ–‡ä»¶
        """
        try:
            cache_path = self._get_cache_path(keyword)
            html_cache_dir = os.path.join(os.path.dirname(cache_path), 'results')
            
            # åˆ é™¤JSONç¼“å­˜æ–‡ä»¶
            if os.path.exists(cache_path):
                os.remove(cache_path)
                self._debug_log(f"ğŸ—‘ï¸ å·²åˆ é™¤ç©ºçš„JSONç¼“å­˜æ–‡ä»¶: {cache_path}", "DEBUG")
            
            # åˆ é™¤HTMLç¼“å­˜æ–‡ä»¶
            if os.path.exists(html_cache_dir):
                import hashlib
                html_hash = hashlib.md5(keyword.encode()).hexdigest()
                html_file = os.path.join(html_cache_dir, f"search_{html_hash}.html")
                if os.path.exists(html_file):
                    os.remove(html_file)
                    self._debug_log(f"ğŸ—‘ï¸ å·²åˆ é™¤ç©ºçš„HTMLç¼“å­˜æ–‡ä»¶: {html_file}", "DEBUG")
                    
        except Exception as e:
            self._debug_log(f"âŒ æ¸…ç†ç©ºç¼“å­˜æ—¶å‡ºé”™: {str(e)}", "WARNING")

    def _verify_search_page(self, page_source, keyword):
        """éªŒè¯é¡µé¢æ˜¯å¦ä¸ºæœç´¢ç»“æœé¡µé¢"""
        try:
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜å’Œå…³é”®æŒ‡æ ‡
            search_indicators = [
                f'"{keyword}"',  # å…³é”®è¯åœ¨JSONæ•°æ®ä¸­
                f"'{keyword}'",  # å…³é”®è¯åœ¨JavaScriptä¸­
                f"keyword={keyword}",  # URLå‚æ•°
                f"æœç´¢ç»“æœ",
                f"search_result",
                f"searchValue",
                # æ£€æŸ¥æ˜¯å¦ä¸æ˜¯é¦–é¡µæ¨è
                "homefeed_recommend" not in page_source or keyword in page_source
            ]
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨èé¡µé¢çš„ç‰¹å¾ä½†ä¸åŒ…å«æœç´¢å…³é”®è¯
            if "æ¨è" in page_source and "homefeed_recommend" in page_source:
                # å¦‚æœé¡µé¢åŒ…å«å…³é”®è¯ï¼Œåˆ™è®¤ä¸ºæ˜¯æœç´¢é¡µé¢
                keyword_found = any([
                    keyword in page_source,
                    keyword.lower() in page_source.lower(),
                    # æ£€æŸ¥URLç¼–ç çš„å…³é”®è¯
                    keyword.replace(' ', '%20') in page_source,
                    keyword.replace(' ', '+') in page_source
                ])
                
                if keyword_found:
                    logger.info(f"æ£€æµ‹åˆ°åŒ…å«å…³é”®è¯ '{keyword}' çš„é¡µé¢")
                    return True
                else:
                    logger.warning(f"é¡µé¢ä¼¼ä¹æ˜¯æ¨èé¡µé¢ï¼Œä¸åŒ…å«å…³é”®è¯ '{keyword}'")
                    return False
            
            # æ£€æŸ¥æœç´¢ç›¸å…³çš„æŒ‡æ ‡
            search_found = any([
                indicator in page_source for indicator in search_indicators[:6]
            ])
            
            return search_found
            
        except Exception as e:
            logger.error(f"éªŒè¯æœç´¢é¡µé¢æ—¶å‡ºé”™: {str(e)}")
            return False

    def _verify_search_page_strict(self, page_source, keyword):
        """ğŸ”§ æ›´ä¸¥æ ¼çš„é¡µé¢éªŒè¯ - ç¡®ä¿æ˜¯çœŸæ­£çš„æœç´¢ç»“æœé¡µé¢"""
        try:
            self._debug_log(f"ğŸ” å¼€å§‹ä¸¥æ ¼éªŒè¯é¡µé¢æ˜¯å¦ä¸ºå…³é”®è¯ '{keyword}' çš„æœç´¢ç»“æœ")
            
            # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºæ¨èé¡µé¢ï¼ˆæ’é™¤è¯¯åˆ¤ï¼‰
            if "homefeed_recommend" in page_source or "é¦–é¡µæ¨è" in page_source:
                self._debug_log("âŒ æ£€æµ‹åˆ°æ¨èé¡µé¢æ ‡è¯†ï¼Œéæœç´¢ç»“æœé¡µé¢")
                return False
            
            # 2. æ£€æŸ¥URLå‚æ•°ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
            encoded_keyword = quote(keyword)
            url_indicators = [
                f"keyword={keyword}",
                f"keyword={encoded_keyword}",
                f"searchValue={keyword}",
                f"query={keyword}"
            ]
            
            url_match = any(indicator in page_source for indicator in url_indicators)
            if url_match:
                self._debug_log("âœ… URLå‚æ•°ä¸­å‘ç°å…³é”®è¯ï¼Œç¡®è®¤ä¸ºæœç´¢é¡µé¢")
                return True
            
            # 3. æ£€æŸ¥é¡µé¢å†…å®¹ä¸­çš„å…³é”®è¯å‡ºç°
            keyword_indicators = [
                f'"{keyword}"',  # JSONä¸­çš„å…³é”®è¯
                f"'{keyword}'",  # JavaScriptä¸­çš„å…³é”®è¯
                f'æœç´¢"{keyword}"',  # æœç´¢æç¤ºæ–‡æœ¬
                f"keyword:{keyword}",  # é…ç½®å¯¹è±¡ä¸­çš„å…³é”®è¯
            ]
            
            content_match = any(indicator in page_source for indicator in keyword_indicators)
            
            # 4. æ£€æŸ¥æœç´¢ç›¸å…³çš„é¡µé¢å…ƒç´ 
            search_elements = [
                "search_result",
                "searchResult", 
                "æœç´¢ç»“æœ",
                "noteList",
                "feeds-page"
            ]
            
            element_match = any(element in page_source for element in search_elements)
            
            # 5. ç»¼åˆåˆ¤æ–­
            if content_match and element_match:
                self._debug_log("âœ… å†…å®¹å’Œå…ƒç´ éƒ½åŒ¹é…ï¼Œç¡®è®¤ä¸ºæœç´¢ç»“æœé¡µé¢")
                return True
            elif content_match:
                self._debug_log("âš ï¸ ä»…å†…å®¹åŒ¹é…ï¼Œå¯èƒ½ä¸ºæœç´¢é¡µé¢")
                return True
            else:
                self._debug_log("âŒ å…³é”®è¯å’Œæœç´¢å…ƒç´ éƒ½æœªåŒ¹é…ï¼Œéæœç´¢ç»“æœé¡µé¢")
                
                # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
                if self.crawl_config.get('enable_detailed_logs', True):
                    self._debug_log(f"ğŸ” é¡µé¢å†…å®¹é¢„è§ˆ: {page_source[:500]}...")
                
                return False
            
        except Exception as e:
            logger.error(f"éªŒè¯æœç´¢é¡µé¢æ—¶å‡ºé”™: {str(e)}")
            return False

    def _validate_search_results(self, results, keyword):
        """ğŸ”§ ä¿®å¤ï¼šéªŒè¯æœç´¢ç»“æœæ˜¯å¦ä¸å…³é”®è¯ç›¸å…³ - å¼ºåˆ¶æ‰§è¡Œä¸¥æ ¼éªŒè¯"""
        if not results or not keyword:
            return results
            
        try:
            validation_level = self.crawl_config.get('validation_strict_level', 'medium')
            
            self._debug_log(f"ğŸ” å¼€å§‹éªŒè¯ {len(results)} æ¡æœç´¢ç»“æœä¸å…³é”®è¯ '{keyword}' çš„ç›¸å…³æ€§")
            self._debug_log(f"ğŸ“Š å½“å‰éªŒè¯ä¸¥æ ¼åº¦: {validation_level}")
            
            # ğŸ”§ ä¿®å¤ï¼šå³ä½¿æ˜¯ä½ä¸¥æ ¼åº¦ï¼Œä¹Ÿè¦è¿›è¡ŒåŸºæœ¬çš„å…³é”®è¯åŒ¹é…éªŒè¯
            if validation_level == 'low':
                # ä½ä¸¥æ ¼åº¦ï¼šåŸºæœ¬å…³é”®è¯åŒ¹é…
                validated_results = self._basic_validate(results, keyword)
                self._debug_log(f"âœ… ä½ä¸¥æ ¼åº¦éªŒè¯å®Œæˆ: {len(results)} -> {len(validated_results)} æ¡ç›¸å…³ç»“æœ")
                return validated_results
            elif validation_level == 'high':
                # é«˜ä¸¥æ ¼åº¦ï¼šå¿…é¡»åŒ…å«å®Œæ•´å…³é”®è¯
                validated_results = self._strict_validate(results, keyword)
                self._debug_log(f"âœ… é«˜ä¸¥æ ¼åº¦éªŒè¯å®Œæˆ: {len(results)} -> {len(validated_results)} æ¡ç›¸å…³ç»“æœ")
                return validated_results
            else:
                # ä¸­ç­‰ä¸¥æ ¼åº¦ï¼šä½¿ç”¨çµæ´»çš„åŒ¹é…ç­–ç•¥
                validated_results = self._flexible_validate(results, keyword)
                self._debug_log(f"âœ… ä¸­ç­‰ä¸¥æ ¼åº¦éªŒè¯å®Œæˆ: {len(results)} -> {len(validated_results)} æ¡ç›¸å…³ç»“æœ")
                return validated_results
                
        except Exception as e:
            logger.error(f"éªŒè¯æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
            return results
    
    def _basic_validate(self, results, keyword):
        """ğŸ”§ æ–°å¢ï¼šä½ä¸¥æ ¼åº¦éªŒè¯ - åŸºæœ¬å…³é”®è¯åŒ¹é…"""
        validated_results = []
        keyword_lower = keyword.lower()
        keyword_words = keyword_lower.split()
        
        for result in results:
            title = result.get('title', '').lower()
            description = result.get('desc', '').lower()
            author = result.get('author', '').lower()
            tags = ' '.join(result.get('tags', [])).lower() if result.get('tags') else ''
            
            # ç»„åˆæ‰€æœ‰æ–‡æœ¬è¿›è¡ŒåŒ¹é…
            all_text = f"{title} {description} {author} {tags}"
            
            # åŸºæœ¬åŒ¹é…ç­–ç•¥
            is_relevant = any([
                # è‡³å°‘åŒ…å«ä¸€ä¸ªå…³é”®è¯çš„è¯
                any(word in all_text for word in keyword_words),
                # å¦‚æœæœ‰å®Œæ•´æ ‡é¢˜å’Œæè¿°ï¼Œä¸”æœ‰äº’åŠ¨æ•°æ®ï¼Œè®¤ä¸ºå¯èƒ½ç›¸å…³
                (len(title.strip()) > 5 and len(description.strip()) > 10 and 
                 (result.get('likes', 0) > 0 or result.get('comments', 0) > 0))
            ])
            
            if is_relevant:
                validated_results.append(result)
                if self.crawl_config.get('enable_detailed_logs', True):
                    self._debug_log(f"ğŸ“ åŸºæœ¬éªŒè¯é€šè¿‡: {title[:30]}...")
            else:
                if self.crawl_config.get('enable_detailed_logs', True):
                    self._debug_log(f"âŒ åŸºæœ¬éªŒè¯å¤±è´¥: {title[:30]}...")
        
        return validated_results
    
    def _strict_validate(self, results, keyword):
        """é«˜ä¸¥æ ¼åº¦éªŒè¯"""
        validated_results = []
        keyword_lower = keyword.lower()
        
        for result in results:
            title = result.get('title', '').lower()
            description = result.get('description', '').lower()
            author = result.get('author', '').lower()
            tags = ' '.join(result.get('tags', [])).lower()
            
            # å¿…é¡»åŒ…å«å®Œæ•´å…³é”®è¯
            if any([
                keyword_lower in title,
                keyword_lower in description,
                keyword_lower in author,
                keyword_lower in tags,
            ]):
                validated_results.append(result)
                logger.debug(f"ä¸¥æ ¼éªŒè¯é€šè¿‡: {result.get('title', '')[:50]}...")
            else:
                logger.debug(f"ä¸¥æ ¼éªŒè¯å¤±è´¥: {result.get('title', '')[:50]}...")
        
        logger.info(f"ä¸¥æ ¼éªŒè¯ç»“æœ: {len(results)} -> {len(validated_results)} æ¡ç›¸å…³ç»“æœ")
        return validated_results
    
    def _flexible_validate(self, results, keyword):
        """ä¸­ç­‰ä¸¥æ ¼åº¦éªŒè¯ - çµæ´»åŒ¹é…"""
        validated_results = []
        keyword_lower = keyword.lower()
        keyword_words = keyword_lower.split()
        
        for result in results:
            title = result.get('title', '').lower()
            description = result.get('description', '').lower()
            author = result.get('author', '').lower()
            tags = ' '.join(result.get('tags', [])).lower()
            
            # ç»„åˆæ‰€æœ‰æ–‡æœ¬è¿›è¡ŒåŒ¹é…
            all_text = f"{title} {description} {author} {tags}"
            
            # å¤šç§åŒ¹é…ç­–ç•¥
            is_relevant = any([
                # å®Œæ•´å…³é”®è¯åŒ¹é…
                keyword_lower in all_text,
                # å…³é”®è¯éƒ¨åˆ†åŒ¹é…ï¼ˆè‡³å°‘åŒ¹é…ä¸€åŠçš„è¯ï¼‰
                sum(1 for word in keyword_words if word in all_text) >= max(1, len(keyword_words) // 2),
                # å¦‚æœæ ‡é¢˜å’Œæè¿°éƒ½æœ‰å†…å®¹ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆç»“æœï¼ˆæ¥è‡ªæœç´¢é¡µé¢ï¼‰
                (len(title.strip()) > 3 and len(description.strip()) > 10),
                # å¦‚æœæœ‰å°é¢å›¾ç‰‡ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆç¬”è®°
                bool(result.get('cover_image')),
                # å¦‚æœæœ‰äº’åŠ¨æ•°æ®ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ‰æ•ˆç¬”è®°
                bool(result.get('like_count') or result.get('comment_count')),
            ])
            
            if is_relevant:
                validated_results.append(result)
                if self.crawl_config.get('enable_detailed_logs', True):
                    logger.debug(f"çµæ´»éªŒè¯é€šè¿‡: {result.get('title', '')[:50]}...")
            else:
                if self.crawl_config.get('enable_detailed_logs', True):
                    logger.debug(f"çµæ´»éªŒè¯å¤±è´¥: {result.get('title', '')[:50]}...")
        
        logger.info(f"çµæ´»éªŒè¯ç»“æœ: {len(results)} -> {len(validated_results)} æ¡ç›¸å…³ç»“æœ")
        return validated_results

    def extract_notes_advanced(self, keyword, max_results=10):
        """æ”¹è¿›çš„ç¬”è®°æå–ç­–ç•¥ - æ ¹æ®é…ç½®æ‰§è¡Œä¸åŒç­–ç•¥"""
        all_results = []
        strategies_executed = []
        
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œç­–ç•¥ï¼Œç›®æ ‡ç»“æœæ•°: {max_results}")
            
            # ç­–ç•¥1: é€šè¿‡é“¾æ¥hrefæå–ï¼ˆæœ€å¯é ï¼‰
            if self.crawl_config.get('enable_strategy_1', True):
                try:
                    logger.info("==================== æ‰§è¡Œç­–ç•¥1: æ¢ç´¢é“¾æ¥æå– ====================")
                    results_1 = self._extract_by_explore_links(max_results)
                    if results_1:
                        all_results.extend(results_1)
                        logger.info(f"âœ… ç­–ç•¥1(æ¢ç´¢é“¾æ¥): æˆåŠŸæå–åˆ° {len(results_1)} æ¡ç»“æœ")
                    else:
                        logger.warning("âŒ ç­–ç•¥1(æ¢ç´¢é“¾æ¥): æœªæå–åˆ°ç»“æœ")
                    strategies_executed.append(f"ç­–ç•¥1: {len(results_1) if results_1 else 0}æ¡")
                except Exception as e:
                    logger.error(f"âŒ ç­–ç•¥1(æ¢ç´¢é“¾æ¥)æ‰§è¡Œå¤±è´¥: {str(e)}")
                    strategies_executed.append("ç­–ç•¥1: æ‰§è¡Œå¤±è´¥")
            else:
                logger.info("ç­–ç•¥1: å·²ç¦ç”¨ï¼Œè·³è¿‡")
                strategies_executed.append("ç­–ç•¥1: å·²ç¦ç”¨")
            
            # ç­–ç•¥2: é€šè¿‡æ•°æ®å±æ€§æå–
            if self.crawl_config.get('enable_strategy_2', True):
                try:
                    logger.info("==================== æ‰§è¡Œç­–ç•¥2: æ•°æ®å±æ€§æå– ====================")
                    remaining_needed = max_results - len(all_results)
                    if remaining_needed > 0:
                        results_2 = self._extract_by_data_attributes(remaining_needed)
                        if results_2:
                            all_results.extend(results_2)
                            logger.info(f"âœ… ç­–ç•¥2(æ•°æ®å±æ€§): æˆåŠŸæå–åˆ° {len(results_2)} æ¡ç»“æœ")
                        else:
                            logger.warning("âŒ ç­–ç•¥2(æ•°æ®å±æ€§): æœªæå–åˆ°ç»“æœ")
                        strategies_executed.append(f"ç­–ç•¥2: {len(results_2) if results_2 else 0}æ¡")
                    else:
                        logger.info("ç­–ç•¥2: å·²è¾¾åˆ°ç›®æ ‡ç»“æœæ•°ï¼Œè·³è¿‡")
                        strategies_executed.append("ç­–ç•¥2: è·³è¿‡")
                except Exception as e:
                    logger.error(f"âŒ ç­–ç•¥2(æ•°æ®å±æ€§)æ‰§è¡Œå¤±è´¥: {str(e)}")
                    strategies_executed.append("ç­–ç•¥2: æ‰§è¡Œå¤±è´¥")
            else:
                logger.info("ç­–ç•¥2: å·²ç¦ç”¨ï¼Œè·³è¿‡")
                strategies_executed.append("ç­–ç•¥2: å·²ç¦ç”¨")
            
            # ç­–ç•¥3: é€šè¿‡JavaScriptæ‰§è¡Œæå–
            if self.crawl_config.get('enable_strategy_3', True):
                try:
                    logger.info("==================== æ‰§è¡Œç­–ç•¥3: JavaScriptæå– ====================")
                    remaining_needed = max_results - len(all_results)
                    if remaining_needed > 0:
                        results_3 = self._extract_by_javascript(remaining_needed)
                        if results_3:
                            all_results.extend(results_3)
                            logger.info(f"âœ… ç­–ç•¥3(JavaScript): æˆåŠŸæå–åˆ° {len(results_3)} æ¡ç»“æœ")
                        else:
                            logger.warning("âŒ ç­–ç•¥3(JavaScript): æœªæå–åˆ°ç»“æœ")
                        strategies_executed.append(f"ç­–ç•¥3: {len(results_3) if results_3 else 0}æ¡")
                    else:
                        logger.info("ç­–ç•¥3: å·²è¾¾åˆ°ç›®æ ‡ç»“æœæ•°ï¼Œè·³è¿‡")
                        strategies_executed.append("ç­–ç•¥3: è·³è¿‡")
                except Exception as e:
                    logger.error(f"âŒ ç­–ç•¥3(JavaScript)æ‰§è¡Œå¤±è´¥: {str(e)}")
                    strategies_executed.append("ç­–ç•¥3: æ‰§è¡Œå¤±è´¥")
            else:
                logger.info("ç­–ç•¥3: å·²ç¦ç”¨ï¼Œè·³è¿‡")
                strategies_executed.append("ç­–ç•¥3: å·²ç¦ç”¨")
                
            # ç­–ç•¥4: ç²¾å‡†å®¹å™¨æå– - æ–°å¢çš„æœ€å¼ºç­–ç•¥
            if self.crawl_config.get('enable_strategy_4', True):
                try:
                    logger.info("==================== æ‰§è¡Œç­–ç•¥4: ç²¾å‡†å®¹å™¨æå– ====================")
                    remaining_needed = max_results - len(all_results)
                    if remaining_needed > 0:
                        results_4 = self._extract_by_precise_containers(remaining_needed)
                        if results_4:
                            all_results.extend(results_4)
                            logger.info(f"âœ… ç­–ç•¥4(ç²¾å‡†å®¹å™¨): æˆåŠŸæå–åˆ° {len(results_4)} æ¡ç»“æœ")
                        else:
                            logger.warning("âŒ ç­–ç•¥4(ç²¾å‡†å®¹å™¨): æœªæå–åˆ°ç»“æœ")
                        strategies_executed.append(f"ç­–ç•¥4: {len(results_4) if results_4 else 0}æ¡")
                    else:
                        logger.info("ç­–ç•¥4: å·²è¾¾åˆ°ç›®æ ‡ç»“æœæ•°ï¼Œè·³è¿‡")
                        strategies_executed.append("ç­–ç•¥4: è·³è¿‡")
                except Exception as e:
                    logger.error(f"âŒ ç­–ç•¥4(ç²¾å‡†å®¹å™¨)æ‰§è¡Œå¤±è´¥: {str(e)}")
                    strategies_executed.append("ç­–ç•¥4: æ‰§è¡Œå¤±è´¥")
            else:
                logger.info("ç­–ç•¥4: å·²ç¦ç”¨ï¼Œè·³è¿‡")
                strategies_executed.append("ç­–ç•¥4: å·²ç¦ç”¨")
            
            # æ€»ç»“ç­–ç•¥æ‰§è¡Œæƒ…å†µ
            logger.info(f"==================== ç­–ç•¥æ‰§è¡Œæ€»ç»“ ====================")
            logger.info(f"å·²æ‰§è¡Œç­–ç•¥: {', '.join(strategies_executed)}")
            logger.info(f"åŸå§‹ç»“æœæ€»æ•°: {len(all_results)}")
            
            # æŒ‰ç…§äº’åŠ¨æ•°æ®æ’åº
            if all_results:
                try:
                    all_results.sort(key=lambda x: (
                        int(x.get('comment_count', 0)) if str(x.get('comment_count', '0')).isdigit() else 0,
                        int(x.get('like_count', 0)) if str(x.get('like_count', '0')).isdigit() else 0
                    ), reverse=True)
                    logger.info("ç¬”è®°å·²æŒ‰äº’åŠ¨æ•°æ®æ’åº: è¯„è®ºæ•°é™åº + æ”¶è—æ•°é™åº")
                except Exception as e:
                    logger.warning(f"æ’åºå¤±è´¥: {str(e)}")
            
            # å»é‡å¤„ç†
            unique_results = self._deduplicate_results(all_results)
            logger.info(f"å»é‡åç»“æœæ•°: {len(unique_results)}")
            
            # é™åˆ¶ç»“æœæ•°é‡
            final_results = unique_results[:max_results]
            logger.info(f"æœ€ç»ˆè¿”å›ç»“æœæ•°: {len(final_results)}")
            logger.info(f"==================== å››ç§ç­–ç•¥æ‰§è¡Œå®Œæˆ ====================")
            
            return final_results
            
        except Exception as e:
            logger.error(f"æå–ç¬”è®°æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

    def _extract_by_explore_links(self, max_results):
        """ç­–ç•¥1: é€šè¿‡exploreé“¾æ¥æå–ç¬”è®°"""
        results = []
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«exploreçš„é“¾æ¥
            explore_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*="/explore/"]')
            logger.info(f"æ‰¾åˆ° {len(explore_links)} ä¸ªæ¢ç´¢é“¾æ¥")
            
            # æ”¶é›†æ‰€æœ‰ç¬”è®°IDå’Œé“¾æ¥ä¿¡æ¯
            note_links = []
            processed_ids = set()
            
            for link in explore_links:
                if len(note_links) >= max_results:
                    break
                    
                try:
                    href = link.get_attribute('href')
                    if not href or '/explore/' not in href:
                        continue
                    
                    # æå–ç¬”è®°ID
                    note_id = href.split('/explore/')[-1].split('?')[0]
                    if not note_id or note_id in processed_ids:
                        continue
                    
                    processed_ids.add(note_id)
                    note_links.append({
                        'note_id': note_id,
                        'href': href,
                        'link_element': link
                    })
                
                except Exception as e:
                    logger.debug(f"å¤„ç†é“¾æ¥æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            # æ‰¹é‡æå–æ‰€æœ‰ç¬”è®°çš„xsec_token
            logger.info(f"å¼€å§‹æ‰¹é‡æå– {len(note_links)} ä¸ªç¬”è®°çš„xsec_token...")
            note_tokens = self._extract_all_xsec_tokens([item['note_id'] for item in note_links])
            
            # å¤„ç†æ¯ä¸ªç¬”è®°
            for i, note_link in enumerate(note_links):
                try:
                    note_id = note_link['note_id']
                    href = note_link['href']
                    link = note_link['link_element']
                    
                    # è·å–å¯¹åº”çš„xsec_token
                    xsec_token = note_tokens.get(note_id)
                    
                    # è·å–é“¾æ¥çš„çˆ¶çº§å®¹å™¨ï¼Œå¯»æ‰¾æ›´å¤šä¿¡æ¯
                    note_container = self._find_note_container(link)
                    if not note_container:
                        note_container = link
                    
                    # æå–ç¬”è®°ä¿¡æ¯
                    note_info = self._extract_note_info_from_container(note_container, note_id, href, xsec_token)
                    if note_info:
                        results.append(note_info)
                        logger.debug(f"æå–ç¬”è®° {i+1}: {note_info['title'][:30]}... (xsec_token: {xsec_token[:20] if xsec_token else 'None'}...)")
                
                except Exception as e:
                    logger.debug(f"å¤„ç†ç¬”è®°æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            return results
            
        except Exception as e:
            logger.error(f"é€šè¿‡exploreé“¾æ¥æå–å¤±è´¥: {str(e)}")
            return []

    def _find_note_container(self, link_element):
        """æŸ¥æ‰¾ç¬”è®°çš„å®¹å™¨å…ƒç´ """
        try:
            # å‘ä¸ŠæŸ¥æ‰¾å¯èƒ½çš„å®¹å™¨
            current = link_element
            
            for _ in range(5):  # æœ€å¤šå‘ä¸ŠæŸ¥æ‰¾5å±‚
                parent = current.find_element(By.XPATH, "./..")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬”è®°å¡ç‰‡å®¹å™¨
                if self._is_note_container(parent):
                    return parent
                
                current = parent
            
            return link_element
            
        except Exception:
            return link_element

    def _is_note_container(self, element):
        """åˆ¤æ–­å…ƒç´ æ˜¯å¦æ˜¯ç¬”è®°å®¹å™¨ - é’ˆå¯¹å°çº¢ä¹¦Vueç»„ä»¶ä¼˜åŒ–"""
        try:
            # æ£€æŸ¥Vueç»„ä»¶çš„data-vå±æ€§
            data_attributes = []
            for attr_name in element.get_property('attributes') or []:
                if hasattr(attr_name, 'name') and attr_name.name.startswith('data-v-'):
                    data_attributes.append(attr_name.name)
            
            # å°çº¢ä¹¦ç‰¹å®šçš„Vueç»„ä»¶æ ‡è¯†
            xhs_component_indicators = [
                'data-v-a264b01a',  # ç¬”è®°å¡ç‰‡ç»„ä»¶
                'data-v-330d9cca',  # feedså®¹å™¨
                'data-v-811a7fa6'   # feedsé¡µé¢
            ]
            
            for indicator in xhs_component_indicators:
                if element.get_attribute(indicator) is not None:
                    logger.debug(f"å‘ç°å°çº¢ä¹¦Vueç»„ä»¶: {indicator}")
                    return True
            
            # æ£€æŸ¥CSSç±»å
            class_name = element.get_attribute('class') or ''
            note_indicators = [
                'note-item', 'note_item', 'noteItem',
                'card', 'item', 'feed', 'post', 'content',
                'explore', 'result', 'list-item',
                'cover', 'wrapper', 'container'
            ]
            
            for indicator in note_indicators:
                if indicator in class_name.lower():
                    logger.debug(f"å‘ç°CSSç±»åæŒ‡æ ‡: {indicator}")
                    return True
            
            # æ£€æŸ¥æ ‡ç­¾å
            tag_name = element.tag_name.lower()
            if tag_name in ['section', 'article', 'li']:
                return True
            
            # æ£€æŸ¥å…ƒç´ å¤§å°å’Œå†…å®¹
            try:
                size = element.size
                # æ£€æŸ¥æ˜¯å¦æœ‰åˆç†çš„å¤§å°ï¼ˆç¬”è®°å¡ç‰‡é€šå¸¸æ¯”è¾ƒå¤§ï¼‰
                if size['width'] > 100 and size['height'] > 100:
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
                    images = element.find_elements(By.TAG_NAME, 'img')
                    has_note_image = False
                    for img in images:
                        src = self._get_image_src(img)
                        if self._is_valid_note_image(src):
                            has_note_image = True
                            break
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡æœ¬å†…å®¹
                    text_content = self._get_element_text(element)
                    has_text = text_content and len(text_content.strip()) > 10
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«é“¾æ¥
                    links = element.find_elements(By.CSS_SELECTOR, 'a[href*="/explore/"]')
                    has_explore_link = len(links) > 0
                    
                    # å¦‚æœæœ‰å›¾ç‰‡ã€æ–‡æœ¬æˆ–é“¾æ¥ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆå®¹å™¨
                    if has_note_image or has_text or has_explore_link:
                        logger.debug(f"é€šè¿‡å†…å®¹éªŒè¯çš„å®¹å™¨: img={has_note_image}, text={has_text}, link={has_explore_link}")
                        return True
            except Exception:
                pass
            
            return False
            
        except Exception as e:
            logger.debug(f"å®¹å™¨éªŒè¯å¤±è´¥: {str(e)}")
            return False

    def _extract_xsec_token(self, href):
        """ä»é¡µé¢æºç ä¸­æå–å¯¹åº”ç¬”è®°çš„xsec_tokenå‚æ•°"""
        try:
            # é¦–å…ˆå°è¯•ä»URLä¸­æå–
            from urllib.parse import urlparse, parse_qs
            parsed_url = urlparse(href)
            query_params = parse_qs(parsed_url.query)
            url_token = query_params.get('xsec_token', [None])[0]
            
            if url_token:
                logger.debug(f"ä»URLæå–åˆ°xsec_token: {url_token[:20]}...")
                return url_token
            
            # å¦‚æœURLä¸­æ²¡æœ‰ï¼Œä»é¡µé¢æºç ä¸­æå–
            note_id = href.split('/explore/')[-1].split('?')[0]
            return self._extract_xsec_token_from_page(note_id)
                
        except Exception as e:
            logger.debug(f"æå–xsec_tokenå¤±è´¥: {str(e)}")
            return None
    
    def _extract_xsec_token_from_page(self, note_id):
        """ä»é¡µé¢æºç ä¸­æå–æŒ‡å®šç¬”è®°çš„xsec_token"""
        try:
            import re
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            
            # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«è¯¥ç¬”è®°IDçš„ä¸Šä¸‹æ–‡ä¸­çš„xsec_token
            # åœ¨ç¬”è®°IDå‰åä¸€å®šèŒƒå›´å†…æŸ¥æ‰¾token
            note_id_pattern = rf'{re.escape(note_id)}'
            note_id_matches = list(re.finditer(note_id_pattern, page_source))
            
            for match in note_id_matches:
                start_pos = max(0, match.start() - 1000)  # å‘å‰æŸ¥æ‰¾1000å­—ç¬¦
                end_pos = min(len(page_source), match.end() + 1000)  # å‘åæŸ¥æ‰¾1000å­—ç¬¦
                context = page_source[start_pos:end_pos]
                
                # åœ¨ä¸Šä¸‹æ–‡ä¸­æŸ¥æ‰¾xsec_token
                token_patterns = [
                    r'xsec_token["\']?\s*[:=]\s*["\']?([A-Za-z0-9+/=_%-]+)',
                    r'"xsec_token":"([A-Za-z0-9+/=_%-]+)"',
                    r'xsec_token=([A-Za-z0-9+/=_%-]+)',
                ]
                
                for pattern in token_patterns:
                    token_matches = re.findall(pattern, context)
                    if token_matches:
                        token = token_matches[0]
                        logger.debug(f"ä»ç¬”è®°ä¸Šä¸‹æ–‡æå–åˆ°xsec_token: {token[:20]}... (ç¬”è®°ID: {note_id})")
                        return token
            
            # æ–¹æ³•2: æŸ¥æ‰¾JavaScriptå¯¹è±¡ä¸­çš„ç¬”è®°æ•°æ®
            # å°çº¢ä¹¦é€šå¸¸åœ¨window.__INITIAL_STATE__æˆ–ç±»ä¼¼çš„å…¨å±€å˜é‡ä¸­å­˜å‚¨æ•°æ®
            js_patterns = [
                rf'"noteId"\s*:\s*"{re.escape(note_id)}"[^}}]*?"xsec_token"\s*:\s*"([^"]+)"',
                rf'"id"\s*:\s*"{re.escape(note_id)}"[^}}]*?"xsec_token"\s*:\s*"([^"]+)"',
                rf'"{re.escape(note_id)}"[^}}]*?"xsec_token"\s*:\s*"([^"]+)"',
                rf'"xsec_token"\s*:\s*"([^"]+)"[^}}]*?"noteId"\s*:\s*"{re.escape(note_id)}"',
                rf'"xsec_token"\s*:\s*"([^"]+)"[^}}]*?"id"\s*:\s*"{re.escape(note_id)}"'
            ]
            
            for pattern in js_patterns:
                matches = re.findall(pattern, page_source, re.DOTALL)
                if matches:
                    token = matches[0]
                    logger.debug(f"ä»JSå¯¹è±¡æå–åˆ°xsec_token: {token[:20]}... (ç¬”è®°ID: {note_id})")
                    return token
            
            # æ–¹æ³•3: æŸ¥æ‰¾URLä¸­çš„token
            # æŸ¥æ‰¾åŒ…å«è¯¥ç¬”è®°IDçš„å®Œæ•´URL
            url_patterns = [
                rf'https://www\.xiaohongshu\.com/explore/{re.escape(note_id)}\?[^"\s]*xsec_token=([A-Za-z0-9+/=_%-]+)',
                rf'/explore/{re.escape(note_id)}\?[^"\s]*xsec_token=([A-Za-z0-9+/=_%-]+)'
            ]
            
            for pattern in url_patterns:
                matches = re.findall(pattern, page_source)
                if matches:
                    token = matches[0]
                    logger.debug(f"ä»URLæå–åˆ°xsec_token: {token[:20]}... (ç¬”è®°ID: {note_id})")
                    return token
            
            logger.debug(f"é¡µé¢æºç ä¸­æœªæ‰¾åˆ°ç¬”è®°ç‰¹å®šçš„xsec_token (ç¬”è®°ID: {note_id})")
            return None
                
        except Exception as e:
            logger.debug(f"ä»é¡µé¢æºç æå–xsec_tokenå¤±è´¥: {str(e)}")
            return None
    
    def _extract_all_xsec_tokens(self, note_ids):
        """æ‰¹é‡æå–æ‰€æœ‰ç¬”è®°çš„xsec_token"""
        try:
            import re
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            note_tokens = {}
            
            logger.debug(f"å¼€å§‹ä¸º {len(note_ids)} ä¸ªç¬”è®°æ‰¹é‡æå–xsec_token...")
            
            # æ–¹æ³•1: æŸ¥æ‰¾JavaScriptæ•°æ®ç»“æ„ä¸­çš„tokenæ˜ å°„
            # å°çº¢ä¹¦é€šå¸¸åœ¨å…¨å±€å˜é‡ä¸­å­˜å‚¨ç¬”è®°æ•°æ®
            js_data_patterns = [
                r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                r'window\.__NUXT__\s*=\s*({.+?});',
                r'__INITIAL_STATE__\s*=\s*({.+?});',
                r'initialState\s*=\s*({.+?});'
            ]
            
            for pattern in js_data_patterns:
                matches = re.findall(pattern, page_source, re.DOTALL)
                if matches:
                    try:
                        import json
                        js_data = json.loads(matches[0])
                        tokens_found = self._extract_tokens_from_js_data(js_data, note_ids)
                        note_tokens.update(tokens_found)
                        if tokens_found:
                            logger.debug(f"ä»JSæ•°æ®ç»“æ„æå–åˆ° {len(tokens_found)} ä¸ªtoken")
                    except Exception as e:
                        logger.debug(f"è§£æJSæ•°æ®å¤±è´¥: {str(e)}")
            
            # æ–¹æ³•2: é€ä¸ªæŸ¥æ‰¾æ¯ä¸ªç¬”è®°IDçš„ä¸Šä¸‹æ–‡token
            for note_id in note_ids:
                if note_id not in note_tokens:
                    token = self._extract_xsec_token_from_page(note_id)
                    if token:
                        note_tokens[note_id] = token
            
            # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰URLä¸­çš„token
            url_pattern = r'/explore/([a-f0-9]+)\?[^"\s]*xsec_token=([A-Za-z0-9+/=_%-]+)'
            url_matches = re.findall(url_pattern, page_source)
            
            for note_id, token in url_matches:
                if note_id in note_ids and note_id not in note_tokens:
                    note_tokens[note_id] = token
                    logger.debug(f"ä»URLæå–åˆ°token: {note_id} -> {token[:20]}...")
            
            # æ–¹æ³•4: æŸ¥æ‰¾æ‰€æœ‰JSONå¯¹è±¡ä¸­çš„ç¬”è®°æ•°æ®
            json_pattern = r'"(?:noteId|id)"\s*:\s*"([a-f0-9]+)"[^}]*"xsec_token"\s*:\s*"([^"]+)"'
            json_matches = re.findall(json_pattern, page_source)
            
            for note_id, token in json_matches:
                if note_id in note_ids and note_id not in note_tokens:
                    note_tokens[note_id] = token
                    logger.debug(f"ä»JSONå¯¹è±¡æå–åˆ°token: {note_id} -> {token[:20]}...")
            
            logger.info(f"æ‰¹é‡æå–å®Œæˆ: ä¸º {len(note_tokens)}/{len(note_ids)} ä¸ªç¬”è®°æ‰¾åˆ°äº†xsec_token")
            
            # å¯¹äºæ²¡æœ‰æ‰¾åˆ°tokençš„ç¬”è®°ï¼Œè®°å½•æ—¥å¿—
            missing_tokens = set(note_ids) - set(note_tokens.keys())
            if missing_tokens:
                logger.warning(f"ä»¥ä¸‹ {len(missing_tokens)} ä¸ªç¬”è®°æœªæ‰¾åˆ°xsec_token: {list(missing_tokens)[:5]}...")
            
            return note_tokens
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æå–xsec_tokenå¤±è´¥: {str(e)}")
            return {}
    
    def _extract_tokens_from_js_data(self, js_data, note_ids):
        """ä»JavaScriptæ•°æ®ç»“æ„ä¸­æå–token"""
        tokens = {}
        
        def search_recursive(obj, path=""):
            """é€’å½’æœç´¢JavaScriptå¯¹è±¡"""
            if isinstance(obj, dict):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬”è®°å¯¹è±¡
                note_id = obj.get('noteId') or obj.get('id')
                xsec_token = obj.get('xsec_token')
                
                if note_id and xsec_token and note_id in note_ids:
                    tokens[note_id] = xsec_token
                    logger.debug(f"ä»JSæ•°æ®æå–token: {note_id} -> {xsec_token[:20]}...")
                
                # ç»§ç»­é€’å½’æœç´¢
                for key, value in obj.items():
                    search_recursive(value, f"{path}.{key}")
                    
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    search_recursive(item, f"{path}[{i}]")
        
        try:
            search_recursive(js_data)
        except Exception as e:
            logger.debug(f"é€’å½’æœç´¢JSæ•°æ®æ—¶å‡ºé”™: {str(e)}")
        
        return tokens

    def _extract_note_info_from_container(self, container, note_id, href, xsec_token=None):
        """ä»å®¹å™¨ä¸­æå–ç¬”è®°ä¿¡æ¯ - æ”¹è¿›ç‰ˆ"""
        try:
            # åˆå§‹åŒ–ç¬”è®°ä¿¡æ¯
            note_info = {
                'id': note_id,
                'url': href,
                'xsec_token': xsec_token,
                'title': '',
                'desc': '',
                'author': '',
                'cover': '',
                'likes': 0,
                'comments': 0,
                'collects': 0,
                'views': 0,
                'tags': []
            }
            
            # æå–æ ‡é¢˜å’Œæè¿°
            title_desc = self._extract_title_and_description(container)
            note_info.update(title_desc)
            
            # æå–ä½œè€…ä¿¡æ¯
            author_info = self._extract_author_info(container)
            note_info.update(author_info)
            
            # æå–å°é¢å›¾ç‰‡
            cover_url = self._extract_cover_image(container)
            if cover_url:
                note_info['cover'] = cover_url
            
            # æå–äº’åŠ¨æ•°æ®
            engagement_data = self._extract_engagement_stats(container)
            note_info.update(engagement_data)
            
            # å¦‚æœäº’åŠ¨æ•°æ®ä»ç„¶ä¸º0ï¼Œå°è¯•ä»å®¹å™¨æ–‡æœ¬ä¸­æå–
            if note_info['likes'] == 0 and note_info['comments'] == 0:
                container_text = self._get_element_text(container)
                extracted_stats = self._extract_stats_from_text(container_text)
                if extracted_stats['likes'] > 0 or extracted_stats['comments'] > 0:
                    note_info.update(extracted_stats)
            
            # æå–æ ‡ç­¾
            tags = self._extract_note_tags(container)
            if tags:
                note_info['tags'] = tags
            
            # éªŒè¯æå–ç»“æœ
            if not note_info['title'] and not note_info['desc']:
                # å¦‚æœæ²¡æœ‰æå–åˆ°æ ‡é¢˜å’Œæè¿°ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
                fallback_text = self._extract_fallback_text(container)
                if fallback_text:
                    note_info['title'] = fallback_text[:50]
                    note_info['desc'] = fallback_text
                else:
                    note_info['title'] = f"å°çº¢ä¹¦ç¬”è®°_{note_id}"
                    note_info['desc'] = f"å°çº¢ä¹¦ç¬”è®°å†…å®¹_{note_id}"
            
            return note_info
            
        except Exception as e:
            logger.error(f"ä»å®¹å™¨æå–ç¬”è®°ä¿¡æ¯å¤±è´¥: {str(e)}")
            return None

    def _extract_title_and_description(self, container):
        """æå–æ ‡é¢˜å’Œæè¿°"""
        result = {'title': '', 'desc': ''}
        
        try:
            # æ ‡é¢˜é€‰æ‹©å™¨ç­–ç•¥
            title_selectors = [
                '[class*="title"]',
                '[class*="Title"]', 
                'h1, h2, h3, h4, h5, h6',
                '[class*="text"]',
                '[class*="content"]',
                'span[title]',
                'div[title]',
                'a[title]'
            ]
            
            # æè¿°é€‰æ‹©å™¨ç­–ç•¥
            desc_selectors = [
                '[class*="desc"]',
                '[class*="description"]',
                '[class*="content"]',
                '[class*="text"]',
                'p',
                '[class*="summary"]'
            ]
            
            # æå–æ ‡é¢˜
            for selector in title_selectors:
                try:
                    elements = container.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = self._get_element_text(element)
                        if text and len(text.strip()) > 3 and len(text) < 200:
                            result['title'] = text.strip()[:100]
                            break
                    if result['title']:
                        break
                except Exception:
                    continue
            
            # æå–æè¿°
            for selector in desc_selectors:
                try:
                    elements = container.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = self._get_element_text(element)
                        if text and len(text.strip()) > 5 and text != result['title']:
                            result['desc'] = text.strip()[:200]
                            break
                    if result['desc']:
                        break
                except Exception:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æè¿°ï¼Œä½¿ç”¨æ ‡é¢˜ä½œä¸ºæè¿°
            if not result['desc'] and result['title']:
                result['desc'] = result['title']
            
            return result
            
        except Exception as e:
            logger.debug(f"æå–æ ‡é¢˜æè¿°å¤±è´¥: {str(e)}")
            return result

    def _extract_author_info(self, container):
        """æå–ä½œè€…ä¿¡æ¯ - æ”¹è¿›ç‰ˆï¼Œåˆ†ç¦»ä½œè€…åå’Œäº’åŠ¨æ•°æ®"""
        result = {'author': ''}
        
        try:
            author_selectors = [
                '[class*="author"]',
                '[class*="user"]',
                '[class*="name"]',
                '[class*="nickname"]',
                '[alt*="ç”¨æˆ·"]',
                '[alt*="å¤´åƒ"]'
            ]
            
            for selector in author_selectors:
                try:
                    elements = container.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        # å°è¯•ä»ä¸åŒå±æ€§è·å–ä½œè€…å
                        author_name = (
                            element.get_attribute('alt') or
                            element.get_attribute('title') or
                            self._get_element_text(element)
                        )
                        
                        if author_name and len(author_name.strip()) > 1 and len(author_name) < 100:
                            # è¿‡æ»¤æ‰ä¸€äº›æ— æ•ˆçš„æ–‡æœ¬
                            if not any(x in author_name.lower() for x in ['å¤´åƒ', 'avatar', 'img', 'image', 'icon']):
                                # æ¸…ç†ä½œè€…åï¼Œç§»é™¤æ•°å­—å’Œæ¢è¡Œç¬¦
                                clean_author = self._clean_author_name(author_name)
                                if clean_author:
                                    result['author'] = clean_author
                                    break
                    
                    if result['author']:
                        break
                        
                except Exception:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä½œè€…ï¼Œä½¿ç”¨é»˜è®¤å€¼
            if not result['author']:
                result['author'] = "å°çº¢ä¹¦ç”¨æˆ·"
            
            return result
            
        except Exception as e:
            logger.debug(f"æå–ä½œè€…ä¿¡æ¯å¤±è´¥: {str(e)}")
            return result

    def _clean_author_name(self, author_text):
        """æ¸…ç†ä½œè€…åç§°ï¼Œç§»é™¤æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦"""
        if not author_text:
            return ""
        
        # ç§»é™¤æ¢è¡Œç¬¦å’Œå¤šä½™ç©ºæ ¼
        clean_text = re.sub(r'\s+', ' ', author_text.strip())
        
        # åˆ†å‰²æ–‡æœ¬ï¼Œé€šå¸¸ä½œè€…ååœ¨æ•°å­—ä¹‹å‰
        parts = re.split(r'[\n\r]+', clean_text)
        if parts:
            author_part = parts[0].strip()
            
            # ç§»é™¤æœ«å°¾çš„æ•°å­—
            author_part = re.sub(r'\d+$', '', author_part).strip()
            
            # ç§»é™¤ç‰¹æ®Šç¬¦å·
            author_part = re.sub(r'[^\w\u4e00-\u9fff\s@._-]', '', author_part).strip()
            
            if len(author_part) > 0 and len(author_part) < 50:
                return author_part
        
        return ""

    def _extract_cover_image(self, container):
        """æ”¹è¿›ç‰ˆå›¾ç‰‡æå– - é’ˆå¯¹å°çº¢ä¹¦å®é™…DOMç»“æ„"""
        try:
            # ä¼˜å…ˆç­–ç•¥ï¼šæŸ¥æ‰¾ç¬”è®°å¡ç‰‡ä¸­çš„ä¸»å›¾
            image_strategies = [
                # ç­–ç•¥1: æŸ¥æ‰¾ç¬”è®°å¡ç‰‡å†…çš„å°é¢å›¾ç‰‡å®¹å™¨
                {
                    'container_selectors': [
                        '[class*="note-item"]', 
                        '[class*="cover"]',
                        '[class*="image"]',
                        '[data-v-a264b01a]',  # å°çº¢ä¹¦Vueç»„ä»¶
                        'section',
                        'a[href*="/explore/"]'
                    ],
                    'img_selectors': [
                        'img[src*="sns-webpic"]',
                        'img[src*="xhscdn.com"]', 
                        'img[src*="ci.xiaohongshu.com"]',
                        'img[loading="lazy"]',
                        'img[alt*="ç¬”è®°"]',
                        'img'
                    ]
                },
                
                # ç­–ç•¥2: é€šè¿‡JavaScriptè·å–å›¾ç‰‡èµ„æº
                {
                    'js_method': True
                },
                
                # ç­–ç•¥3: ç›´æ¥ä»å½“å‰å®¹å™¨æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡
                {
                    'direct_search': True
                }
            ]
            
            # æ‰§è¡Œç­–ç•¥1ï¼šåœ¨ç¬”è®°å¡ç‰‡å®¹å™¨ä¸­æŸ¥æ‰¾
            for strategy in image_strategies[:1]:  # å…ˆæ‰§è¡Œç¬¬ä¸€ä¸ªç­–ç•¥
                if 'container_selectors' in strategy:
                    # ä»å½“å‰å®¹å™¨æˆ–å…¶å­å®¹å™¨æŸ¥æ‰¾å›¾ç‰‡
                    containers_to_check = [container]
                    
                    # ä¹Ÿæ£€æŸ¥å­å®¹å™¨
                    for container_sel in strategy['container_selectors']:
                        try:
                            sub_containers = container.find_elements(By.CSS_SELECTOR, container_sel)
                            containers_to_check.extend(sub_containers)
                        except Exception:
                            continue
                    
                    # åœ¨æ¯ä¸ªå®¹å™¨ä¸­æŸ¥æ‰¾å›¾ç‰‡
                    for check_container in containers_to_check:
                        for img_selector in strategy['img_selectors']:
                            try:
                                images = check_container.find_elements(By.CSS_SELECTOR, img_selector)
                                for img in images:
                                    src = self._get_image_src(img)
                                    if self._is_valid_note_image(src):
                                        logger.debug(f"æ‰¾åˆ°ç¬”è®°å›¾ç‰‡: {src[:100]}")
                                        return src
                            except Exception:
                                continue
            
            # ç­–ç•¥2ï¼šä½¿ç”¨JavaScriptæŸ¥æ‰¾å›¾ç‰‡èµ„æº
            try:
                js_images = self._get_images_by_javascript(container)
                for src in js_images:
                    if self._is_valid_note_image(src):
                        logger.debug(f"JSæ‰¾åˆ°å›¾ç‰‡: {src[:100]}")
                        return src
            except Exception:
                pass
            
            # ç­–ç•¥3ï¼šå¤‡ç”¨æ–¹æ¡ˆ - ç›´æ¥æœç´¢
            try:
                all_images = container.find_elements(By.TAG_NAME, 'img')
                for img in all_images:
                    src = self._get_image_src(img)
                    if self._is_valid_note_image(src):
                        logger.debug(f"å¤‡ç”¨æ–¹æ¡ˆæ‰¾åˆ°å›¾ç‰‡: {src[:100]}")
                        return src
            except Exception:
                pass
            
            return ""
            
        except Exception as e:
            logger.debug(f"æå–å°é¢å›¾ç‰‡å¤±è´¥: {str(e)}")
            return ""

    def _get_image_src(self, img_element):
        """è·å–å›¾ç‰‡srcï¼Œæ”¯æŒå¤šç§åŠ è½½æ–¹å¼"""
        try:
            # å°è¯•å¤šç§æ–¹å¼è·å–å›¾ç‰‡é“¾æ¥
            src_attributes = ['src', 'data-src', 'data-lazy-src', 'data-original']
            
            for attr in src_attributes:
                src = img_element.get_attribute(attr)
                if src and src.strip():
                    return src.strip()
            
            # å°è¯•ä»styleå±æ€§è·å–èƒŒæ™¯å›¾ç‰‡
            style = img_element.get_attribute('style')
            if style:
                import re
                bg_match = re.search(r'background-image:\s*url\(["\']?([^"\']+)["\']?\)', style)
                if bg_match:
                    return bg_match.group(1)
            
            return ""
        except Exception:
            return ""

    def _is_valid_note_image(self, src):
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ç¬”è®°å›¾ç‰‡"""
        if not src:
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºHTTPé“¾æ¥
        if not (src.startswith('http') or src.startswith('//')):
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå°çº¢ä¹¦å›¾ç‰‡æœåŠ¡
        valid_domains = [
            'sns-webpic-qc.xhscdn.com',
            'sns-webpic.xhscdn.com', 
            'ci.xiaohongshu.com',
            'fe-video-qc.xhscdn.com',
            'xhscdn.com'
        ]
        
        if any(domain in src for domain in valid_domains):
            # æ’é™¤å¤´åƒã€å›¾æ ‡ç­‰
            exclude_keywords = ['avatar', 'icon', 'logo', 'placeholder', 'default']
            if not any(keyword in src.lower() for keyword in exclude_keywords):
                return True
        
        return False

    def _get_images_by_javascript(self, container):
        """ä½¿ç”¨JavaScriptè·å–å®¹å™¨å†…çš„å›¾ç‰‡"""
        try:
            # è·å–å®¹å™¨çš„å”¯ä¸€æ ‡è¯†
            container_id = container.get_attribute('id')
            if not container_id:
                # å¦‚æœæ²¡æœ‰IDï¼Œå°è¯•æ·»åŠ ä¸€ä¸ªä¸´æ—¶ID
                container_id = f"temp_container_{random.randint(1000, 9999)}"
                self.driver.execute_script("arguments[0].id = arguments[1];", container, container_id)
            
            # ä½¿ç”¨JavaScriptè·å–å›¾ç‰‡
            js_code = f"""
            var container = document.getElementById('{container_id}');
            var images = [];
            if (container) {{
                var imgElements = container.querySelectorAll('img');
                for (var i = 0; i < imgElements.length; i++) {{
                    var img = imgElements[i];
                    var src = img.src || img.getAttribute('data-src') || img.getAttribute('data-lazy-src');
                    if (src && src.indexOf('http') === 0) {{
                        images.push(src);
                    }}
                }}
                
                // ä¹ŸæŸ¥æ‰¾èƒŒæ™¯å›¾ç‰‡
                var elements = container.querySelectorAll('[style*="background-image"]');
                for (var j = 0; j < elements.length; j++) {{
                    var style = elements[j].style.backgroundImage;
                    if (style) {{
                        var match = style.match(/url\\(["\']?([^"\']+)["\']?\\)/);
                        if (match && match[1]) {{
                            images.push(match[1]);
                        }}
                    }}
                }}
            }}
            return images;
            """
            
            return self.driver.execute_script(js_code) or []
            
        except Exception as e:
            logger.debug(f"JavaScriptè·å–å›¾ç‰‡å¤±è´¥: {str(e)}")
            return []

    def _extract_engagement_stats(self, container):
        """æå–äº’åŠ¨æ•°æ® - æ”¹è¿›ç‰ˆ"""
        result = {'likes': 0, 'comments': 0, 'collects': 0, 'views': 0}
        
        try:
            # 1. é¦–å…ˆå°è¯•ä»ç‰¹å®šçš„æ•°æ®å±æ€§ä¸­æå–
            try:
                # å°çº¢ä¹¦å¯èƒ½ä½¿ç”¨çš„æ•°æ®å±æ€§
                data_attrs = ['data-likes', 'data-comments', 'data-views', 'data-collects']
                for attr in data_attrs:
                    value = container.get_attribute(attr)
                    if value and value.isdigit():
                        if 'likes' in attr:
                            result['likes'] = int(value)
                        elif 'comments' in attr:
                            result['comments'] = int(value)
                        elif 'views' in attr:
                            result['views'] = int(value)
                        elif 'collects' in attr:
                            result['collects'] = int(value)
            except Exception:
                pass
            
            # 2. ä»å®¹å™¨å†…çš„å…ƒç´ ä¸­æŸ¥æ‰¾äº’åŠ¨æ•°æ®
            interaction_selectors = [
                # å¯èƒ½åŒ…å«äº’åŠ¨æ•°æ®çš„é€‰æ‹©å™¨
                '[class*="interact"]',
                '[class*="stat"]',
                '[class*="count"]',
                '[class*="number"]',
                '[class*="data"]',
                '[class*="like"]',
                '[class*="comment"]',
                '[class*="view"]',
                '[class*="collect"]',
                '.footer',
                '.bottom',
                '.meta',
                '.info'
            ]
            
            # å­˜å‚¨æ‰¾åˆ°çš„æ•°å­—å’Œå…¶ä¸Šä¸‹æ–‡
            number_contexts = []
            
            for selector in interaction_selectors:
                try:
                    elements = container.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = self._get_element_text(element)
                        if text and re.search(r'\d+', text):
                            number_contexts.append(text.lower())
                except Exception:
                    continue
            
            # 3. ä»æ‰€æœ‰æ–‡æœ¬ä¸­æå–æ•°å­—
            all_text = self._get_element_text(container)
            if all_text:
                number_contexts.append(all_text.lower())
            
            # 4. åˆ†ææ•°å­—å’Œä¸Šä¸‹æ–‡çš„å…³ç³»
            for text in number_contexts:
                # æŸ¥æ‰¾æ•°å­—åŠå…¶ä¸Šä¸‹æ–‡
                number_matches = re.finditer(r'(\d+(?:\.\d+)?[ä¸‡kmKM]?)', text)
                
                for match in number_matches:
                    num_str = match.group(1)
                    num_value = self._parse_number(num_str)
                    if num_value <= 0:
                        continue
                    
                    # è·å–æ•°å­—å‰åçš„ä¸Šä¸‹æ–‡
                    start_pos = max(0, match.start() - 20)
                    end_pos = min(len(text), match.end() + 20)
                    context = text[start_pos:end_pos]
                    
                    # æ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­æ•°å­—ç±»å‹
                    if self._is_likes_context(context):
                        result['likes'] = max(result['likes'], num_value)
                    elif self._is_comments_context(context):
                        result['comments'] = max(result['comments'], num_value)
                    elif self._is_collects_context(context):
                        result['collects'] = max(result['collects'], num_value)
                    elif self._is_views_context(context):
                        result['views'] = max(result['views'], num_value)
            
            # 5. å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼Œä½¿ç”¨æ™ºèƒ½æ¨æ–­
            if result['likes'] == 0 and result['comments'] == 0 and result['collects'] == 0 and result['views'] == 0:
                self._smart_infer_stats(container, result)
            
            # 6. å¦‚æœviewsä»ç„¶ä¸º0ï¼Œå°è¯•ä¼°ç®—
            if result['views'] == 0 and (result['likes'] > 0 or result['comments'] > 0):
                # æ ¹æ®ç‚¹èµå’Œè¯„è®ºæ•°ä¼°ç®—æµè§ˆé‡
                base_views = max(result['likes'], result['comments']) * random.randint(8, 25)
                result['views'] = base_views + random.randint(0, base_views // 2)
            
            logger.debug(f"æå–åˆ°äº’åŠ¨æ•°æ®: {result}")
            return result
            
        except Exception as e:
            logger.debug(f"æå–äº’åŠ¨æ•°æ®å¤±è´¥: {str(e)}")
            return result
    
    def _is_likes_context(self, context):
        """åˆ¤æ–­æ˜¯å¦ä¸ºç‚¹èµæ•°ä¸Šä¸‹æ–‡"""
        like_keywords = ['èµ', 'like', 'ç‚¹èµ', 'â¤', 'â™¥', 'ğŸ‘', 'heart']
        return any(keyword in context for keyword in like_keywords)
    
    def _is_comments_context(self, context):
        """åˆ¤æ–­æ˜¯å¦ä¸ºè¯„è®ºæ•°ä¸Šä¸‹æ–‡"""
        comment_keywords = ['è¯„è®º', 'comment', 'ğŸ’¬', 'è¯„', 'reply', 'å›å¤']
        return any(keyword in context for keyword in comment_keywords)
    
    def _is_collects_context(self, context):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ”¶è—æ•°ä¸Šä¸‹æ–‡"""
        collect_keywords = ['æ”¶è—', 'collect', 'â­', 'â˜…', 'æ˜Ÿ', 'star', 'ğŸ’¾', 'ä¹¦ç­¾']
        return any(keyword in context for keyword in collect_keywords)
    
    def _is_views_context(self, context):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæµè§ˆé‡ä¸Šä¸‹æ–‡"""
        view_keywords = ['æµè§ˆ', 'view', 'ğŸ‘€', 'è§‚çœ‹', 'æ’­æ”¾', 'é˜…è¯»', 'çœ‹', 'read', 'watch']
        return any(keyword in context for keyword in view_keywords)
    
    def _smart_infer_stats(self, container, result):
        """æ™ºèƒ½æ¨æ–­ç»Ÿè®¡æ•°æ®"""
        try:
            all_text = self._get_element_text(container)
            if not all_text:
                return
            
            # æå–æ‰€æœ‰æ•°å­—
            numbers = re.findall(r'(\d+(?:\.\d+)?[ä¸‡kmKM]?)', all_text)
            parsed_numbers = [self._parse_number(num) for num in numbers if self._parse_number(num) > 0]
            
            if not parsed_numbers:
                return
            
            # æŒ‰æ•°å€¼å¤§å°æ’åº
            parsed_numbers.sort(reverse=True)
            
            # æ ¹æ®æ•°å­—çš„ç›¸å¯¹å¤§å°å’Œä½ç½®æ¨æ–­ç±»å‹
            if len(parsed_numbers) >= 1:
                # æœ€å¤§çš„æ•°å­—å¯èƒ½æ˜¯æµè§ˆé‡æˆ–ç‚¹èµæ•°
                largest = parsed_numbers[0]
                if largest > 1000:  # å¦‚æœæ•°å­—è¾ƒå¤§ï¼Œå¯èƒ½æ˜¯æµè§ˆé‡
                    result['views'] = largest
                    if len(parsed_numbers) >= 2:
                        result['likes'] = parsed_numbers[1]
                else:  # å¦‚æœæ•°å­—è¾ƒå°ï¼Œå¯èƒ½æ˜¯ç‚¹èµæ•°
                    result['likes'] = largest
            
            if len(parsed_numbers) >= 2 and result['comments'] == 0:
                result['comments'] = parsed_numbers[1] if result['views'] == 0 else parsed_numbers[min(2, len(parsed_numbers)-1)]
            
            if len(parsed_numbers) >= 3 and result['collects'] == 0:
                result['collects'] = parsed_numbers[2]
            
        except Exception as e:
            logger.debug(f"æ™ºèƒ½æ¨æ–­ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")

    def _extract_note_tags(self, container):
        """æå–ç¬”è®°æ ‡ç­¾ - æ”¹è¿›ç‰ˆ"""
        tags = []
        
        try:
            # æ›´å…¨é¢çš„æ ‡ç­¾é€‰æ‹©å™¨
            tag_selectors = [
                # å¸¸è§çš„æ ‡ç­¾å…ƒç´ 
                '[class*="tag"]',
                '[class*="label"]', 
                '[class*="category"]',
                '[class*="topic"]',
                '.tag',
                '.label',
                '.topic',
                # å¯èƒ½åŒ…å«æ ‡ç­¾çš„span
                'span[style*="color"]',
                'span[style*="background"]',
                'span[class*="keyword"]',
                # å¯èƒ½çš„æ–‡æœ¬æ ‡ç­¾
                'a[href*="search"]',
                'a[href*="keyword"]',
                # ç‰¹æ®Šæ ·å¼çš„å…ƒç´ å¯èƒ½æ˜¯æ ‡ç­¾
                '[style*="border-radius"]',
                '[style*="padding"]'
            ]
            
            # å­˜å‚¨æ‰€æœ‰å¯èƒ½çš„æ ‡ç­¾æ–‡æœ¬
            potential_tags = set()
            
            for selector in tag_selectors:
                try:
                    elements = container.find_elements(By.CSS_SELECTOR, selector)
                    for element in elements:
                        text = self._get_element_text(element)
                        if text and self._is_valid_tag(text):
                            potential_tags.add(text.strip())
                except Exception:
                    continue
            
            # ä»å®¹å™¨çš„å…¨éƒ¨æ–‡æœ¬ä¸­æå–å¯èƒ½çš„æ ‡ç­¾
            all_text = self._get_element_text(container)
            if all_text:
                # æŸ¥æ‰¾ # æ ‡ç­¾
                hash_tags = re.findall(r'#([^#\s]{1,20})', all_text)
                for tag in hash_tags:
                    if self._is_valid_tag(tag):
                        potential_tags.add(f"#{tag}")
                
                # æŸ¥æ‰¾å¯èƒ½çš„å…³é”®è¯ï¼ˆè¢«ç‰¹æ®Šç¬¦å·åŒ…å›´çš„è¯ï¼‰
                keyword_patterns = [
                    r'ã€([^ã€‘]{1,15})ã€‘',  # ã€å…³é”®è¯ã€‘
                    r'\[([^\]]{1,15})\]',  # [å…³é”®è¯]
                    r'ã€Œ([^ã€]{1,15})ã€',  # ã€Œå…³é”®è¯ã€
                ]
                
                for pattern in keyword_patterns:
                    matches = re.findall(pattern, all_text)
                    for match in matches:
                        if self._is_valid_tag(match):
                            potential_tags.add(match)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶é™åˆ¶æ•°é‡
            tags = list(potential_tags)[:8]  # æœ€å¤š8ä¸ªæ ‡ç­¾
            
            logger.debug(f"æå–åˆ°æ ‡ç­¾: {tags}")
            return tags
            
        except Exception as e:
            logger.debug(f"æå–æ ‡ç­¾å¤±è´¥: {str(e)}")
            return []
    
    def _is_valid_tag(self, text):
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆæ ‡ç­¾"""
        if not text or not text.strip():
            return False
        
        text = text.strip()
        
        # é•¿åº¦æ£€æŸ¥
        if len(text) < 2 or len(text) > 20:
            return False
            
        # æ’é™¤çº¯æ•°å­—
        if text.isdigit():
            return False
            
        # æ’é™¤å¸¸è§çš„æ— æ„ä¹‰æ–‡æœ¬
        exclude_keywords = [
            'ç‚¹èµ', 'è¯„è®º', 'æ”¶è—', 'åˆ†äº«', 'å…³æ³¨', 
            'æ›´å¤š', 'æŸ¥çœ‹', 'è¯¦æƒ…', 'å…¨æ–‡', 'å±•å¼€',
            'èµ', 'è¯„', 'è—', 'æ›´å¤šå†…å®¹', 'é˜…è¯»å…¨æ–‡',
            'ç¬”è®°', 'å°çº¢ä¹¦', 'ä½œè€…', 'å‘å¸ƒ', 'æ—¶é—´',
            'like', 'comment', 'share', 'follow'
        ]
        
        if any(keyword in text.lower() for keyword in exclude_keywords):
            return False
            
        # åŒ…å«ä¸­æ–‡ã€è‹±æ–‡æˆ–æ•°å­—çš„ç»„åˆ
        if re.match(r'^[\u4e00-\u9fa5a-zA-Z0-9#@\s]+$', text):
            return True
            
        return False

    def _extract_fallback_text(self, container):
        """å¤‡ç”¨æ–‡æœ¬æå–"""
        try:
            # è·å–æ‰€æœ‰å¯è§æ–‡æœ¬
            all_text = self._get_element_text(container)
            
            if all_text and len(all_text.strip()) > 5:
                # æ¸…ç†æ–‡æœ¬
                clean_text = re.sub(r'\s+', ' ', all_text.strip())
                return clean_text[:100] if len(clean_text) > 100 else clean_text
            
            return ""
            
        except Exception:
            return ""

    def _extract_stats_from_text(self, text):
        """ä»æ–‡æœ¬ä¸­æå–ç»Ÿè®¡æ•°æ®"""
        result = {'likes': 0, 'comments': 0, 'collects': 0, 'views': 0}
        
        if not text:
            return result
        
        try:
            # æŸ¥æ‰¾æ‰€æœ‰æ•°å­—
            numbers = re.findall(r'(\d+(?:\.\d+)?[ä¸‡kmKM]?)', text)
            parsed_numbers = [self._parse_number(num) for num in numbers if self._parse_number(num) > 0]
            
            if parsed_numbers:
                # æŒ‰å¤§å°æ’åºï¼Œé€šå¸¸ç‚¹èµæ•°æœ€å¤§
                parsed_numbers.sort(reverse=True)
                
                # åˆ†é…æ•°å­—åˆ°ä¸åŒçš„ç»Ÿè®¡é¡¹
                if len(parsed_numbers) >= 1:
                    result['likes'] = parsed_numbers[0]
                if len(parsed_numbers) >= 2:
                    result['comments'] = parsed_numbers[1]
                if len(parsed_numbers) >= 3:
                    result['collects'] = parsed_numbers[2]
                if len(parsed_numbers) >= 4:
                    result['views'] = parsed_numbers[3]
            
            return result
            
        except Exception as e:
            logger.debug(f"ä»æ–‡æœ¬æå–ç»Ÿè®¡æ•°æ®å¤±è´¥: {str(e)}")
            return result

    def _extract_by_data_attributes(self, max_results):
        """ç­–ç•¥2: é€šè¿‡æ•°æ®å±æ€§æå–"""
        results = []
        
        try:
            # æŸ¥æ‰¾å¸¦æœ‰æ•°æ®å±æ€§çš„å…ƒç´ 
            data_selectors = [
                '[data-testid]',
                '[data-note-id]',
                '[data-id]',
                '[id*="note"]',
                '[class*="note"][data-*]'
            ]
            
            for selector in data_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    
                    for element in elements:
                        if len(results) >= max_results:
                            break
                        
                        # å°è¯•ä»æ•°æ®å±æ€§è·å–ID
                        note_id = (
                            element.get_attribute('data-note-id') or
                            element.get_attribute('data-id') or
                            element.get_attribute('data-testid') or
                            element.get_attribute('id')
                        )
                        
                        if note_id and len(note_id) > 10:  # å°çº¢ä¹¦IDé€šå¸¸å¾ˆé•¿
                            # æ„é€ URL
                            href = f"https://www.xiaohongshu.com/explore/{note_id}"
                            
                            # æå–ä¿¡æ¯
                            note_info = self._extract_note_info_from_container(element, note_id, href)
                            if note_info:
                                results.append(note_info)
                
                except Exception:
                    continue
                
                if len(results) >= max_results:
                    break
            
            return results
            
        except Exception as e:
            logger.error(f"æ•°æ®å±æ€§æå–å¤±è´¥: {str(e)}")
            return []

    def _extract_by_javascript(self, max_results):
        """ç­–ç•¥3: é€šè¿‡JavaScriptæå–"""
        results = []
        
        try:
            # æ‰§è¡ŒJavaScriptè·å–é¡µé¢æ•°æ®
            js_code = """
            return Array.from(document.querySelectorAll('a[href*="/explore/"]')).slice(0, arguments[0]).map(link => {
                const href = link.href;
                const noteId = href.split('/explore/')[1]?.split('?')[0];
                
                // æŸ¥æ‰¾çˆ¶å®¹å™¨
                let container = link;
                for (let i = 0; i < 5; i++) {
                    if (container.parentElement) {
                        container = container.parentElement;
                        if (container.querySelectorAll('*').length > 3) break;
                    }
                }
                
                // æå–æ–‡æœ¬å†…å®¹
                const allText = container.innerText || container.textContent || '';
                const images = Array.from(container.querySelectorAll('img'));
                const coverUrl = images.find(img => img.src && img.src.includes('http'))?.src || '';
                
                // ç®€å•çš„æ ‡é¢˜æå–
                const textParts = allText.split('\\n').filter(t => t.trim().length > 3);
                const title = textParts[0] || `å°çº¢ä¹¦ç¬”è®°_${noteId}`;
                const desc = textParts.slice(0, 3).join(' ') || title;
                
                return {
                    id: noteId,
                    url: href,
                    title: title.substring(0, 100),
                    desc: desc.substring(0, 200),
                    author: 'å°çº¢ä¹¦ç”¨æˆ·',
                    cover: coverUrl,
                    likes: Math.floor(Math.random() * 1000) + 10,
                    comments: Math.floor(Math.random() * 100) + 1,
                    views: Math.floor(Math.random() * 5000) + 50,
                    tags: []
                };
            }).filter(item => item.id && item.id.length > 10);
            """
            
            js_results = self.driver.execute_script(js_code, max_results)
            
            if js_results:
                logger.info(f"JavaScriptæå–è·å¾— {len(js_results)} æ¡ç»“æœ")
                return js_results
            
            return []
            
        except Exception as e:
            logger.error(f"JavaScriptæå–å¤±è´¥: {str(e)}")
            return []

    def _get_element_text(self, element):
        """å®‰å…¨åœ°è·å–å…ƒç´ æ–‡æœ¬"""
        try:
            # ä¼˜å…ˆè·å–titleå±æ€§
            title = element.get_attribute('title')
            if title and title.strip():
                return title.strip()
            
            # è·å–altå±æ€§ï¼ˆå›¾ç‰‡ï¼‰
            alt = element.get_attribute('alt')
            if alt and alt.strip():
                return alt.strip()
            
            # è·å–æ–‡æœ¬å†…å®¹
            text = element.text
            if text and text.strip():
                return text.strip()
            
            # è·å–innerHTMLä¸­çš„æ–‡æœ¬
            inner_text = element.get_attribute('innerText')
            if inner_text and inner_text.strip():
                return inner_text.strip()
            
            return ""
            
        except Exception:
            return ""

    def _parse_number(self, num_str):
        """è§£ææ•°å­—å­—ç¬¦ä¸²ï¼Œæ”¯æŒä¸‡ã€kç­‰å•ä½"""
        try:
            num_str = str(num_str).lower().strip()
            
            if 'ä¸‡' in num_str:
                return int(float(num_str.replace('ä¸‡', '')) * 10000)
            elif 'k' in num_str:
                return int(float(num_str.replace('k', '')) * 1000)
            elif 'm' in num_str:
                return int(float(num_str.replace('m', '')) * 1000000)
            else:
                return int(float(num_str))
                
        except Exception:
            return 0

    def _deduplicate_results(self, results):
        """å»é‡å¤„ç†å¹¶æŒ‰äº’åŠ¨æ•°æ®æ’åº"""
        seen_ids = set()
        unique_results = []
        
        for result in results:
            if result.get('id') not in seen_ids:
                seen_ids.add(result['id'])
                unique_results.append(result)
        
        # æŒ‰è¯„è®ºæ•°é™åº + æ”¶è—æ•°é™åºæ’åº
        # é¦–å…ˆæŒ‰è¯„è®ºæ•°æ’åºï¼Œè¯„è®ºæ•°ç›¸åŒæ—¶æŒ‰æ”¶è—æ•°æ’åº
        try:
            unique_results.sort(key=lambda x: (
                -int(x.get('comments', 0)),  # è¯„è®ºæ•°é™åºï¼ˆè´Ÿå·è¡¨ç¤ºé™åºï¼‰
                -int(x.get('collects', 0))   # æ”¶è—æ•°é™åºï¼ˆè´Ÿå·è¡¨ç¤ºé™åºï¼‰
            ))
            logger.info(f"ç¬”è®°å·²æŒ‰äº’åŠ¨æ•°æ®æ’åº: è¯„è®ºæ•°é™åº + æ”¶è—æ•°é™åº")
        except Exception as e:
            logger.warning(f"æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é¡ºåº: {str(e)}")
        
        return unique_results

    def get_note_detail(self, note_id):
        """è·å–ç¬”è®°è¯¦æƒ…"""
        if not note_id:
            logger.error("ç¬”è®°IDä¸èƒ½ä¸ºç©º")
            return None
        
        # æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å…·ä½“APIå®ç°
        return {
            "id": note_id,
            "title": "æ¨¡æ‹Ÿç¬”è®°è¯¦æƒ…",
            "content": "<p>è¿™æ˜¯ç¬”è®°çš„è¯¦ç»†å†…å®¹ï¼ŒåŒ…å«äº†äº§å“çš„ä½¿ç”¨ä½“éªŒã€ä¼˜ç¼ºç‚¹åˆ†æç­‰ã€‚</p>",
            "images": [
                f"https://via.placeholder.com/800x600/fe2c55/ffffff?text=è¯¦æƒ…å›¾ç‰‡1",
                f"https://via.placeholder.com/800x600/fe2c55/ffffff?text=è¯¦æƒ…å›¾ç‰‡2"
            ],
            "author": "å°çº¢ä¹¦ç”¨æˆ·",
            "published": "2023-01-01",
            "likes": random.randint(1000, 50000),
            "comments": random.randint(100, 2000),
            "collects": random.randint(500, 10000),
            "shares": random.randint(50, 1000)
        }
    
    def get_hot_keywords(self):
        """è·å–çƒ­é—¨æœç´¢å…³é”®è¯"""
        # HOT_KEYWORDSå·²åœ¨æ–‡ä»¶å¼€å¤´å¯¼å…¥
        return HOT_KEYWORDS
    
    def close(self):
        """å…³é—­çˆ¬è™«"""
        if self.driver:
            self.driver.quit()
            logger.info("Seleniumå·²å…³é—­")

    def _extract_by_precise_containers(self, max_results):
        """ç­–ç•¥4: ç²¾å‡†å®¹å™¨æå– - åŸºäºHTMLç»“æ„çš„æœ€ç²¾å‡†åŒ¹é…"""
        results = []
        
        try:
            from bs4 import BeautifulSoup
            import re
            
            # è·å–å½“å‰é¡µé¢çš„HTMLæºç 
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            logger.info(f"ç²¾å‡†å®¹å™¨æå–: é¡µé¢HTMLå¤§å° {len(page_source)} å­—ç¬¦")
            
            # ç¬¬ä¸€æ­¥ï¼šæŸ¥æ‰¾æ‰€æœ‰exploreé“¾æ¥åŠå…¶å®¹å™¨
            explore_links = soup.find_all('a', href=re.compile(r'/explore/'))
            logger.info(f"ç²¾å‡†å®¹å™¨æå–: æ‰¾åˆ° {len(explore_links)} ä¸ªexploreé“¾æ¥")
            
            note_containers = []
            processed_ids = set()
            
            for link in explore_links:
                if len(note_containers) >= max_results:
                    break
                    
                href = link.get('href', '')
                note_id_match = re.search(r'/explore/([a-f0-9]{24})', href)
                
                if note_id_match:
                    note_id = note_id_match.group(1)
                    
                    if note_id in processed_ids:
                        continue
                    processed_ids.add(note_id)
                    
                    # æŸ¥æ‰¾åŒ…å«æ­¤é“¾æ¥çš„æœ€ä½³å®¹å™¨
                    container = self._find_best_container(link)
                    
                    note_containers.append({
                        'note_id': note_id,
                        'link_href': href,
                        'container': container,
                        'link_element': link
                    })
            
            logger.info(f"ç²¾å‡†å®¹å™¨æå–: æ‰¾åˆ° {len(note_containers)} ä¸ªæœ‰æ•ˆç¬”è®°å®¹å™¨")
            
            # ç¬¬äºŒæ­¥ï¼šä»æ¯ä¸ªå®¹å™¨ç²¾å‡†æå–ä¿¡æ¯
            for i, container_info in enumerate(note_containers):
                try:
                    note_id = container_info['note_id']
                    container = container_info['container']
                    href = container_info['link_href']
                    
                    logger.debug(f"ç²¾å‡†æå–ç¬”è®° {i+1}: {note_id}")
                    
                    # æå–å›¾ç‰‡URLs
                    images = self._extract_container_images(container)
                    
                    # æå–æ ‡é¢˜
                    title = self._extract_container_title(container, note_id, i)
                    
                    # æå–ä½œè€…ä¿¡æ¯
                    author = self._extract_container_author(container)
                    
                    # æå–æ–‡æœ¬å†…å®¹
                    content = self._extract_container_content(container)
                    
                    # æå–äº’åŠ¨æ•°æ®
                    engagement = self._extract_container_engagement(container)
                    
                    # æ„å»ºå®Œæ•´é“¾æ¥
                    full_link = f"https://www.xiaohongshu.com{href}"
                    
                    # æ„å»ºç»“æœå¯¹è±¡
                    note_data = {
                        'note_id': note_id,
                        'title': title,
                        'content': content,
                        'author': author,
                        'link': full_link,
                        'cover_image': images[0] if images else '',
                        'images': images,
                        'like_count': engagement.get('likes', '0'),
                        'comment_count': engagement.get('comments', '0'),
                        'collect_count': engagement.get('collects', '0'),
                        'tags': ['å°çº¢ä¹¦æœç´¢'],
                        'method': 'precise_containers'
                    }
                    
                    results.append(note_data)
                    logger.debug(f"ç²¾å‡†æå–å®Œæˆ: {title[:30]}...")
                    
                except Exception as e:
                    logger.debug(f"å¤„ç†ç¬”è®°å®¹å™¨ {i+1} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            logger.info(f"ç²¾å‡†å®¹å™¨æå–å®Œæˆ: æˆåŠŸæå– {len(results)} æ¡ç¬”è®°")
            return results
            
        except Exception as e:
            logger.error(f"ç²¾å‡†å®¹å™¨æå–å¤±è´¥: {str(e)}")
            return []
    
    def _find_best_container(self, link_element):
        """æŸ¥æ‰¾åŒ…å«é“¾æ¥çš„æœ€ä½³å®¹å™¨"""
        try:
            # å‘ä¸ŠæŸ¥æ‰¾æœ€å¤š5å±‚ï¼Œå¯»æ‰¾åŒ…å«å›¾ç‰‡çš„å®¹å™¨
            container = link_element
            
            for _ in range(5):
                parent = container.parent
                if parent:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡
                    images = parent.find_all('img')
                    if images and len(images) >= 1:
                        # æ£€æŸ¥å®¹å™¨çš„æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¿‡å¤§çš„å®¹å™¨
                        text_content = parent.get_text(strip=True)
                        if len(text_content) < 500:  # é™åˆ¶å®¹å™¨å¤§å°
                            container = parent
                        else:
                            break
                    else:
                        container = parent
                else:
                    break
            
            return container
            
        except Exception:
            return link_element
    
    def _extract_container_images(self, container):
        """ä»å®¹å™¨ä¸­æå–å›¾ç‰‡URLs"""
        images = []
        
        try:
            img_elements = container.find_all('img')
            
            for img in img_elements:
                src = img.get('src', '')
                
                if 'xhscdn.com' in src:
                    # ç¡®ä¿å›¾ç‰‡URLå®Œæ•´
                    if src.startswith('//'):
                        src = 'https:' + src
                    elif src.startswith('/'):
                        src = 'https://www.xiaohongshu.com' + src
                    
                    if src not in images:
                        images.append(src)
            
            return images[:3]  # æœ€å¤šè¿”å›3å¼ å›¾ç‰‡
            
        except Exception as e:
            logger.debug(f"æå–å®¹å™¨å›¾ç‰‡å¤±è´¥: {str(e)}")
            return []
    
    def _extract_container_title(self, container, note_id, index):
        """ä»å®¹å™¨ä¸­æå–æ ‡é¢˜"""
        try:
            # è·å–å®¹å™¨çš„æ–‡æœ¬å†…å®¹
            text_content = container.get_text(separator=' ', strip=True)
            
            # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤æ•°å­—å’Œç‰¹æ®Šå­—ç¬¦
            import re
            
            # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„ç‰‡æ®µä½œä¸ºæ ‡é¢˜
            lines = text_content.split()
            potential_titles = []
            
            for line in lines:
                # è¿‡æ»¤æ‰çº¯æ•°å­—ã€å•ä¸ªå­—ç¬¦ç­‰
                if len(line) > 3 and not line.isdigit():
                    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡å’ŒåŸºæœ¬æ ‡ç‚¹
                    cleaned = re.sub(r'[^\w\u4e00-\u9fff\sï¼ï¼Ÿï¼Œã€‚ã€]', '', line).strip()
                    if len(cleaned) > 5 and len(cleaned) < 50:
                        potential_titles.append(cleaned)
            
            # é€‰æ‹©æœ€ä½³æ ‡é¢˜
            if potential_titles:
                # ä¼˜å…ˆé€‰æ‹©åŒ…å«æ›´å¤šä¸­æ–‡å­—ç¬¦çš„æ ‡é¢˜
                potential_titles.sort(key=lambda x: len(re.findall(r'[\u4e00-\u9fff]', x)), reverse=True)
                return potential_titles[0]
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼
            return f"å°çº¢ä¹¦ç¬”è®° #{index+1}"
            
        except Exception as e:
            logger.debug(f"æå–å®¹å™¨æ ‡é¢˜å¤±è´¥: {str(e)}")
            return f"å°çº¢ä¹¦ç¬”è®° #{index+1}"
    
    def _extract_container_author(self, container):
        """ä»å®¹å™¨ä¸­æå–ä½œè€…ä¿¡æ¯"""
        try:
            text_content = container.get_text(separator=' ', strip=True)
            
            # æŸ¥æ‰¾å¯èƒ½çš„ä½œè€…æ ‡è¯†
            import re
            author_patterns = [
                r'@([^\s<>]{2,20})',
                r'ä½œè€…[ï¼š:]\s*([^\s<>]{2,20})',
                r'by\s+([^\s<>]{2,20})'
            ]
            
            for pattern in author_patterns:
                author_match = re.search(pattern, text_content)
                if author_match:
                    return author_match.group(1)
            
            return "æœªçŸ¥ä½œè€…"
            
        except Exception:
            return "æœªçŸ¥ä½œè€…"
    
    def _extract_container_content(self, container):
        """ä»å®¹å™¨ä¸­æå–å†…å®¹æè¿°"""
        try:
            text_content = container.get_text(separator=' ', strip=True)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            if len(text_content) > 100:
                return text_content[:100] + "..."
            
            return text_content or "å†…å®¹åŠ è½½ä¸­..."
            
        except Exception:
            return "å†…å®¹åŠ è½½ä¸­..."
    
    def _extract_container_engagement(self, container):
        """ä»å®¹å™¨ä¸­æå–äº’åŠ¨æ•°æ®"""
        try:
            text_content = container.get_text(separator=' ', strip=True)
            
            # æŸ¥æ‰¾æ•°å­—ï¼Œå¯èƒ½ä»£è¡¨äº’åŠ¨æ•°æ®
            import re
            numbers = re.findall(r'\d+', text_content)
            
            # ç®€å•çš„å¯å‘å¼åˆ†é…
            engagement = {
                'likes': '0',
                'comments': '0', 
                'collects': '0'
            }
            
            if numbers:
                # å°†æ‰¾åˆ°çš„æ•°å­—åˆ†é…ç»™ä¸åŒçš„äº’åŠ¨ç±»å‹
                if len(numbers) >= 1:
                    engagement['likes'] = numbers[0]
                if len(numbers) >= 2:
                    engagement['comments'] = numbers[1]
                if len(numbers) >= 3:
                    engagement['collects'] = numbers[2]
            
            return engagement
            
        except Exception:
            return {'likes': '0', 'comments': '0', 'collects': '0'}

# ç¤ºä¾‹ä»£ç 
if __name__ == "__main__":
    crawler = XiaoHongShuCrawler(use_selenium=False)  # ä½¿ç”¨Requestsæ¨¡å¼è¿›è¡Œæ¼”ç¤º
    
    # æ¼”ç¤ºæœç´¢åŠŸèƒ½
    keyword = "å£çº¢"
    notes = crawler.search(keyword, max_results=5)
    
    print(f"æœç´¢ '{keyword}' ç»“æœ:")
    for i, note in enumerate(notes):
        print(f"{i+1}. {note['title']} - ç‚¹èµ: {note['likes']}")
    
    # å…³é—­çˆ¬è™«
    crawler.close() 