#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å°çº¢ä¹¦åå°é™é»˜çˆ¬è™«
ç”¨äºæ‰¹é‡æå–æœç´¢ç»“æœä¸­æ‰€æœ‰ç¬”è®°çš„è¯¦ç»†å†…å®¹
"""

import os
import sys
import json
import time
import logging
import threading
import random
from datetime import datetime
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from urllib.parse import urljoin

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, PROJECT_ROOT)

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
import requests
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('cache/logs/backend_crawler.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class BackendXHSCrawler:
    """åå°å°çº¢ä¹¦ç¬”è®°å†…å®¹çˆ¬è™« - å¢å¼ºååçˆ¬åŠŸèƒ½"""
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„cacheç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.cache_dir = os.path.join(project_root, "cache")
        self.notes_dir = os.path.join(self.cache_dir, "notes")
        self.cookies_file = os.path.join('cache', 'cookies', 'xiaohongshu_cookies.json')
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.notes_dir, exist_ok=True)
        os.makedirs('cache/logs', exist_ok=True)
        
        # çˆ¬è™«é…ç½®
        self.max_workers = 2  # å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé™ä½ä»¥é¿å…è¢«å°ï¼‰
        self.request_delay = 3  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        self.timeout = 30  # è¯·æ±‚è¶…æ—¶æ—¶é—´
        self.retry_count = 2  # é‡è¯•æ¬¡æ•°
        
        # åçˆ¬è™«é…ç½®
        self.human_behavior_config = {
            'min_wait_between_requests': 3,    # æœ€å°ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            'max_wait_between_requests': 8,    # æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            'scroll_pause_time': 2,            # æ»šåŠ¨åœç•™æ—¶é—´
            'random_mouse_move': True,         # éšæœºé¼ æ ‡ç§»åŠ¨
            'page_stay_time': (5, 15),        # é¡µé¢åœç•™æ—¶é—´èŒƒå›´
            'retry_on_error': True,            # é‡åˆ°é”™è¯¯æ—¶é‡è¯•
            'max_retries': 3,                  # æœ€å¤§é‡è¯•æ¬¡æ•°
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_notes': 0,
            'success_count': 0,
            'failed_count': 0,
            'start_time': None,
            'end_time': None
        }
        
        self.driver = None
        logger.info("ğŸš€ åå°å°çº¢ä¹¦çˆ¬è™«åˆå§‹åŒ–å®Œæˆ - å¢å¼ºååçˆ¬åŠŸèƒ½")
    
    def start_batch_crawl(self, notes_data: List[Dict[str, Any]], session_id: str = None) -> Dict[str, Any]:
        """å¯åŠ¨æ‰¹é‡çˆ¬å–ä»»åŠ¡"""
        if not session_id:
            session_id = f"batch_{int(time.time())}"
        
        self.stats['start_time'] = datetime.now()
        self.stats['total_notes'] = len(notes_data)
        
        logger.info(f"ğŸ¯ å¼€å§‹æ‰¹é‡çˆ¬å–ä»»åŠ¡")
        logger.info(f"ğŸ“Š ä¼šè¯ID: {session_id}")
        logger.info(f"ğŸ“ å¾…çˆ¬å–ç¬”è®°æ•°é‡: {len(notes_data)}")
        logger.info(f"ğŸ”§ å¹¶å‘çº¿ç¨‹æ•°: {self.max_workers}")
        logger.info(f"â±ï¸ è¯·æ±‚é—´éš”: {self.request_delay}ç§’")
        
        # åˆ›å»ºä¼šè¯ç›®å½•
        session_dir = os.path.join(self.notes_dir, f"batch_{session_id}")
        os.makedirs(session_dir, exist_ok=True)
        
        # ä¿å­˜ä»»åŠ¡ä¿¡æ¯
        task_info = {
            'session_id': session_id,
            'start_time': self.stats['start_time'].isoformat(),
            'total_notes': len(notes_data),
            'notes_list': notes_data
        }
        
        task_file = os.path.join(session_dir, 'task_info.json')
        with open(task_file, 'w', encoding='utf-8') as f:
            json.dump(task_info, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ ä»»åŠ¡ä¿¡æ¯å·²ä¿å­˜: {task_file}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘çˆ¬å–
        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_note = {
                executor.submit(self._crawl_single_note, note_data, session_id, i+1): note_data 
                for i, note_data in enumerate(notes_data)
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_note):
                note_data = future_to_note[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result['success']:
                        self.stats['success_count'] += 1
                        note_id_display = note_data.get('note_id') or note_data.get('id', 'N/A')
                        logger.info(f"âœ… [{self.stats['success_count']}/{len(notes_data)}] ç¬”è®°çˆ¬å–æˆåŠŸ: {note_id_display}")
                    else:
                        self.stats['failed_count'] += 1
                        note_id_display = note_data.get('note_id') or note_data.get('id', 'N/A')
                        logger.error(f"âŒ [{self.stats['failed_count']}/{len(notes_data)}] ç¬”è®°çˆ¬å–å¤±è´¥: {note_id_display} - {result.get('error', 'Unknown error')}")
                        
                except Exception as e:
                    self.stats['failed_count'] += 1
                    note_id_display = note_data.get('note_id') or note_data.get('id', 'N/A')
                    logger.error(f"âŒ ç¬”è®°çˆ¬å–å¼‚å¸¸: {note_id_display} - {str(e)}")
                    results.append({
                        'note_id': note_data.get('note_id') or note_data.get('id'),
                        'success': False,
                        'error': str(e)
                    })
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(self.request_delay)
        
        self.stats['end_time'] = datetime.now()
        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        
        # ä¿å­˜ç»“æœç»Ÿè®¡
        final_stats = {
            **self.stats,
            'start_time': self.stats['start_time'].isoformat(),
            'end_time': self.stats['end_time'].isoformat(),
            'duration_seconds': duration,
            'success_rate': self.stats['success_count'] / len(notes_data) * 100 if notes_data else 0,
            'results': results
        }
        
        stats_file = os.path.join(session_dir, 'crawl_stats.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, ensure_ascii=False, indent=2)
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        logger.info("ğŸ‰ æ‰¹é‡çˆ¬å–ä»»åŠ¡å®Œæˆ!")
        logger.info(f"ğŸ“Š æ€»è®¡: {len(notes_data)} ç¯‡ç¬”è®°")
        logger.info(f"âœ… æˆåŠŸ: {self.stats['success_count']} ç¯‡")
        logger.info(f"âŒ å¤±è´¥: {self.stats['failed_count']} ç¯‡")
        logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {final_stats['success_rate']:.1f}%")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {duration:.1f} ç§’")
        logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {session_dir}")
        
        return final_stats
    
    def _crawl_single_note(self, note_data: Dict[str, Any], session_id: str, index: int) -> Dict[str, Any]:
        """çˆ¬å–å•ä¸ªç¬”è®°çš„è¯¦ç»†å†…å®¹"""
        # æ”¯æŒå¤šç§IDå­—æ®µåæ ¼å¼
        note_id = note_data.get('note_id') or note_data.get('id')
        note_url = note_data.get('url') or note_data.get('link')
        
        if not note_id:
            logger.error(f"âŒ [{index}] ç¬”è®°æ•°æ®ç¼ºå°‘IDå­—æ®µï¼Œæ•°æ®ç»“æ„: {list(note_data.keys())}")
            return {'note_id': None, 'success': False, 'error': 'ç¼ºå°‘ç¬”è®°ID'}
        
        logger.info(f"ğŸ” [{index}] å¼€å§‹çˆ¬å–ç¬”è®°: {note_id}")
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.retry_count):
            try:
                if attempt > 0:
                    logger.info(f"ğŸ”„ [{index}] ç¬¬ {attempt + 1} æ¬¡é‡è¯•: {note_id}")
                    time.sleep(attempt * 2)  # é€’å¢å»¶è¿Ÿ
                
                # åˆ›å»ºæµè§ˆå™¨å®ä¾‹
                driver = self._create_browser_instance()
                if not driver:
                    raise Exception("æ— æ³•åˆ›å»ºæµè§ˆå™¨å®ä¾‹")
                
                try:
                    # åŠ è½½cookies
                    self._load_cookies()
                    
                    # æ„å»ºç¬”è®°URLï¼ˆæ·»åŠ å¿…è¦çš„xsecå‚æ•°ï¼‰
                    xsec_token = note_data.get('xsec_token', '')
                    
                    if note_url and note_url.startswith('http'):
                        # å¦‚æœå·²æœ‰å®Œæ•´URLï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«xsecå‚æ•°
                        if 'xsec_token' not in note_url and xsec_token:
                            separator = '&' if '?' in note_url else '?'
                            target_url = f"{note_url}{separator}xsec_source=pc_feed&xsec_token={xsec_token}"
                        else:
                            target_url = note_url
                    else:
                        # æ„å»ºå®Œæ•´URLå¹¶æ·»åŠ xsecå‚æ•°
                        base_url = f"https://www.xiaohongshu.com/explore/{note_id}"
                        if xsec_token:
                            target_url = f"{base_url}?xsec_source=pc_feed&xsec_token={xsec_token}"
                        else:
                            target_url = base_url
                            logger.warning(f"âš ï¸ [{index}] ç¬”è®° {note_id} ç¼ºå°‘xsec_tokenï¼Œå¯èƒ½å½±å“è®¿é—®æˆåŠŸç‡")
                    
                    logger.debug(f"ğŸŒ [{index}] è®¿é—®URL: {target_url}")
                    
                    # è®¿é—®ç¬”è®°é¡µé¢
                    driver.get(target_url)
                    
                    # ç­‰å¾…é¡µé¢åŠ è½½
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    
                    # ç­‰å¾…å†…å®¹åŠ è½½
                    time.sleep(3)
                    
                    # è·å–é¡µé¢æºç 
                    page_source = driver.page_source
                    
                    # ä¿å­˜é¡µé¢æºç 
                    source_file = self._save_page_source(note_id, page_source, session_id, index)
                    
                    # è§£æé¡µé¢å†…å®¹
                    note_detail = self._parse_note_content(page_source, note_id, session_id, index)
                    
                    # ä¸‹è½½å›¾ç‰‡
                    images = self._download_note_images_from_source(page_source, note_id, session_id, index)
                    note_detail['images'] = images
                    
                    # ä¿å­˜ç¬”è®°è¯¦æƒ…
                    detail_file = self._save_note_detail(note_detail, note_id, session_id, index)
                    
                    logger.info(f"âœ… [{index}] ç¬”è®°çˆ¬å–å®Œæˆ: {note_id}")
                    logger.info(f"ğŸ“„ [{index}] æ ‡é¢˜: {note_detail.get('title', 'N/A')[:50]}...")
                    logger.info(f"ğŸ“ [{index}] å†…å®¹é•¿åº¦: {len(note_detail.get('content', ''))} å­—ç¬¦")
                    logger.info(f"ğŸ·ï¸ [{index}] æ ‡ç­¾æ•°é‡: {len(note_detail.get('tags', []))}")
                    logger.info(f"ğŸ–¼ï¸ [{index}] å›¾ç‰‡æ•°é‡: {len(images)}")
                    
                    return {
                        'note_id': note_id,
                        'success': True,
                        'source_file': source_file,
                        'detail_file': detail_file,
                        'images_count': len(images),
                        'title': note_detail.get('title', ''),
                        'content_length': len(note_detail.get('content', '')),
                        'tags_count': len(note_detail.get('tags', []))
                    }
                    
                finally:
                    driver.quit()
                    
            except Exception as e:
                logger.error(f"âŒ [{index}] çˆ¬å–å¤±è´¥ (å°è¯• {attempt + 1}/{self.retry_count}): {note_id} - {str(e)}")
                if attempt == self.retry_count - 1:  # æœ€åä¸€æ¬¡å°è¯•
                    return {
                        'note_id': note_id,
                        'success': False,
                        'error': str(e),
                        'attempts': attempt + 1
                    }
        
        return {'note_id': note_id, 'success': False, 'error': 'æ‰€æœ‰é‡è¯•å‡å¤±è´¥'}
    
    def _create_browser_instance(self):
        """åˆ›å»ºæµè§ˆå™¨å®ä¾‹"""
        try:
            from selenium.webdriver.chrome.service import Service
            
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--ignore-certificate-errors')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            chrome_options.add_argument('--window-size=1920,1080')
            
            # æ™ºèƒ½é€‰æ‹©ChromeDriver
            service = None
            
            # 1. å°è¯•ä½¿ç”¨æœ¬åœ°ChromeDriver
            driver_paths = [
                'drivers/chromedriver-mac-arm64/chromedriver',
                'drivers/chromedriver',
                '/usr/local/bin/chromedriver',
                '/opt/homebrew/bin/chromedriver'
            ]
            
            for driver_path in driver_paths:
                if os.path.exists(driver_path):
                    try:
                        service = Service(executable_path=driver_path)
                        driver = webdriver.Chrome(service=service, options=chrome_options)
                        logger.debug(f"ä½¿ç”¨æœ¬åœ°ChromeDriver: {driver_path}")
                        return driver
                    except Exception as e:
                        logger.debug(f"ä½¿ç”¨è·¯å¾„ {driver_path} åˆ›å»ºé©±åŠ¨å¤±è´¥: {str(e)}")
                        continue
            
            # 2. ä½¿ç”¨webdriver-managerè‡ªåŠ¨ä¸‹è½½ï¼ˆä¼˜å…ˆçº§æé«˜ï¼‰
            try:
                from webdriver_manager.chrome import ChromeDriverManager
                # ä½¿ç”¨ä¸­å›½é•œåƒæºåŠ é€Ÿä¸‹è½½
                os.environ['WDM_LOCAL'] = '1'  # æœ¬åœ°å­˜å‚¨
                driver_path = ChromeDriverManager().install()
                service = Service(driver_path)
                driver = webdriver.Chrome(service=service, options=chrome_options)
                logger.debug(f"ä½¿ç”¨webdriver-managerä¸‹è½½çš„ChromeDriver: {driver_path}")
                return driver
            except Exception as e:
                logger.debug(f"webdriver-managerä¸‹è½½å¤±è´¥: {str(e)}")
            
            # 3. æœ€åå°è¯•ç³»ç»ŸPATHä¸­çš„chromedriver
            try:
                driver = webdriver.Chrome(options=chrome_options)
                logger.debug("ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„chromedriver")
                return driver
            except Exception as e:
                logger.debug(f"ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„chromedriverå¤±è´¥: {str(e)}")
            
            return None
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæµè§ˆå™¨å®ä¾‹å¤±è´¥: {str(e)}")
            return None
    
    def _load_cookies(self):
        """åŠ è½½cookies"""
        try:
            if os.path.exists(self.cookies_file):
                with open(self.cookies_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                
                # å…ˆè®¿é—®å°çº¢ä¹¦ä¸»é¡µ
                self.driver.get("https://www.xiaohongshu.com")
                time.sleep(2)
                
                # æ·»åŠ cookies
                for cookie in cookies:
                    try:
                        self.driver.add_cookie(cookie)
                    except Exception as e:
                        logger.debug(f"æ·»åŠ cookieå¤±è´¥: {e}")
                
                logger.debug("âœ… cookiesåŠ è½½å®Œæˆ")
            else:
                logger.warning("âš ï¸ cookiesæ–‡ä»¶ä¸å­˜åœ¨")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½cookieså¤±è´¥: {str(e)}")
    
    def _save_page_source(self, note_id: str, page_source: str, session_id: str, index: int) -> str:
        """ä¿å­˜é¡µé¢æºç """
        try:
            filename = f"{index:03d}_{note_id}_source.html"
            session_dir = os.path.join(self.notes_dir, f"batch_{session_id}")
            file_path = os.path.join(session_dir, filename)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(page_source)
            
            logger.debug(f"é¡µé¢æºç å·²ä¿å­˜: {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜é¡µé¢æºç å¤±è´¥: {str(e)}")
            return ""
    
    def _parse_note_content(self, page_source: str, note_id: str, session_id: str, index: int) -> Dict[str, Any]:
        """è§£æç¬”è®°å†…å®¹"""
        try:
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = self._extract_title(soup)
            
            # æå–å†…å®¹
            content = self._extract_content(soup)
            
            # æå–æ ‡ç­¾
            tags = self._extract_tags(soup)
            
            # æå–ä½œè€…ä¿¡æ¯
            author = self._extract_author(soup)
            
            return {
                'note_id': note_id,
                'title': title,
                'content': content,
                'tags': tags,
                'author': author,
                'crawl_time': datetime.now().isoformat(),
                'session_id': session_id,
                'index': index
            }
            
        except Exception as e:
            logger.error(f"è§£æç¬”è®°å†…å®¹å¤±è´¥: {str(e)}")
            return {
                'note_id': note_id,
                'title': 'è§£æå¤±è´¥',
                'content': 'è§£æå¤±è´¥',
                'tags': [],
                'author': 'æœªçŸ¥',
                'error': str(e)
            }
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–æ ‡é¢˜"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            title_selectors = [
                'h1.title',
                '.note-title',
                '[data-testid="note-title"]',
                'h1',
                '.content-title',
                'title'
            ]
            
            for selector in title_selectors:
                element = soup.select_one(selector)
                if element and element.get_text(strip=True):
                    title = element.get_text(strip=True)
                    return title
            
            # ä»é¡µé¢titleæ ‡ç­¾æå–
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
                # æ¸…ç†æ ‡é¢˜
                title = title.replace(' - å°çº¢ä¹¦', '').replace(' | å°çº¢ä¹¦', '').strip()
                if title:
                    return title
            
            return "æœªæ‰¾åˆ°æ ‡é¢˜"
            
        except Exception as e:
            logger.error(f"æå–æ ‡é¢˜å¤±è´¥: {str(e)}")
            return "æ ‡é¢˜æå–å¤±è´¥"
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """æå–å†…å®¹"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            content_selectors = [
                '.note-content',
                '.content-text',
                '[data-testid="note-content"]',
                '.desc',
                '.note-desc',
                '.content-desc'
            ]
            
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element and element.get_text(strip=True):
                    content = element.get_text(strip=True)
                    return content
            
            # å°è¯•ä»scriptæ ‡ç­¾ä¸­æå–JSONæ•°æ®
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string and 'window.__INITIAL_STATE__' in script.string:
                    try:
                        # æå–JSONæ•°æ®
                        json_start = script.string.find('{')
                        json_end = script.string.rfind('}') + 1
                        if json_start != -1 and json_end != -1:
                            json_str = script.string[json_start:json_end]
                            data = json.loads(json_str)
                            
                            # åœ¨JSONä¸­æŸ¥æ‰¾å†…å®¹
                            content = self._extract_content_from_json(data)
                            if content:
                                return content
                    except:
                        continue
            
            return "æœªæ‰¾åˆ°å†…å®¹"
            
        except Exception as e:
            logger.error(f"æå–å†…å®¹å¤±è´¥: {str(e)}")
            return "å†…å®¹æå–å¤±è´¥"
    
    def _extract_content_from_json(self, data: dict) -> str:
        """ä»JSONæ•°æ®ä¸­æå–å†…å®¹"""
        try:
            def search_content(obj, path=""):
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        if key in ['desc', 'content', 'text', 'title'] and isinstance(value, str) and len(value) > 10:
                            return value
                        result = search_content(value, f"{path}.{key}")
                        if result:
                            return result
                elif isinstance(obj, list):
                    for i, item in enumerate(obj):
                        result = search_content(item, f"{path}[{i}]")
                        if result:
                            return result
                return None
            
            return search_content(data) or ""
            
        except Exception as e:
            logger.error(f"ä»JSONæå–å†…å®¹å¤±è´¥: {str(e)}")
            return ""
    
    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """æå–æ ‡ç­¾"""
        try:
            tags = []
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            tag_selectors = [
                '.tag',
                '.hashtag',
                '.topic',
                '[data-testid="tag"]',
                '.note-tag',
                '.topic-tag'
            ]
            
            for selector in tag_selectors:
                elements = soup.select(selector)
                for element in elements:
                    tag_text = element.get_text(strip=True)
                    if tag_text and tag_text not in tags:
                        # æ¸…ç†æ ‡ç­¾æ–‡æœ¬
                        tag_text = tag_text.replace('#', '').strip()
                        if tag_text:
                            tags.append(tag_text)
            
            # ä»æ–‡æœ¬ä¸­æå–#æ ‡ç­¾
            text_content = soup.get_text()
            hashtag_pattern = r'#([^#\s]+)'
            import re
            hashtags = re.findall(hashtag_pattern, text_content)
            for tag in hashtags:
                if tag not in tags:
                    tags.append(tag)
            
            return tags[:10]  # æœ€å¤šè¿”å›10ä¸ªæ ‡ç­¾
            
        except Exception as e:
            logger.error(f"æå–æ ‡ç­¾å¤±è´¥: {str(e)}")
            return []
    
    def _extract_author(self, soup: BeautifulSoup) -> str:
        """æå–ä½œè€…ä¿¡æ¯"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            author_selectors = [
                '.author-name',
                '.user-name',
                '[data-testid="author"]',
                '.note-author',
                '.username'
            ]
            
            for selector in author_selectors:
                element = soup.select_one(selector)
                if element and element.get_text(strip=True):
                    author = element.get_text(strip=True)
                    return author
            
            return "æœªçŸ¥ä½œè€…"
            
        except Exception as e:
            logger.error(f"æå–ä½œè€…å¤±è´¥: {str(e)}")
            return "ä½œè€…æå–å¤±è´¥"
    
    def _download_note_images_from_source(self, page_source: str, note_id: str, session_id: str, index: int) -> List[Dict[str, str]]:
        """ä»é¡µé¢æºç ä¸‹è½½ç¬”è®°å›¾ç‰‡"""
        try:
            soup = BeautifulSoup(page_source, 'html.parser')
            images = []
            
            # åˆ›å»ºå›¾ç‰‡ç›®å½•
            session_dir = os.path.join(self.notes_dir, f"batch_{session_id}")
            images_dir = os.path.join(session_dir, f"{index:03d}_{note_id}_images")
            os.makedirs(images_dir, exist_ok=True)
            
            # æŸ¥æ‰¾å›¾ç‰‡å…ƒç´ 
            img_elements = soup.find_all('img')
            
            for i, img in enumerate(img_elements):
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if not src:
                    continue
                
                # è¿‡æ»¤æ‰å°å›¾æ ‡å’Œæ— å…³å›¾ç‰‡
                if any(keyword in src.lower() for keyword in ['icon', 'avatar', 'logo', 'button']):
                    continue
                
                # ç¡®ä¿æ˜¯å®Œæ•´URL
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = 'https://www.xiaohongshu.com' + src
                
                # ä¸‹è½½å›¾ç‰‡
                image_info = self._download_single_image(src, images_dir, f"{index:03d}_{note_id}_{i}")
                if image_info:
                    images.append(image_info)
            
            logger.debug(f"ä¸‹è½½äº† {len(images)} å¼ å›¾ç‰‡")
            return images
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {str(e)}")
            return []
    
    def _download_single_image(self, url: str, save_dir: str, filename_prefix: str) -> Optional[Dict[str, str]]:
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://www.xiaohongshu.com/'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            content_type = response.headers.get('content-type', '')
            if 'jpeg' in content_type or 'jpg' in content_type:
                ext = '.jpg'
            elif 'png' in content_type:
                ext = '.png'
            elif 'webp' in content_type:
                ext = '.webp'
            else:
                ext = '.jpg'  # é»˜è®¤
            
            filename = f"{filename_prefix}{ext}"
            file_path = os.path.join(save_dir, filename)
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            # ç”ŸæˆWebè®¿é—®è·¯å¾„
            relative_path = os.path.relpath(file_path, self.temp_dir)
            web_path = f"/temp/{relative_path.replace(os.sep, '/')}"
            
            return {
                'original_url': url,
                'local_path': file_path,
                'web_path': web_path,
                'filename': filename
            }
            
        except Exception as e:
            logger.debug(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {url}: {str(e)}")
            return None
    
    def _save_note_detail(self, note_detail: Dict[str, Any], note_id: str, session_id: str, index: int) -> str:
        """ä¿å­˜ç¬”è®°è¯¦æƒ…"""
        try:
            filename = f"{index:03d}_{note_id}_detail.json"
            session_dir = os.path.join(self.notes_dir, f"batch_{session_id}")
            file_path = os.path.join(session_dir, filename)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(note_detail, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ç¬”è®°è¯¦æƒ…å·²ä¿å­˜: {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç¬”è®°è¯¦æƒ…å¤±è´¥: {str(e)}")
            return ""

    def create_stealth_driver(self):
        """åˆ›å»ºéšè”½æ€§æµè§ˆå™¨é©±åŠ¨"""
        try:
            chrome_options = Options()
            
            # åŸºç¡€åæ£€æµ‹é…ç½®
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ç¯å¢ƒ
            user_agents = [
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
            chrome_options.add_argument(f'--user-agent={random.choice(user_agents)}')
            
            # å…¶ä»–éšè”½æ€§è®¾ç½®
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--ignore-certificate-errors')
            chrome_options.add_argument('--allow-running-insecure-content')
            chrome_options.add_argument('--disable-features=VizDisplayCompositor')
            
            # çª—å£å¤§å°éšæœºåŒ–
            window_sizes = ['1366,768', '1920,1080', '1440,900', '1280,720']
            chrome_options.add_argument(f'--window-size={random.choice(window_sizes)}')
            
            # åˆ›å»ºé©±åŠ¨
            driver = webdriver.Chrome(options=chrome_options)
            
            # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5]
                    });
                    Object.defineProperty(navigator, 'languages', {
                        get: () => ['zh-CN', 'zh', 'en']
                    });
                    window.chrome = {
                        runtime: {}
                    };
                '''
            })
            
            logger.info("âœ… åˆ›å»ºéšè”½æ€§æµè§ˆå™¨é©±åŠ¨æˆåŠŸ")
            return driver
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºæµè§ˆå™¨é©±åŠ¨å¤±è´¥: {str(e)}")
            raise
    
    def simulate_human_behavior(self, driver):
        """æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º"""
        try:
            # éšæœºé¼ æ ‡ç§»åŠ¨
            if self.human_behavior_config['random_mouse_move']:
                action = ActionChains(driver)
                # è·å–é¡µé¢å°ºå¯¸
                page_width = driver.execute_script("return window.innerWidth")
                page_height = driver.execute_script("return window.innerHeight")
                
                # éšæœºç§»åŠ¨é¼ æ ‡
                for _ in range(random.randint(1, 3)):
                    x = random.randint(100, page_width - 100)
                    y = random.randint(100, page_height - 100)
                    action.move_by_offset(x, y).perform()
                    time.sleep(random.uniform(0.5, 1.5))
            
            # éšæœºæ»šåŠ¨
            scroll_times = random.randint(1, 3)
            for _ in range(scroll_times):
                scroll_height = random.randint(200, 800)
                driver.execute_script(f"window.scrollBy(0, {scroll_height});")
                time.sleep(random.uniform(1, 2))
            
            # é¡µé¢åœç•™æ—¶é—´
            stay_time = random.uniform(*self.human_behavior_config['page_stay_time'])
            logger.debug(f"ğŸ­ æ¨¡æ‹Ÿäººç±»è¡Œä¸º: é¡µé¢åœç•™ {stay_time:.1f} ç§’")
            time.sleep(stay_time)
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ¨¡æ‹Ÿäººç±»è¡Œä¸ºæ—¶å‡ºç°å¼‚å¸¸: {str(e)}")
    
    def smart_wait_between_requests(self):
        """æ™ºèƒ½ç­‰å¾…ç­–ç•¥"""
        base_wait = random.uniform(
            self.human_behavior_config['min_wait_between_requests'],
            self.human_behavior_config['max_wait_between_requests']
        )
        
        # æ ¹æ®æ—¶é—´æ®µè°ƒæ•´ç­‰å¾…æ—¶é—´ï¼ˆå¤œé—´å»¶é•¿ç­‰å¾…ï¼‰
        current_hour = datetime.now().hour
        if 23 <= current_hour or current_hour <= 6:  # å¤œé—´
            base_wait *= 1.5
        elif 12 <= current_hour <= 14:  # åˆä¼‘æ—¶é—´
            base_wait *= 1.2
        
        logger.debug(f"â±ï¸ æ™ºèƒ½ç­‰å¾…: {base_wait:.1f} ç§’")
        time.sleep(base_wait)
    
    def extract_note_content_with_retry(self, note_url, session_id, retries=0):
        """å¸¦é‡è¯•æœºåˆ¶çš„ç¬”è®°å†…å®¹æå–"""
        max_retries = self.human_behavior_config['max_retries']
        
        if retries >= max_retries:
            logger.error(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})ï¼Œæ”¾å¼ƒæå–: {note_url}")
            return None
        
        try:
            if retries > 0:
                logger.info(f"ğŸ”„ ç¬¬ {retries + 1} æ¬¡å°è¯•æå–ç¬”è®°: {note_url}")
                # é‡è¯•å‰ç­‰å¾…æ›´é•¿æ—¶é—´
                extended_wait = random.uniform(10, 20)
                logger.info(f"â³ é‡è¯•å‰ç­‰å¾… {extended_wait:.1f} ç§’...")
                time.sleep(extended_wait)
            
            # åˆ›å»ºæ–°çš„é©±åŠ¨ï¼ˆæ¯æ¬¡é‡è¯•ä½¿ç”¨æ–°çš„æµè§ˆå™¨å®ä¾‹ï¼‰
            if self.driver:
                self.driver.quit()
            
            self.driver = self.create_stealth_driver()
            
            # åŠ è½½cookies
            self._load_cookies()
            
            # æ™ºèƒ½ç­‰å¾…
            self.smart_wait_between_requests()
            
            logger.info(f"ğŸŒ æ­£åœ¨è®¿é—®ç¬”è®°é¡µé¢: {note_url}")
            
            # è®¿é—®é¡µé¢
            self.driver.get(note_url)
            
            # æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º
            self.simulate_human_behavior(self.driver)
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°"ä½ è®¿é—®çš„ç¬”è®°ä¸è§äº†"
            page_source = self.driver.page_source
            error_indicators = [
                "ä½ è®¿é—®çš„ç¬”è®°ä¸è§äº†",
                "é¡µé¢ä¸å­˜åœ¨",
                "å†…å®¹å·²åˆ é™¤",
                "access denied",
                "ç½‘ç»œå¼‚å¸¸"
            ]
            
            for error in error_indicators:
                if error in page_source:
                    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°é”™è¯¯é¡µé¢: {error}")
                    if self.human_behavior_config['retry_on_error']:
                        return self.extract_note_content_with_retry(note_url, session_id, retries + 1)
                    else:
                        return None
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # æå–å†…å®¹
            content = self._extract_note_details()
            
            # ä¿å­˜é¡µé¢æºç ç”¨äºè°ƒè¯•
            self._save_page_source(note_url, session_id)
            
            logger.info(f"âœ… æˆåŠŸæå–ç¬”è®°å†…å®¹")
            return content
            
        except TimeoutException:
            logger.warning(f"â° é¡µé¢åŠ è½½è¶…æ—¶: {note_url}")
            if self.human_behavior_config['retry_on_error']:
                return self.extract_note_content_with_retry(note_url, session_id, retries + 1)
            return None
            
        except Exception as e:
            logger.error(f"âŒ æå–ç¬”è®°å†…å®¹æ—¶å‡ºé”™: {str(e)}")
            if self.human_behavior_config['retry_on_error'] and "ä½ è®¿é—®çš„ç¬”è®°ä¸è§äº†" not in str(e):
                return self.extract_note_content_with_retry(note_url, session_id, retries + 1)
            return None
    
    def batch_extract_notes(self, note_links, session_id):
        """æ‰¹é‡æå–ç¬”è®°å†…å®¹ - é‡‡ç”¨äººä¸ºè¡Œä¸ºæ¨¡å¼"""
        if not note_links:
            logger.warning("âš ï¸ æ²¡æœ‰æä¾›ç¬”è®°é“¾æ¥")
            return []
        
        results = []
        total_notes = len(note_links)
        
        logger.info(f"ğŸ“‹ å¼€å§‹æ‰¹é‡æå– {total_notes} ä¸ªç¬”è®°å†…å®¹ï¼ˆäººä¸ºè¡Œä¸ºæ¨¡å¼ï¼‰")
        
        for i, note_url in enumerate(note_links, 1):
            try:
                logger.info(f"ğŸ“– æ­£åœ¨å¤„ç†ç¬¬ {i}/{total_notes} ä¸ªç¬”è®°")
                
                # æå–ç¬”è®°å†…å®¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                content = self.extract_note_content_with_retry(note_url, session_id)
                
                if content:
                    results.append({
                        'url': note_url,
                        'content': content,
                        'extracted_at': datetime.now().isoformat()
                    })
                    logger.info(f"âœ… ç¬¬ {i} ä¸ªç¬”è®°æå–æˆåŠŸ")
                else:
                    logger.warning(f"âŒ ç¬¬ {i} ä¸ªç¬”è®°æå–å¤±è´¥")
                
                # æ¯å¤„ç†å‡ ä¸ªç¬”è®°åè¿›è¡Œæ›´é•¿æ—¶é—´çš„ä¼‘æ¯
                if i % 5 == 0 and i < total_notes:
                    long_break = random.uniform(30, 60)
                    logger.info(f"ğŸ˜´ å¤„ç†äº† {i} ä¸ªç¬”è®°ï¼Œä¼‘æ¯ {long_break:.1f} ç§’...")
                    time.sleep(long_break)
                
            except KeyboardInterrupt:
                logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­æ‰¹é‡æå–")
                break
            except Exception as e:
                logger.error(f"âŒ å¤„ç†ç¬¬ {i} ä¸ªç¬”è®°æ—¶å‡ºé”™: {str(e)}")
                continue
        
        # æ¸…ç†èµ„æº
        if self.driver:
            self.driver.quit()
            self.driver = None
        
        logger.info(f"ğŸ‰ æ‰¹é‡æå–å®Œæˆï¼ŒæˆåŠŸæå– {len(results)}/{total_notes} ä¸ªç¬”è®°")
        
        # ä¿å­˜ç»“æœ
        self._save_batch_results(results, session_id)
        
        return results

    def _extract_note_details(self):
        """æå–ç¬”è®°è¯¦ç»†ä¿¡æ¯"""
        try:
            content = {}
            
            # æå–æ ‡é¢˜
            try:
                title_element = self.driver.find_element(By.CSS_SELECTOR, ".note-detail-wrapper .title")
                content['title'] = title_element.text.strip()
            except:
                content['title'] = ""
            
            # æå–æ­£æ–‡å†…å®¹
            try:
                desc_element = self.driver.find_element(By.CSS_SELECTOR, ".note-detail-wrapper .desc")
                content['description'] = desc_element.text.strip()
            except:
                content['description'] = ""
            
            # æå–ä½œè€…ä¿¡æ¯
            try:
                author_element = self.driver.find_element(By.CSS_SELECTOR, ".author-wrapper .author-name")
                content['author'] = author_element.text.strip()
            except:
                content['author'] = ""
            
            # æå–ç‚¹èµæ•°ã€æ”¶è—æ•°ç­‰
            try:
                stats_elements = self.driver.find_elements(By.CSS_SELECTOR, ".note-detail-wrapper .count")
                content['stats'] = [elem.text.strip() for elem in stats_elements]
            except:
                content['stats'] = []
            
            # æå–å›¾ç‰‡é“¾æ¥
            try:
                img_elements = self.driver.find_elements(By.CSS_SELECTOR, ".note-detail-wrapper img")
                content['images'] = [img.get_attribute('src') for img in img_elements if img.get_attribute('src')]
            except:
                content['images'] = []
            
            return content
            
        except Exception as e:
            logger.error(f"âŒ æå–ç¬”è®°è¯¦æƒ…å¤±è´¥: {str(e)}")
            return {}
    
    def _save_page_source(self, note_url, session_id):
        """ä¿å­˜é¡µé¢æºç ç”¨äºè°ƒè¯•"""
        try:
            # ä»URLä¸­æå–note_id
            note_id = note_url.split('/')[-1] if '/' in note_url else 'unknown'
            
            # åˆ›å»ºæ–‡ä»¶å
            filename = f"{note_id}_{session_id}_source.html"
            filepath = os.path.join(self.notes_dir, filename)
            
            # ä¿å­˜é¡µé¢æºç 
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(self.driver.page_source)
            
            logger.debug(f"ğŸ“ é¡µé¢æºç å·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜é¡µé¢æºç å¤±è´¥: {str(e)}")
    
    def _save_batch_results(self, results, session_id):
        """ä¿å­˜æ‰¹é‡æå–ç»“æœ"""
        try:
            filename = f"batch_{session_id}_results.json"
            filepath = os.path.join(self.notes_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ æ‰¹é‡æå–ç»“æœå·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ‰¹é‡ç»“æœå¤±è´¥: {str(e)}")

def start_backend_crawl(notes_data: List[Dict[str, Any]], session_id: str = None) -> Dict[str, Any]:
    """
    å¯åŠ¨åå°çˆ¬å–ä»»åŠ¡çš„å…¥å£å‡½æ•° - ä½¿ç”¨äººä¸ºè¡Œä¸ºæ¨¡å¼
    
    Args:
        notes_data: ç¬”è®°æ•°æ®åˆ—è¡¨
        session_id: ä¼šè¯ID
        
    Returns:
        Dict: çˆ¬å–ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"ğŸš€ å¯åŠ¨åå°çˆ¬è™«ä»»åŠ¡ï¼ˆäººä¸ºè¡Œä¸ºæ¨¡å¼ï¼‰ï¼Œå…± {len(notes_data)} ä¸ªç¬”è®°")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = BackendXHSCrawler()
        
        # æå–ç¬”è®°é“¾æ¥å¹¶æ·»åŠ å¿…è¦çš„xsecå‚æ•°
        note_links = []
        for note in notes_data:
            note_url = None
            xsec_token = note.get('xsec_token', '')
            note_id = note.get('note_id') or note.get('id', '')
            
            # è·å–åŸºç¡€URL
            if 'link' in note and note['link']:
                note_url = note['link']
            elif 'url' in note and note['url']:
                note_url = note['url']
            elif 'note_url' in note and note['note_url']:
                note_url = note['note_url']
            elif note_id:
                # å¦‚æœæ²¡æœ‰URLä½†æœ‰note_idï¼Œæ„å»ºåŸºç¡€URL
                note_url = f"https://www.xiaohongshu.com/explore/{note_id}"
            
            if note_url:
                # æ·»åŠ xsecå‚æ•°ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
                if 'xsec_token' not in note_url and xsec_token:
                    separator = '&' if '?' in note_url else '?'
                    note_url = f"{note_url}{separator}xsec_source=pc_feed&xsec_token={xsec_token}"
                    logger.debug(f"ğŸ”— ä¸ºç¬”è®° {note_id} æ·»åŠ xsecå‚æ•°")
                elif not xsec_token:
                    logger.warning(f"âš ï¸ ç¬”è®° {note_id} ç¼ºå°‘xsec_tokenï¼Œå¯èƒ½å½±å“è®¿é—®æˆåŠŸç‡")
                
                note_links.append(note_url)
        
        if not note_links:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç¬”è®°é“¾æ¥")
            return {
                'success': False,
                'error': 'æ²¡æœ‰æœ‰æ•ˆçš„ç¬”è®°é“¾æ¥',
                'total_crawled': 0,
                'success_count': 0,
                'failed_count': 0
            }
        
        # ä½¿ç”¨æ–°çš„äººä¸ºè¡Œä¸ºæ¨¡å¼è¿›è¡Œæ‰¹é‡æå–
        results = crawler.batch_extract_notes(note_links, session_id)
        
        success_count = len([r for r in results if r.get('content')])
        failed_count = len(note_links) - success_count
        
        return {
            'success': True,
            'total_crawled': len(note_links),
            'success_count': success_count,
            'failed_count': failed_count,
            'results': results,
            'session_id': session_id
        }
        
    except Exception as e:
        logger.error(f"âŒ åå°çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'total_crawled': 0,
            'success_count': 0,
            'failed_count': 0
        }

if __name__ == '__main__':
    # æµ‹è¯•ç”¨ä¾‹
    test_notes = [
        {
            'note_id': '65a1b2c3d4e5f6789abcdef0', 
            'url': 'https://www.xiaohongshu.com/explore/65a1b2c3d4e5f6789abcdef0',
            'xsec_token': 'XYZ123456789ABC'
        },
        {
            'note_id': '65a1b2c3d4e5f6789abcdef1', 
            'url': 'https://www.xiaohongshu.com/explore/65a1b2c3d4e5f6789abcdef1',
            'xsec_token': 'ABC987654321XYZ'
        },
    ]
    
    result = start_backend_crawl(test_notes, "test_session")
    print(json.dumps(result, ensure_ascii=False, indent=2)) 