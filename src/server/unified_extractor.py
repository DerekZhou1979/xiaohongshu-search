#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ•°æ®æå–å™¨ - æ•´åˆæ‰€æœ‰æå–æ–¹æ³•çš„ç»Ÿä¸€æ¥å£
"""

import os
import re
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class UnifiedExtractor:
    """ç»Ÿä¸€æ•°æ®æå–å™¨"""
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # æå–ç­–ç•¥é…ç½®
        self.strategies = {
            'enhanced': True,
            'precise': True,
            'content': True,
            'hybrid': True
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed': 0,
            'success': 0,
            'failed': 0,
            'start_time': None,
            'strategy_stats': {}
        }
    
    def extract_from_html_files(self, pattern: str = "*.html", max_files: int = None) -> List[Dict[str, Any]]:
        """ä»HTMLæ–‡ä»¶æ‰¹é‡æå–ç¬”è®°æ•°æ®"""
        self.stats['start_time'] = datetime.now()
        results = []
        
        try:
            # å…ˆåœ¨resultsç›®å½•ä¸­æŸ¥æ‰¾HTMLæ–‡ä»¶
            results_dir = self.cache_dir / "results"
            temp_dir = self.cache_dir / "temp"
            
            html_files = []
            
            # ä¼˜å…ˆä»resultsç›®å½•æŸ¥æ‰¾
            if results_dir.exists():
                html_files.extend(list(results_dir.glob(pattern)))
                logger.info(f"åœ¨resultsç›®å½•æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
            
            # ç„¶åä»tempç›®å½•æŸ¥æ‰¾
            if temp_dir.exists():
                temp_files = list(temp_dir.glob(pattern))
                html_files.extend(temp_files)
                logger.info(f"åœ¨tempç›®å½•æ‰¾åˆ° {len(temp_files)} ä¸ªHTMLæ–‡ä»¶")
            
            if not html_files:
                logger.warning(f"åœ¨ {results_dir} å’Œ {temp_dir} ä¸­éƒ½æœªæ‰¾åˆ°HTMLæ–‡ä»¶")
                return []
            
            if max_files:
                html_files = html_files[:max_files]
            
            logger.info(f"æ€»å…±æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶å¾…å¤„ç†")
            
            for html_file in html_files:
                logger.info(f"å¤„ç†æ–‡ä»¶: {html_file.name}")
                
                extracted_data = self.extract_from_html_file_hybrid(str(html_file))
                
                if extracted_data and extracted_data.get('notes'):
                    results.extend(extracted_data['notes'])
                    logger.info(f"âœ… æˆåŠŸæå–: {len(extracted_data['notes'])} æ¡ç¬”è®°")
                else:
                    logger.warning(f"âš ï¸ æå–å¤±è´¥: {html_file.name}")
                    
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æå–å¤±è´¥: {e}")
        
        self._print_stats()
        return results
    
    def extract_from_html_file_hybrid(self, html_file_path: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨æ··åˆç­–ç•¥ä»HTMLæ–‡ä»¶æå–æ•°æ®"""
        self.stats['processed'] += 1
        
        try:
            with open(html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
                
            logger.info(f"å¼€å§‹æ··åˆç­–ç•¥æå–: {os.path.basename(html_file_path)}")
            
            all_results = {}
            
            # ç­–ç•¥1: ç²¾å‡†å®¹å™¨æå–
            if self.strategies['precise']:
                try:
                    precise_results = self._extract_precise_containers(html_content, html_file_path)
                    if precise_results and precise_results.get('notes'):
                        all_results['precise'] = precise_results
                        logger.info(f"âœ… ç²¾å‡†å®¹å™¨ç­–ç•¥: {len(precise_results['notes'])} æ¡")
                except Exception as e:
                    logger.error(f"âŒ ç²¾å‡†å®¹å™¨ç­–ç•¥å¤±è´¥: {e}")
            
            # é€‰æ‹©æœ€ä½³ç»“æœ
            best_result = self._select_best_result(all_results)
            
            if best_result:
                best_result['extraction_info'] = {
                    'source_file': html_file_path,
                    'extracted_at': datetime.now().isoformat(),
                    'extractor_version': '2.0.0',
                    'strategies_used': list(all_results.keys()),
                    'best_strategy': best_result.get('extraction_method', 'unknown')
                }
                
                self.stats['success'] += 1
                return best_result
            else:
                self.stats['failed'] += 1
                return None
                
        except Exception as e:
            logger.error(f"âŒ æ··åˆç­–ç•¥æå–å¤±è´¥: {e}")
            self.stats['failed'] += 1
            return None
    
    def _extract_precise_containers(self, html_content: str, source_file: str) -> Optional[Dict[str, Any]]:
        """ç²¾å‡†å®¹å™¨æå–ç­–ç•¥"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æ–¹æ³•1: æŸ¥æ‰¾å¸¦æœ‰data-note-idçš„note-card
            note_cards = soup.find_all('div', class_='note-card')
            note_containers = []
            
            for card in note_cards:
                note_id = card.get('data-note-id')
                if note_id:
                    note_containers.append({
                        'note_id': note_id,
                        'link_href': f'/explore/{note_id}',
                        'container': card,
                        'link_element': card
                    })
            
            # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°note-cardï¼Œåˆ™æŸ¥æ‰¾ä¼ ç»Ÿçš„exploreé“¾æ¥
            if not note_containers:
                explore_links = soup.find_all('a', href=re.compile(r'/explore/'))
                
                for link in explore_links:
                    href = link.get('href', '')
                    note_id_match = re.search(r'/explore/([a-f0-9]{24})', href)
                    
                    if note_id_match:
                        note_id = note_id_match.group(1)
                        
                        # å‘ä¸ŠæŸ¥æ‰¾åŒ…å«æ­¤é“¾æ¥çš„å®¹å™¨
                        container = link
                        for _ in range(5):
                            parent = container.parent
                            if parent:
                                container = parent
                                images = container.find_all('img')
                                if images:
                                    break
                        
                        note_containers.append({
                            'note_id': note_id,
                            'link_href': href,
                            'container': container,
                            'link_element': link
                        })
            
            logger.info(f"æ‰¾åˆ° {len(note_containers)} ä¸ªç¬”è®°å®¹å™¨")
            
            # ä»æ¯ä¸ªå®¹å™¨æå–è¯¦ç»†ä¿¡æ¯
            extracted_notes = []
            
            for i, container_info in enumerate(note_containers):
                note_id = container_info['note_id']
                container = container_info['container']
                
                # æå–å›¾ç‰‡
                images = container.find_all('img')
                image_urls = []
                
                for img in images:
                    src = img.get('src', '')
                    if 'xhscdn.com' in src:
                        if src.startswith('//'):
                            src = 'https:' + src
                        elif src.startswith('/'):
                            src = 'https://www.xiaohongshu.com' + src
                        image_urls.append(src)
                
                # æå–æ ‡é¢˜
                title_element = container.find('h3', class_='note-title') or container.find('.note-title')
                title = title_element.get_text(strip=True) if title_element else self._extract_title_from_container(container_info, "", i)
                
                # æå–æè¿°
                desc_element = container.find('p', class_='note-desc') or container.find('.note-desc')
                description = desc_element.get_text(strip=True) if desc_element else ""
                
                # æå–ä½œè€…
                author_element = container.find('div', class_='note-author')
                if author_element:
                    author_text = author_element.get_text(strip=True)
                    # ç§»é™¤@ç¬¦å·ï¼Œä¿ç•™ä½œè€…åç§°ï¼Œå»é™¤ç²‰ä¸æ•°ç­‰æ•°å­—åç¼€
                    author = re.sub(r'^@', '', author_text).strip()
                    # å»é™¤å¯èƒ½çš„ç²‰ä¸æ•°ï¼ˆå¦‚"1åƒ"ã€"3ä¸‡"ç­‰ï¼‰
                    author = re.sub(r'\s*\d+[åƒä¸‡kKwW]*\s*$', '', author).strip()
                    author = author or "æœªçŸ¥ä½œè€…"
                else:
                    author = "æœªçŸ¥ä½œè€…"
                
                # æå–äº’åŠ¨æ•°æ®
                stats = self._extract_stats_from_container(container)
                
                # æå–æ–‡æœ¬å†…å®¹
                text_content = container.get_text(separator=' ', strip=True)
                
                # æ„å»ºå®Œæ•´é“¾æ¥
                full_link = f"https://www.xiaohongshu.com{container_info['link_href']}"
                
                note_data = {
                    'note_id': note_id,
                    'title': title or 'æœªçŸ¥æ ‡é¢˜',
                    'desc': description or title or 'æš‚æ— æè¿°',
                    'link': full_link,
                    'cover': image_urls[0] if image_urls else '',
                    'cover_image': image_urls[0] if image_urls else '',
                    'images': image_urls,
                    'author': author,
                    'content': description or text_content[:200] + "..." if len(text_content) > 200 else text_content,
                    'likes': stats.get('likes', 0),
                    'comments': stats.get('comments', 0),
                    'collects': stats.get('collects', 0),
                    'like_count': stats.get('likes', 0),
                    'comment_count': stats.get('comments', 0),
                    'collect_count': stats.get('collects', 0),
                    'tags': ['å°çº¢ä¹¦æœç´¢'],
                    'raw_text': text_content,
                    'method': 'precise_containers',
                    'published': '',
                    'avatar': ''
                }
                
                extracted_notes.append(note_data)
            
            return {
                "count": len(extracted_notes),
                "notes": extracted_notes,
                "message": f"ç²¾å‡†å®¹å™¨æå–åˆ°{len(extracted_notes)}æ¡ç¬”è®°æ•°æ®",
                "status": "success",
                "extraction_method": "precise_containers"
            }
            
        except Exception as e:
            logger.error(f"ç²¾å‡†å®¹å™¨æå–å¤±è´¥: {e}")
            return None
    
    def _select_best_result(self, all_results: Dict) -> Optional[Dict[str, Any]]:
        """é€‰æ‹©æœ€ä½³æå–ç»“æœ"""
        if not all_results:
            return None
        
        # ç­–ç•¥ä¼˜å…ˆçº§ï¼šç²¾å‡†å®¹å™¨ > å¢å¼ºæå– > å†…å®¹ç»“æ„
        priority_order = ['precise', 'enhanced', 'content']
        
        for strategy in priority_order:
            if strategy in all_results:
                result = all_results[strategy]
                notes = result.get('notes', [])
                
                # éªŒè¯ç»“æœè´¨é‡
                if self._validate_result_quality(notes):
                    logger.info(f"âœ… é€‰æ‹©æœ€ä½³ç­–ç•¥: {strategy} ({len(notes)} æ¡ç¬”è®°)")
                    return result
        
        # å¦‚æœæ²¡æœ‰é«˜è´¨é‡ç»“æœï¼Œè¿”å›æ•°é‡æœ€å¤šçš„
        if all_results:
            best_strategy = max(all_results.keys(), key=lambda k: len(all_results[k].get('notes', [])))
            logger.info(f"âš ï¸ é€‰æ‹©æ•°é‡æœ€å¤šçš„ç­–ç•¥: {best_strategy}")
            return all_results[best_strategy]
        
        return None
    
    def _validate_result_quality(self, notes: List[Dict]) -> bool:
        """éªŒè¯ç»“æœè´¨é‡"""
        if not notes:
            return False
        
        # æ£€æŸ¥åŸºæœ¬å­—æ®µå®Œæ•´æ€§
        valid_count = 0
        for note in notes:
            if (note.get('note_id') and 
                note.get('title') and 
                note.get('link') and
                len(note.get('note_id', '')) == 24):  # å°çº¢ä¹¦IDé•¿åº¦
                valid_count += 1
        
        # è‡³å°‘50%çš„ç¬”è®°æœ‰æ•ˆ
        quality_rate = valid_count / len(notes)
        return quality_rate >= 0.5
    
    def _extract_title_from_container(self, container_info: Dict, text_content: str, index: int) -> str:
        """ä»å®¹å™¨æå–æ ‡é¢˜"""
        # æ–¹æ³•1ï¼šæŸ¥æ‰¾é“¾æ¥æ–‡æœ¬
        link_text = container_info['link_element'].get_text(strip=True)
        if link_text and len(link_text) > 3:
            return link_text
        
        # æ–¹æ³•2ï¼šæŸ¥æ‰¾å®¹å™¨ä¸­çš„å…³é”®è¯æ–‡æœ¬
        text_lines = text_content.split()
        for line in text_lines:
            if any(keyword in line for keyword in ['è€åº™', 'é»„é‡‘', 'ç å®']) and 5 < len(line) < 50:
                return line
        
        # æ–¹æ³•3ï¼šä½¿ç”¨é»˜è®¤æ ¼å¼
        return f"å°çº¢ä¹¦ç¬”è®° #{index+1}"
    
    def _extract_author_from_text(self, text_content: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–ä½œè€…ä¿¡æ¯"""
        author_patterns = [
            r'@([^\s<>]{2,20})',
            r'ä½œè€…[ï¼š:]\s*([^\s<>]{2,20})',
            r'by\s+([^\s<>]{2,20})'
        ]
        
        for pattern in author_patterns:
            author_match = re.search(pattern, text_content)
            if author_match:
                return author_match.group(1)
        
        return "æœªçŸ¥ä½œè€…"
    
    def _extract_stats_from_container(self, container) -> Dict[str, int]:
        """ä»å®¹å™¨ä¸­æå–äº’åŠ¨ç»Ÿè®¡æ•°æ®"""
        stats = {'likes': 0, 'comments': 0, 'collects': 0, 'shares': 0}
        
        try:
            # æŸ¥æ‰¾ç»Ÿè®¡æ•°æ®å®¹å™¨
            stats_container = container.find('div', class_='note-stats')
            if stats_container:
                stat_items = stats_container.find_all('span', class_='stat-item')
                
                for item in stat_items:
                    icon = item.find('i')
                    text = item.get_text(strip=True)
                    
                    # æå–æ•°å­—
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        count = int(numbers[0])
                        
                        # æ ¹æ®å›¾æ ‡ç±»å‹åˆ¤æ–­ç»Ÿè®¡ç±»å‹
                        if icon:
                            icon_class = icon.get('class', [])
                            if 'fa-heart' in icon_class:
                                stats['likes'] = count
                            elif 'fa-comment' in icon_class:
                                stats['comments'] = count
                            elif 'fa-star' in icon_class:
                                stats['collects'] = count
                            elif 'fa-share' in icon_class:
                                stats['shares'] = count
                
        except Exception as e:
            logger.debug(f"æå–ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
        
        return stats
    
    def save_unified_results(self, results: List[Dict[str, Any]], keyword: str = "search") -> str:
        """ä¿å­˜ç»Ÿä¸€æ ¼å¼çš„ç»“æœ"""
        try:
            # æ„å»ºAPIå“åº”æ ¼å¼
            api_response = {
                "keyword": keyword,
                "count": len(results),
                "notes": results,
                "message": f"ç»Ÿä¸€æå–å™¨æˆåŠŸæå–{len(results)}æ¡ç¬”è®°æ•°æ®",
                "status": "success",
                "extraction_info": {
                    "extractor_version": "2.0.0",
                    "processed_at": datetime.now().isoformat(),
                    "strategies_used": list(set(note.get('method', 'unknown') for note in results))
                }
            }
            
            # ä¿å­˜JSONç»“æœ
            json_file = self.cache_dir / f"unified_extracted_{keyword}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(api_response, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ç»Ÿä¸€ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
            return str(json_file)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»Ÿä¸€ç»“æœå¤±è´¥: {e}")
            raise
    
    def _generate_html_preview(self, results: List[Dict[str, Any]], keyword: str) -> str:
        """ç”ŸæˆHTMLé¢„è§ˆé¡µé¢"""
        try:
            html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç»Ÿä¸€æå–å™¨ç»“æœ - {keyword}</title>
    <style>
        body {{ 
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1200px; margin: 0 auto; padding: 20px; background: #f5f5f5;
        }}
        .header {{ 
            text-align: center; margin-bottom: 30px; 
            background: linear-gradient(135deg, #667eea, #764ba2); color: white;
            padding: 30px; border-radius: 16px; box-shadow: 0 8px 25px rgba(102,126,234,0.3);
        }}
        .note-card {{ 
            background: white; border-radius: 12px; padding: 20px; margin-bottom: 20px; 
            box-shadow: 0 2px 10px rgba(0,0,0,0.1); transition: transform 0.2s;
        }}
        .note-card:hover {{ transform: translateY(-2px); }}
        .note-title {{ 
            font-size: 18px; font-weight: 600; color: #333; margin-bottom: 10px;
        }}
        .note-meta {{ 
            display: flex; align-items: center; gap: 15px; margin-bottom: 15px; 
            color: #666; font-size: 14px;
        }}
        .note-images {{ 
            display: flex; gap: 10px; margin-bottom: 15px; flex-wrap: wrap;
        }}
        .note-image {{ 
            width: 100px; height: 100px; object-fit: cover; border-radius: 8px;
        }}
        .note-link {{ 
            color: #667eea; text-decoration: none; font-weight: 500;
        }}
        .method-badge {{ 
            background: #667eea; color: white; padding: 4px 8px; border-radius: 12px; font-size: 12px;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ” ç»Ÿä¸€æå–å™¨ç»“æœ</h1>
        <h2>å…³é”®è¯ï¼š{keyword}</h2>
        <div>å…±æ‰¾åˆ° {len(results)} æ¡ç¬”è®° | æå–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</div>
    </div>
    
    <div class="notes-container">
"""
            
            for i, note in enumerate(results, 1):
                images_html = ""
                if note.get('images'):
                                         images_html = f"""
                        <div class="note-images">
                            {' '.join([f'<img src="{img}" alt="ç¬”è®°å›¾ç‰‡" class="note-image" onerror="this.style.display=' + "'none'" + '">' for img in note['images'][:3]])}
                        </div>
                    """
                
                html_content += f"""
        <div class="note-card">
            <div class="note-title">{note.get('title', 'æœªçŸ¥æ ‡é¢˜')}</div>
            <div class="note-meta">
                <span>ğŸ‘¤ {note.get('author', 'æœªçŸ¥ä½œè€…')}</span>
                <span>â¤ï¸ {note.get('like_count', '0')}</span>
                <span>ğŸ’¬ {note.get('comment_count', '0')}</span>
                <span class="method-badge">{note.get('method', 'unknown')}</span>
            </div>
            {images_html}
            <div>
                <a href="{note.get('link', '#')}" target="_blank" class="note-link">ğŸ”— æŸ¥çœ‹å®Œæ•´ç¬”è®°</a>
            </div>
        </div>
                """
            
            html_content += """
    </div>
</body>
</html>
"""
            
            # ä¿å­˜HTMLæ–‡ä»¶
            html_file = self.cache_dir / f"unified_preview_{keyword}.html"
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            logger.info(f"âœ… HTMLé¢„è§ˆå·²ä¿å­˜åˆ°: {html_file}")
            return str(html_file)
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆHTMLé¢„è§ˆå¤±è´¥: {e}")
            return ""

    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if self.stats['start_time']:
            duration = datetime.now() - self.stats['start_time']
            logger.info("\n" + "="*50)
            logger.info("ğŸ“Š ç»Ÿä¸€æå–å™¨å¤„ç†ç»Ÿè®¡")
            logger.info("="*50)
            logger.info(f"æ€»å¤„ç†æ•°: {self.stats['processed']}")
            logger.info(f"æˆåŠŸæ•°: {self.stats['success']}")
            logger.info(f"å¤±è´¥æ•°: {self.stats['failed']}")
            logger.info(f"æˆåŠŸç‡: {self.stats['success']/max(self.stats['processed'], 1)*100:.1f}%")
            logger.info(f"å¤„ç†æ—¶é—´: {duration.total_seconds():.1f} ç§’")
            logger.info("="*50)

def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import sys
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    extractor = UnifiedExtractor()
    
    if len(sys.argv) > 1:
        keyword = sys.argv[1]
    else:
        keyword = "search"
    
    logger.info("ğŸš€ å¯åŠ¨ç»Ÿä¸€æ•°æ®æå–å™¨")
    
    # æ‰¹é‡å¤„ç†HTMLæ–‡ä»¶
    results = extractor.extract_from_html_files()
    
    if results:
        # ä¿å­˜ç»Ÿä¸€ç»“æœ
        saved_path = extractor.save_unified_results(results, keyword)
        logger.info(f"âœ… ç»Ÿä¸€æå–å®Œæˆï¼Œå…±å¤„ç† {len(results)} æ¡ç¬”è®°")
        logger.info(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {saved_path}")
    else:
        logger.warning("âŒ æœªæå–åˆ°ä»»ä½•æ•°æ®")

if __name__ == "__main__":
    main() 