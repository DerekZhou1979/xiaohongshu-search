"""
å°çº¢ä¹¦ç¬”è®°å†…å®¹ç”Ÿæˆå™¨
è´Ÿè´£åˆ†æåŸç¬”è®°å†…å®¹å¹¶ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–°çš„åŒç±»ç¬”è®°å†…å®¹
"""

import os
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
import re
import random
import requests
import hashlib
from urllib.parse import urlparse, urljoin
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class NoteContentGenerator:
    """ç¬”è®°å†…å®¹ç”Ÿæˆå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„cacheç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.cache_dir = os.path.join(project_root, "cache")
        self.debug_dir = os.path.join(self.cache_dir, "note_generation_debug")
        self.notes_dir = os.path.join(self.cache_dir, "notes")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.debug_dir, exist_ok=True)
        os.makedirs(self.notes_dir, exist_ok=True)
        
        # åˆå§‹åŒ–å¤§æ¨¡å‹è®¾ç½®ï¼ˆè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿï¼Œå®é™…å¯ä»¥æ¥å…¥OpenAI, Claudeç­‰ï¼‰
        self.ai_enabled = False  # é»˜è®¤å…³é—­ï¼Œå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡å¼€å¯
        
        # Cookieæ–‡ä»¶è·¯å¾„
        self.cookies_file = os.path.join('cache', 'cookies', 'xiaohongshu_cookies.json')
        
        logger.info("ç¬”è®°å†…å®¹ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_similar_note(self, original_note: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ®åŸç¬”è®°ç”ŸæˆåŒç±»ç¬”è®°å†…å®¹
        
        Args:
            original_note: åŸç¬”è®°ä¿¡æ¯ï¼ŒåŒ…å«title, content, tagsç­‰
            
        Returns:
            Dict: ç”Ÿæˆçš„ç¬”è®°å†…å®¹ï¼ŒåŒ…å«title, content, tags, suggestionsç­‰
        """
        try:
            logger.info(f"å¼€å§‹ç”ŸæˆåŒç±»ç¬”è®°ï¼ŒåŸç¬”è®°ID: {original_note.get('note_id')}")
            
            # åˆ›å»ºdebugä¼šè¯
            session_id = self._create_debug_session(original_note)
            
            # è·å–åŸç¬”è®°çš„å®Œæ•´å†…å®¹ï¼ˆé€šè¿‡ä»£ç†è®¿é—®ï¼‰
            note_detail = self._fetch_note_detail(original_note.get('note_id'), session_id)
            self._save_debug_info(session_id, "note_detail", note_detail)
            
            # åˆ†æåŸç¬”è®°å†…å®¹
            analysis_result = self._analyze_original_note(note_detail if note_detail.get('success') else original_note)
            self._save_debug_info(session_id, "analysis", analysis_result)
            
            # ç”Ÿæˆæ–°ç¬”è®°å†…å®¹
            source_note = note_detail if note_detail.get('success') else original_note
            if self.ai_enabled:
                generated_note = self._generate_with_ai(source_note, analysis_result)
            else:
                generated_note = self._generate_with_templates(source_note, analysis_result)
            
            self._save_debug_info(session_id, "generated_note", generated_note)
            
            # æ·»åŠ ç”Ÿæˆæ—¶é—´å’Œä¼šè¯ID
            generated_note['generated_at'] = datetime.now().isoformat()
            generated_note['debug_session_id'] = session_id
            
            # æ·»åŠ åŸç¬”è®°è¯¦ç»†ä¿¡æ¯
            if note_detail and note_detail.get('success'):
                generated_note['original_note_detail'] = {
                    'title': note_detail.get('title', ''),
                    'content': note_detail.get('content', ''),
                    'tags': note_detail.get('tags', []),
                    'images': note_detail.get('images', []),
                    'author': note_detail.get('author', ''),
                    'note_id': original_note.get('note_id', ''),
                    'source_file': note_detail.get('source_file', ''),
                    'images_dir': note_detail.get('images_dir', '')
                }
            
            logger.info(f"ç¬”è®°ç”Ÿæˆå®Œæˆï¼Œä¼šè¯ID: {session_id}")
            return generated_note
            
        except Exception as e:
            logger.error(f"ç”ŸæˆåŒç±»ç¬”è®°å¤±è´¥: {str(e)}")
            raise
    
    def _create_debug_session(self, original_note: Dict[str, Any]) -> str:
        """åˆ›å»ºdebugä¼šè¯"""
        session_id = f"note_gen_{int(time.time())}_{random.randint(1000, 9999)}"
        
        session_info = {
            "session_id": session_id,
            "created_at": datetime.now().isoformat(),
            "original_note_id": original_note.get('note_id', 'unknown'),
            "original_note_title": original_note.get('title', 'æœªçŸ¥æ ‡é¢˜'),
            "status": "started"
        }
        
        session_file = os.path.join(self.debug_dir, f"{session_id}_session.json")
        with open(session_file, 'w', encoding='utf-8') as f:
            json.dump(session_info, f, ensure_ascii=False, indent=2)
        
        return session_id
    
    def _save_debug_info(self, session_id: str, step: str, data: Any):
        """ä¿å­˜debugä¿¡æ¯"""
        try:
            debug_file = os.path.join(self.debug_dir, f"{session_id}_{step}.json")
            debug_data = {
                "session_id": session_id,
                "step": step,
                "timestamp": datetime.now().isoformat(),
                "data": data
            }
            
            with open(debug_file, 'w', encoding='utf-8') as f:
                json.dump(debug_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"ä¿å­˜debugä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def _analyze_original_note(self, original_note: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æåŸç¬”è®°å†…å®¹"""
        title = original_note.get('title', '')
        content = original_note.get('content', '')
        tags = original_note.get('tags', [])
        
        analysis = {
            "content_type": self._detect_content_type(title, content),
            "tone": self._detect_tone(title, content), 
            "topics": self._extract_topics(title, content),
            "keywords": self._extract_keywords(title, content),
            "structure": self._analyze_structure(content),
            "engagement_elements": self._find_engagement_elements(title, content),
            "original_tags": tags
        }
        
        return analysis
    
    def _detect_content_type(self, title: str, content: str) -> str:
        """æ£€æµ‹å†…å®¹ç±»å‹"""
        text = (title + " " + content).lower()
        
        if any(word in text for word in ['æ•™ç¨‹', 'æ”»ç•¥', 'æ­¥éª¤', 'æ–¹æ³•', 'å¦‚ä½•', 'æ€ä¹ˆ']):
            return "æ•™ç¨‹æ”»ç•¥"
        elif any(word in text for word in ['åˆ†äº«', 'æ¨è', 'ç§è‰', 'å¥½ç‰©', 'å€¼å¾—']):
            return "åˆ†äº«æ¨è"
        elif any(word in text for word in ['ç©¿æ­', 'æ­é…', 'æœè£…', 'è¡£æœ', 'æ—¶å°š']):
            return "ç©¿æ­æ—¶å°š"
        elif any(word in text for word in ['ç¾é£Ÿ', 'åƒ', 'é¤å…', 'èœè°±', 'æ–™ç†']):
            return "ç¾é£Ÿ"
        elif any(word in text for word in ['æ—…è¡Œ', 'æ—…æ¸¸', 'æ™¯ç‚¹', 'æ‰“å¡', 'å‡ºè¡Œ']):
            return "æ—…è¡Œ"
        elif any(word in text for word in ['æŠ¤è‚¤', 'åŒ–å¦†', 'ç¾å¦†', 'ä¿å…»', 'æŠ¤ç†']):
            return "ç¾å¦†æŠ¤è‚¤"
        elif any(word in text for word in ['ç”Ÿæ´»', 'æ—¥å¸¸', 'vlog', 'è®°å½•']):
            return "ç”Ÿæ´»æ—¥å¸¸"
        else:
            return "ç»¼åˆåˆ†äº«"
    
    def _detect_tone(self, title: str, content: str) -> str:
        """æ£€æµ‹è¯­è°ƒé£æ ¼"""
        text = (title + " " + content).lower()
        
        if any(word in text for word in ['ï¼', '!', 'å“‡', 'å¤ªå¥½äº†', 'è¶…çº§', 'å·¨']):
            return "æ´»æ³¼å…´å¥‹"
        elif any(word in text for word in ['æ¸©æŸ”', 'è½»æ¾', 'èˆ’é€‚', 'é™è°§']):
            return "æ¸©æŸ”æ²»æ„ˆ"
        elif any(word in text for word in ['ä¸“ä¸š', 'å»ºè®®', 'æ¨è', 'åˆ†æ']):
            return "ä¸“ä¸šç†æ€§"
        elif any(word in text for word in ['å¯çˆ±', 'èŒ', 'å°ä»™å¥³', 'å®è´']):
            return "å¯çˆ±ç”œç¾"
        else:
            return "è‡ªç„¶äº²å’Œ"
    
    def _extract_topics(self, title: str, content: str) -> List[str]:
        """æå–ä¸»é¢˜å…³é”®è¯"""
        text = title + " " + content
        
        # ç®€å•çš„ä¸»é¢˜æå–é€»è¾‘
        topics = []
        
        # å¸¸è§ä¸»é¢˜å…³é”®è¯
        topic_keywords = {
            "ç¾é£Ÿ": ["ç¾é£Ÿ", "åƒ", "é¤å…", "èœè°±", "æ–™ç†", "å°é£Ÿ", "ç”œå“"],
            "ç©¿æ­": ["ç©¿æ­", "æœè£…", "è¡£æœ", "æ­é…", "æ—¶å°š", "é£æ ¼"],
            "æŠ¤è‚¤": ["æŠ¤è‚¤", "ä¿å…»", "è‚Œè‚¤", "é¢è†œ", "ç²¾å", "ä¹³æ¶²"],
            "åŒ–å¦†": ["åŒ–å¦†", "å½©å¦†", "å£çº¢", "çœ¼å½±", "ç²‰åº•", "ç¾å¦†"],
            "æ—…è¡Œ": ["æ—…è¡Œ", "æ—…æ¸¸", "æ™¯ç‚¹", "æ‰“å¡", "å‡ºè¡Œ", "åº¦å‡"],
            "ç”Ÿæ´»": ["ç”Ÿæ´»", "æ—¥å¸¸", "å®¶å±…", "æ”¶çº³", "æ•´ç†"],
            "å­¦ä¹ ": ["å­¦ä¹ ", "è¯»ä¹¦", "çŸ¥è¯†", "æŠ€èƒ½", "æå‡"],
            "å¥èº«": ["å¥èº«", "è¿åŠ¨", "é”»ç‚¼", "ç‘œä¼½", "å‡è‚¥"]
        }
        
        for topic, keywords in topic_keywords.items():
            if any(keyword in text for keyword in keywords):
                topics.append(topic)
        
        return topics[:3]  # æœ€å¤šè¿”å›3ä¸ªä¸»é¢˜
    
    def _extract_keywords(self, title: str, content: str) -> List[str]:
        """æå–å…³é”®è¯"""
        text = title + " " + content
        
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´sophisticatedçš„NLPæŠ€æœ¯ï¼‰
        keywords = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å¯èƒ½çš„å…³é”®è¯
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œæå–ä¸€äº›å¸¸è§çš„åè¯æ€§çŸ­è¯­
        try:
            import jieba
            # åˆ†è¯
            words = jieba.lcut(text)
            
            # è¿‡æ»¤è¯æ±‡
            filtered_words = [
                word for word in words 
                if len(word) >= 2 and word not in ['çš„', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'ä¸', 'äº†', 'ç€', 'è¿‡']
            ]
            
            # ç»Ÿè®¡è¯é¢‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
            word_freq = {}
            for word in filtered_words:
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # æŒ‰é¢‘ç‡æ’åºï¼Œå–å‰8ä¸ª
            keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:8]
            keywords = [word[0] for word in keywords]
            
        except:
            # å¦‚æœjiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„æ–¹æ³•
            keywords = re.findall(r'[\u4e00-\u9fff]{2,4}', text)[:8]
        
        return keywords
    
    def _fetch_note_detail(self, note_id: str, session_id: str) -> Dict[str, Any]:
        """
        é€šè¿‡ä»£ç†æ–¹å¼è·å–ç¬”è®°è¯¦ç»†å†…å®¹
        
        Args:
            note_id: ç¬”è®°ID
            session_id: ä¼šè¯ID
            
        Returns:
            Dict: åŒ…å«ç¬”è®°è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹è·å–ç¬”è®°è¯¦æƒ…: {note_id}")
            
            # æ„å»ºç¬”è®°URL
            note_url = f"https://www.xiaohongshu.com/explore/{note_id}"
            
            # åˆ›å»ºæµè§ˆå™¨å®ä¾‹
            driver = self._create_browser_instance()
            if not driver:
                return {"success": False, "error": "æ— æ³•åˆ›å»ºæµè§ˆå™¨å®ä¾‹"}
            
            try:
                # åŠ è½½cookies
                self._load_cookies_to_browser(driver)
                
                # è®¿é—®ç¬”è®°é¡µé¢
                logger.info(f"è®¿é—®ç¬”è®°é¡µé¢: {note_url}")
                driver.get(note_url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # ç­‰å¾…å†…å®¹åŠ è½½
                time.sleep(3)
                
                # è·å–é¡µé¢æºç 
                page_source = driver.page_source
                
                # ä¿å­˜é¡µé¢æºç 
                source_file = self._save_page_source(note_id, page_source, session_id)
                
                # è§£æé¡µé¢å†…å®¹
                note_detail = self._parse_note_content(page_source, note_id, session_id)
                note_detail['source_file'] = source_file
                note_detail['success'] = True
                
                logger.info(f"ç¬”è®°è¯¦æƒ…è·å–æˆåŠŸ: {note_id}")
                return note_detail
                
            finally:
                driver.quit()
                
        except Exception as e:
            logger.error(f"è·å–ç¬”è®°è¯¦æƒ…å¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _create_browser_instance(self):
        """åˆ›å»ºæµè§ˆå™¨å®ä¾‹"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--ignore-certificate-errors')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # å°è¯•ä¸åŒçš„ChromeDriverè·¯å¾„
            driver_paths = [
                'drivers/chromedriver-mac-arm64/chromedriver',
                'drivers/chromedriver',
                '/usr/local/bin/chromedriver',
                '/opt/homebrew/bin/chromedriver'
            ]
            
            for driver_path in driver_paths:
                if os.path.exists(driver_path):
                    from selenium.webdriver.chrome.service import Service
                    service = Service(driver_path)
                    driver = webdriver.Chrome(service=service, options=chrome_options)
                    return driver
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æŒ‡å®šè·¯å¾„ï¼Œå°è¯•ç³»ç»ŸPATHä¸­çš„chromedriver
            driver = webdriver.Chrome(options=chrome_options)
            return driver
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæµè§ˆå™¨å®ä¾‹å¤±è´¥: {str(e)}")
            return None
    
    def _load_cookies_to_browser(self, driver):
        """åŠ è½½cookiesåˆ°æµè§ˆå™¨"""
        try:
            if os.path.exists(self.cookies_file):
                # å…ˆè®¿é—®ä¸»é¡µä»¥è®¾ç½®åŸŸå
                driver.get("https://www.xiaohongshu.com")
                time.sleep(1)
                
                with open(self.cookies_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                
                for cookie in cookies:
                    try:
                        driver.add_cookie(cookie)
                    except Exception as e:
                        logger.debug(f"æ·»åŠ cookieå¤±è´¥: {str(e)}")
                        continue
                
                logger.info("CookiesåŠ è½½å®Œæˆ")
            else:
                logger.warning("Cookiesæ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"åŠ è½½cookieså¤±è´¥: {str(e)}")
    
    def _save_page_source(self, note_id: str, page_source: str, session_id: str) -> str:
        """ä¿å­˜é¡µé¢æºç """
        try:
            filename = f"{note_id}_{session_id}_source.html"
            file_path = os.path.join(self.notes_dir, filename)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(page_source)
            
            logger.info(f"é¡µé¢æºç å·²ä¿å­˜: {file_path}")
            return file_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜é¡µé¢æºç å¤±è´¥: {str(e)}")
            return ""
    
    def _parse_note_content(self, page_source: str, note_id: str, session_id: str) -> Dict[str, Any]:
        """è§£æç¬”è®°å†…å®¹"""
        try:
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # æå–æ ‡é¢˜
            title = self._extract_title(soup)
            
            # æå–å†…å®¹
            content = self._extract_content(soup)
            
            # æå–æ ‡ç­¾
            tags = self._extract_tags(soup)
            
            # æå–ä½œè€…ä¿¡æ¯
            author = self._extract_author(soup)
            
            # ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡
            images = self._download_note_images(soup, note_id, session_id)
            
            return {
                'title': title,
                'content': content,
                'tags': tags,
                'author': author,
                'images': images,
                'images_dir': os.path.join(self.notes_dir, f"{note_id}_{session_id}_images")
            }
            
        except Exception as e:
            logger.error(f"è§£æç¬”è®°å†…å®¹å¤±è´¥: {str(e)}")
            return {
                'title': '',
                'content': '',
                'tags': [],
                'author': '',
                'images': []
            }
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """æå–æ ‡é¢˜"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            title_selectors = [
                'h1.title',
                '.note-title',
                '[data-testid="note-title"]',
                'h1',
                '.content-title',
                'title'
            ]
            
            for selector in title_selectors:
                element = soup.select_one(selector)
                if element and element.get_text(strip=True):
                    title = element.get_text(strip=True)
                    logger.info(f"æå–åˆ°æ ‡é¢˜: {title[:50]}...")
                    return title
            
            # ä»é¡µé¢titleæ ‡ç­¾æå–
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
                # æ¸…ç†æ ‡é¢˜
                title = title.replace(' - å°çº¢ä¹¦', '').replace(' | å°çº¢ä¹¦', '').strip()
                if title:
                    return title
            
            return "æœªæ‰¾åˆ°æ ‡é¢˜"
            
        except Exception as e:
            logger.error(f"æå–æ ‡é¢˜å¤±è´¥: {str(e)}")
            return "æ ‡é¢˜æå–å¤±è´¥"
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """æå–å†…å®¹"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            content_selectors = [
                '.note-content',
                '.content-text',
                '[data-testid="note-content"]',
                '.desc',
                '.note-desc',
                '.content-desc'
            ]
            
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element and element.get_text(strip=True):
                    content = element.get_text(strip=True)
                    logger.info(f"æå–åˆ°å†…å®¹: {len(content)} å­—ç¬¦")
                    return content
            
            # å°è¯•ä»scriptæ ‡ç­¾ä¸­æå–JSONæ•°æ®
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string and 'window.__INITIAL_STATE__' in script.string:
                    try:
                        # æå–JSONæ•°æ®
                        json_start = script.string.find('{')
                        json_end = script.string.rfind('}') + 1
                        if json_start != -1 and json_end != -1:
                            json_str = script.string[json_start:json_end]
                            data = json.loads(json_str)
                            
                            # åœ¨JSONä¸­æŸ¥æ‰¾å†…å®¹
                            content = self._extract_content_from_json(data)
                            if content:
                                return content
                    except:
                        continue
            
            return "æœªæ‰¾åˆ°å†…å®¹"
            
        except Exception as e:
            logger.error(f"æå–å†…å®¹å¤±è´¥: {str(e)}")
            return "å†…å®¹æå–å¤±è´¥"
    
    def _extract_content_from_json(self, data: dict) -> str:
        """ä»JSONæ•°æ®ä¸­æå–å†…å®¹"""
        try:
            def search_content(obj, path=""):
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        if key in ['desc', 'content', 'text', 'title'] and isinstance(value, str) and len(value) > 10:
                            return value
                        result = search_content(value, f"{path}.{key}")
                        if result:
                            return result
                elif isinstance(obj, list):
                    for i, item in enumerate(obj):
                        result = search_content(item, f"{path}[{i}]")
                        if result:
                            return result
                return None
            
            return search_content(data) or ""
            
        except Exception as e:
            logger.error(f"ä»JSONæå–å†…å®¹å¤±è´¥: {str(e)}")
            return ""
    
    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """æå–æ ‡ç­¾"""
        try:
            tags = []
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            tag_selectors = [
                '.tag',
                '.hashtag',
                '.topic',
                '[data-testid="tag"]',
                '.note-tag',
                '.topic-tag'
            ]
            
            for selector in tag_selectors:
                elements = soup.select(selector)
                for element in elements:
                    tag_text = element.get_text(strip=True)
                    if tag_text and tag_text not in tags:
                        # æ¸…ç†æ ‡ç­¾æ–‡æœ¬
                        tag_text = tag_text.replace('#', '').strip()
                        if tag_text:
                            tags.append(tag_text)
            
            # ä»æ–‡æœ¬ä¸­æå–#æ ‡ç­¾
            text_content = soup.get_text()
            hashtag_pattern = r'#([^#\s]+)'
            hashtags = re.findall(hashtag_pattern, text_content)
            for tag in hashtags:
                if tag not in tags:
                    tags.append(tag)
            
            logger.info(f"æå–åˆ°æ ‡ç­¾: {tags}")
            return tags[:10]  # æœ€å¤šè¿”å›10ä¸ªæ ‡ç­¾
            
        except Exception as e:
            logger.error(f"æå–æ ‡ç­¾å¤±è´¥: {str(e)}")
            return []
    
    def _extract_author(self, soup: BeautifulSoup) -> str:
        """æå–ä½œè€…ä¿¡æ¯"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            author_selectors = [
                '.author-name',
                '.user-name',
                '[data-testid="author"]',
                '.note-author',
                '.username'
            ]
            
            for selector in author_selectors:
                element = soup.select_one(selector)
                if element and element.get_text(strip=True):
                    author = element.get_text(strip=True)
                    logger.info(f"æå–åˆ°ä½œè€…: {author}")
                    return author
            
            return "æœªçŸ¥ä½œè€…"
            
        except Exception as e:
            logger.error(f"æå–ä½œè€…å¤±è´¥: {str(e)}")
            return "ä½œè€…æå–å¤±è´¥"
    
    def _download_note_images(self, soup: BeautifulSoup, note_id: str, session_id: str) -> List[Dict[str, str]]:
        """ä¸‹è½½ç¬”è®°å›¾ç‰‡"""
        try:
            images = []
            images_dir = os.path.join(self.notes_dir, f"{note_id}_{session_id}_images")
            os.makedirs(images_dir, exist_ok=True)
            
            # æŸ¥æ‰¾å›¾ç‰‡å…ƒç´ 
            img_elements = soup.find_all('img')
            
            for i, img in enumerate(img_elements):
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if not src:
                    continue
                
                # è¿‡æ»¤æ‰å°å›¾æ ‡å’Œæ— å…³å›¾ç‰‡
                if any(keyword in src.lower() for keyword in ['icon', 'avatar', 'logo', 'button']):
                    continue
                
                # ç¡®ä¿æ˜¯å®Œæ•´URL
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = 'https://www.xiaohongshu.com' + src
                
                # ä¸‹è½½å›¾ç‰‡
                image_info = self._download_single_image(src, images_dir, f"{note_id}_{i}")
                if image_info:
                    images.append(image_info)
            
            logger.info(f"ä¸‹è½½äº† {len(images)} å¼ å›¾ç‰‡")
            return images
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {str(e)}")
            return []
    
    def _download_single_image(self, url: str, save_dir: str, filename_prefix: str) -> Optional[Dict[str, str]]:
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://www.xiaohongshu.com/'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            content_type = response.headers.get('content-type', '')
            if 'jpeg' in content_type or 'jpg' in content_type:
                ext = '.jpg'
            elif 'png' in content_type:
                ext = '.png'
            elif 'webp' in content_type:
                ext = '.webp'
            else:
                ext = '.jpg'  # é»˜è®¤
            
            filename = f"{filename_prefix}{ext}"
            file_path = os.path.join(save_dir, filename)
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            # ç”ŸæˆWebè®¿é—®è·¯å¾„
            web_path = f"/cache/notes/{os.path.basename(save_dir)}/{filename}"
            
            return {
                'original_url': url,
                'local_path': file_path,
                'web_path': web_path,
                'filename': filename
            }
            
        except Exception as e:
            logger.error(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {url}: {str(e)}")
            return None
    
    def _analyze_structure(self, content: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹ç»“æ„"""
        lines = content.split('\n')
        
        structure = {
            "line_count": len(lines),
            "has_list": 'â€¢' in content or 'Â·' in content or any(line.strip().startswith(('1.', '2.', '3.')) for line in lines),
            "has_emoji": bool(re.search(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', content)),
            "paragraph_count": len([line for line in lines if line.strip()]),
            "avg_line_length": sum(len(line) for line in lines) / max(len(lines), 1)
        }
        
        return structure
    
    def _find_engagement_elements(self, title: str, content: str) -> List[str]:
        """æ‰¾å‡ºå¸å¼•äººçš„å…ƒç´ """
        text = title + " " + content
        elements = []
        
        # æ£€æµ‹å„ç§å¸å¼•äººçš„å…ƒç´ 
        if '?' in text or 'ï¼Ÿ' in text:
            elements.append("äº’åŠ¨é—®å¥")
        
        if any(word in text for word in ['åˆ†äº«', 'æ¨è', 'å¿…ä¹°', 'å¿…çœ‹']):
            elements.append("æ¨èè¯­æ°”")
            
        if any(word in text for word in ['è¶…çº§', 'å·¨', 'ç‰¹åˆ«', 'éå¸¸']):
            elements.append("å¼ºè°ƒè¯æ±‡")
            
        if re.search(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]', text):
            elements.append("è¡¨æƒ…ç¬¦å·")
            
        if any(word in text for word in ['æ”»ç•¥', 'ç§˜ç±', 'æŠ€å·§', 'å¦™æ‹›']):
            elements.append("å®ç”¨ä»·å€¼")
        
        return elements
    
    def _generate_with_templates(self, original_note: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆæ–°ç¬”è®°å†…å®¹ï¼ˆå½“AIä¸å¯ç”¨æ—¶çš„å¤‡é€‰æ–¹æ¡ˆï¼‰"""
        
        content_type = analysis['content_type']
        tone = analysis['tone']
        topics = analysis['topics']
        keywords = analysis['keywords']
        
        # æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©æ¨¡æ¿
        templates = self._get_templates_by_type(content_type)
        selected_template = random.choice(templates)
        
        # ç”Ÿæˆæ ‡é¢˜
        title = self._generate_title_from_template(selected_template, topics, keywords, tone)
        
        # ç”Ÿæˆå†…å®¹
        content = self._generate_content_from_template(selected_template, original_note, analysis)
        
        # ç”Ÿæˆæ ‡ç­¾
        tags = self._generate_tags(topics, keywords, analysis['original_tags'])
        
        # ç”Ÿæˆåˆ›ä½œå»ºè®®
        suggestions = self._generate_suggestions(content_type, tone, analysis)
        
        return {
            "title": title,
            "content": content,
            "tags": tags,
            "suggestions": suggestions,
            "generation_method": "template_based",
            "content_type": content_type,
            "tone": tone
        }
    
    def _generate_with_ai(self, original_note: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–°ç¬”è®°å†…å®¹ï¼ˆéœ€è¦é…ç½®AI APIï¼‰"""
        # è¿™é‡Œæ˜¯AIç”Ÿæˆçš„æ¥å£ï¼Œéœ€è¦æ ¹æ®å®é™…ä½¿ç”¨çš„å¤§æ¨¡å‹è¿›è¡Œå®ç°
        # æ¯”å¦‚OpenAI GPT, Claude, æˆ–è€…æœ¬åœ°æ¨¡å‹ç­‰
        
        # æš‚æ—¶è¿”å›æ¨¡æ¿ç”Ÿæˆçš„ç»“æœ
        logger.info("AIç”ŸæˆåŠŸèƒ½æš‚æœªé…ç½®ï¼Œä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ")
        return self._generate_with_templates(original_note, analysis)
    
    def _get_templates_by_type(self, content_type: str) -> List[Dict[str, Any]]:
        """æ ¹æ®å†…å®¹ç±»å‹è·å–æ¨¡æ¿"""
        
        templates = {
            "æ•™ç¨‹æ”»ç•¥": [
                {
                    "title_patterns": [
                        "{topic}æ–°æ‰‹å¿…çœ‹ï¼{keyword}è¯¦ç»†æ•™ç¨‹æ¥å•¦ğŸ“š",
                        "è¶…å®ç”¨{topic}æ”»ç•¥ï¼{keyword}è¿™æ ·åšå°±å¯¹äº†âœ¨",
                        "{keyword}å®Œæ•´æ•™ç¨‹åˆ†äº«ï¼Œ{topic}å°ç™½ä¹Ÿèƒ½å­¦ä¼šğŸ¯"
                    ],
                    "content_structure": "intro_problem_solution_tips_conclusion"
                }
            ],
            "åˆ†äº«æ¨è": [
                {
                    "title_patterns": [
                        "çœŸçš„å¥½ç”¨ï¼{keyword}åˆ†äº«ç»™å¤§å®¶ğŸ’•",
                        "åˆå‘ç°äº†{topic}å¥½ç‰©ï¼{keyword}å¼ºçƒˆæ¨èâœ¨",
                        "å¿ä¸ä½è¦æ¨èçš„{keyword}ï¼Œ{topic}çˆ±å¥½è€…å¿…å…¥ğŸ›’"
                    ],
                    "content_structure": "discovery_features_experience_recommendation"
                }
            ]
        }
        
        return templates.get(content_type, [
            {
                "title_patterns": [
                    "åˆ†äº«ä¸€ä¸ª{keyword}ï¼Œå…³äº{topic}çš„å°å¿ƒå¾—ğŸ’¡",
                    "{keyword}ä½“éªŒåˆ†äº«ï¼{topic}çœŸçš„ä¸é”™ğŸ‘",
                    "ä»Šå¤©æ¥èŠèŠ{topic}ï¼Œ{keyword}å¿ƒå¾—åˆ†äº«âœ¨"
                ],
                "content_structure": "general_sharing"
            }
        ])
    
    def _generate_title_from_template(self, template: Dict[str, Any], topics: List[str], keywords: List[str], tone: str) -> str:
        """ä»æ¨¡æ¿ç”Ÿæˆæ ‡é¢˜"""
        
        title_patterns = template['title_patterns']
        selected_pattern = random.choice(title_patterns)
        
        # é€‰æ‹©åˆé€‚çš„topicå’Œkeyword
        topic = topics[0] if topics else "ç”Ÿæ´»"
        keyword = keywords[0] if keywords else "åˆ†äº«"
        
        # æ ¹æ®å­£èŠ‚æ·»åŠ é¢å¤–ä¿¡æ¯
        season = self._get_current_season()
        
        # æ›¿æ¢å ä½ç¬¦
        title = selected_pattern.format(
            topic=topic,
            keyword=keyword,  
            season=season
        )
        
        return title
    
    def _generate_content_from_template(self, template: Dict[str, Any], original_note: Dict[str, Any], analysis: Dict[str, Any]) -> str:
        """ä»æ¨¡æ¿ç”Ÿæˆå†…å®¹"""
        
        keywords = analysis['keywords']
        topics = analysis['topics']
        
        topic = topics[0] if topics else "è¿™ä¸ªæ–¹æ³•"
        keyword = keywords[0] if keywords else "æŠ€å·§"
        
        content = f"""ä»Šå¤©æ¥åˆ†äº«ä¸€ä¸ªå…³äº{topic}çš„å®ç”¨{keyword}ï¼

ğŸ¤” é‡åˆ°çš„é—®é¢˜ï¼š
å¾ˆå¤šæœ‹å‹éƒ½åœ¨é—®å…³äº{topic}çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯{keyword}æ–¹é¢çš„å›°æƒ‘ã€‚

ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š
ç»è¿‡æˆ‘çš„å®è·µæ€»ç»“ï¼Œå‘ç°äº†ä¸€ä¸ªç‰¹åˆ«å¥½ç”¨çš„æ–¹æ³•ï¼š

1ï¸âƒ£ é¦–å…ˆï¼Œè¦äº†è§£{keyword}çš„åŸºæœ¬åŸç†
2ï¸âƒ£ ç„¶åï¼Œå‡†å¤‡å¿…è¦çš„å·¥å…·å’Œææ–™  
3ï¸âƒ£ æŒ‰ç…§æ­¥éª¤ä¸€æ­¥æ­¥æ“ä½œ
4ï¸âƒ£ æ³¨æ„å…³é”®ç»†èŠ‚ï¼Œé¿å…å¸¸è§é”™è¯¯

âœ¨ å®ç”¨å°è´´å£«ï¼š
â€¢ å»ºè®®åˆå­¦è€…ä»ç®€å•çš„å¼€å§‹
â€¢ å¤šç»ƒä¹ å‡ æ¬¡å°±èƒ½ç†Ÿç»ƒæŒæ¡
â€¢ æœ‰é—®é¢˜éšæ—¶äº¤æµè®¨è®º

å¸Œæœ›è¿™ä¸ªåˆ†äº«å¯¹å¤§å®¶æœ‰å¸®åŠ©ï¼æœ‰ä»€ä¹ˆé—®é¢˜æ¬¢è¿è¯„è®ºåŒºè®¨è®ºï½

#å®ç”¨æ•™ç¨‹ #{topic} #{keyword}"""
        
        return content
    
    def _generate_tags(self, topics: List[str], keywords: List[str], original_tags: List[str]) -> List[str]:
        """ç”Ÿæˆæ ‡ç­¾"""
        
        tags = []
        
        # æ·»åŠ ä¸»é¢˜ç›¸å…³æ ‡ç­¾
        for topic in topics[:2]:  # æœ€å¤š2ä¸ªä¸»é¢˜æ ‡ç­¾
            tags.append(topic)
        
        # æ·»åŠ å…³é”®è¯æ ‡ç­¾
        for keyword in keywords[:3]:  # æœ€å¤š3ä¸ªå…³é”®è¯æ ‡ç­¾
            if keyword not in tags:
                tags.append(keyword)
        
        # æ·»åŠ ä¸€äº›é€šç”¨æ ‡ç­¾
        general_tags = ["ç”Ÿæ´»åˆ†äº«", "å®ç”¨æ¨è", "æ—¥å¸¸è®°å½•", "å¿ƒå¾—ä½“ä¼š", "ç»éªŒåˆ†äº«"]
        tags.append(random.choice(general_tags))
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_tags = list(dict.fromkeys(tags))[:8]  # æœ€å¤š8ä¸ªæ ‡ç­¾
        
        return unique_tags
    
    def _generate_suggestions(self, content_type: str, tone: str, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ›ä½œå»ºè®®"""
        
        suggestions = []
        
        # æ ¹æ®å†…å®¹ç±»å‹ç»™å»ºè®®
        if content_type == "æ•™ç¨‹æ”»ç•¥":
            suggestions.append("å¯ä»¥æ·»åŠ æ­¥éª¤å›¾ç‰‡æˆ–è§†é¢‘ï¼Œè®©æ•™ç¨‹æ›´ç›´è§‚")
            suggestions.append("è®°å¾—åœ¨å¼€å¤´è¯´æ˜éš¾åº¦ç­‰çº§å’Œæ‰€éœ€æ—¶é—´")
        elif content_type == "åˆ†äº«æ¨è":
            suggestions.append("å¯ä»¥æ·»åŠ äº§å“ç»†èŠ‚å›¾ç‰‡ï¼Œå¢åŠ å¯ä¿¡åº¦")
            suggestions.append("åˆ†äº«è´­ä¹°é“¾æ¥æˆ–æ¸ é“ä¿¡æ¯ä¼šæ›´å®ç”¨")
        
        # é€šç”¨å»ºè®®
        suggestions.extend([
            "é€‚å½“æ·»åŠ è¡¨æƒ…ç¬¦å·å’Œæ¢è¡Œï¼Œè®©ç‰ˆé¢æ›´æ¸…æ™°",
            "åœ¨è¯„è®ºåŒºå¤šä¸è¯»è€…äº’åŠ¨ï¼Œæé«˜ç¬”è®°çƒ­åº¦"
        ])
        
        return " | ".join(suggestions[:4])  # æœ€å¤š4æ¡å»ºè®®
    
    def _get_current_season(self) -> str:
        """è·å–å½“å‰å­£èŠ‚"""
        month = datetime.now().month
        
        if month in [12, 1, 2]:
            return "å†¬å­£"
        elif month in [3, 4, 5]:
            return "æ˜¥å­£"
        elif month in [6, 7, 8]:
            return "å¤å­£"
        else:
            return "ç§‹å­£"
    
    def get_debug_info(self, session_id: str) -> Dict[str, Any]:
        """è·å–debugä¿¡æ¯"""
        debug_info = {}
        
        try:
            # è¯»å–ä¼šè¯ä¿¡æ¯
            session_file = os.path.join(self.debug_dir, f"{session_id}_session.json")
            if os.path.exists(session_file):
                with open(session_file, 'r', encoding='utf-8') as f:
                    debug_info['session'] = json.load(f)
            
            # è¯»å–åˆ†æç»“æœ
            analysis_file = os.path.join(self.debug_dir, f"{session_id}_analysis.json")
            if os.path.exists(analysis_file):
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    debug_info['analysis'] = json.load(f)
            
            # è¯»å–ç”Ÿæˆç»“æœ
            generated_file = os.path.join(self.debug_dir, f"{session_id}_generated_note.json")
            if os.path.exists(generated_file):
                with open(generated_file, 'r', encoding='utf-8') as f:
                    debug_info['generated_note'] = json.load(f)
                    
        except Exception as e:
            logger.error(f"è·å–debugä¿¡æ¯å¤±è´¥: {str(e)}")
        
        return debug_info
    
    def cleanup_old_debug_files(self, days: int = 7):
        """æ¸…ç†æ—§çš„debugæ–‡ä»¶"""
        try:
            current_time = time.time()
            cutoff_time = current_time - (days * 24 * 60 * 60)
            
            for filename in os.listdir(self.debug_dir):
                file_path = os.path.join(self.debug_dir, filename)
                if os.path.getmtime(file_path) < cutoff_time:
                    os.remove(file_path)
                    logger.info(f"åˆ é™¤æ—§debugæ–‡ä»¶: {filename}")
                    
        except Exception as e:
            logger.error(f"æ¸…ç†debugæ–‡ä»¶å¤±è´¥: {str(e)}") 