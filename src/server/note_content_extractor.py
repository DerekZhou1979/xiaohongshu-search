#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°çº¢ä¹¦ç¬”è®°å†…å®¹æå–å™¨ - ç”Ÿäº§ç‰ˆæœ¬
åŸºäºHTMLé¡µé¢æŠ€æœ¯åˆ†æï¼Œå®ç°é«˜æ•ˆçš„ç¬”è®°å†…å®¹æå–
æ”¯æŒæ‰¹é‡å¤„ç†å’Œå¤šç§æ•°æ®æº
"""

import json
import re
import os
import logging
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup
from urllib.parse import unquote, urlparse

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class NoteContentExtractor:
    """å°çº¢ä¹¦ç¬”è®°å†…å®¹æå–å™¨ - ç”Ÿäº§ç‰ˆæœ¬"""
    
    def __init__(self, cache_dir: str = "cache/notes"):
        """
        åˆå§‹åŒ–æå–å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®å­—æ®µæ˜ å°„é…ç½®
        self._setup_field_mappings()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed': 0,
            'success': 0,
            'failed': 0,
            'start_time': None
        }
    
    def _setup_field_mappings(self):
        """è®¾ç½®å­—æ®µæ˜ å°„é…ç½®"""
        
        # JSONæ•°æ®å­—æ®µæ˜ å°„
        self.json_fields_mapping = {
            'title': ['title', 'noteTitle', 'desc'],
            'content': ['content', 'desc', 'description'],
            'note_id': ['noteId', 'id', 'note_id'],
            'user_id': ['userId', 'user_id', 'authorId'],
            'images': ['images', 'imageList', 'pics'],
            'video': ['video', 'videoUrl'],
            'tags': ['tags', 'tagList'],
            'like_count': ['likes', 'likeCount', 'interactInfo'],
            'collect_count': ['collects', 'collectCount'],
            'comment_count': ['comments', 'commentCount'],
        }
        
        # HTMLé€‰æ‹©å™¨æ˜ å°„
        self.html_selectors = {
            'title': ['.note-content .title', '.title', 'h1', '[class*="title"]'],
            'content': ['.note-content .desc', '.desc', '[class*="desc"]', '[class*="content"]'],
            'author_name': ['.author-wrapper .name', '.author .name', '[class*="author"]'],
            'like_count': ['.like-wrapper .count', '[class*="like"] .count'],
            'collect_count': ['.collect-wrapper .count', '[class*="collect"] .count'],
            'comment_count': ['.comments-container .total', '[class*="comment"] .total'],
        }
        
        # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.regex_patterns = {
            'note_id': [r'/explore/([a-f0-9]{24})'],
            'user_id': [r'userId["\']?\s*[:=]\s*["\']?([a-f0-9]{24})'],
            'title': [r'<title[^>]*>([^<]+)</title>'],
            'image_urls': [r'src=["\']([^"\']+xhscdn[^"\']*)["\']'],
        }
        
        # Metaæ ‡ç­¾æ˜ å°„
        self.meta_mappings = {
            'og:title': 'meta_title',
            'og:description': 'meta_description',
            'og:image': 'meta_image',
            'og:url': 'meta_url',
            'description': 'page_description',
            'keywords': 'page_keywords',
        }
    
    def extract_from_html_file(self, html_file_path: str) -> Optional[Dict[str, Any]]:
        """
        ä»HTMLæ–‡ä»¶æå–ç¬”è®°å†…å®¹
        
        Args:
            html_file_path: HTMLæ–‡ä»¶è·¯å¾„
            
        Returns:
            æå–çš„ç¬”è®°æ•°æ®ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            # è¯»å–HTMLæ–‡ä»¶
            with open(html_file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            return self.extract_from_html_content(html_content, html_file_path)
            
        except Exception as e:
            logger.error(f"è¯»å–HTMLæ–‡ä»¶å¤±è´¥ {html_file_path}: {e}")
            return None
    
    def extract_from_html_content(self, html_content: str, source_file: str = None) -> Optional[Dict[str, Any]]:
        """
        ä»HTMLå†…å®¹æå–ç¬”è®°æ•°æ®
        
        Args:
            html_content: HTMLå†…å®¹
            source_file: æºæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            æå–çš„ç¬”è®°æ•°æ®
        """
        self.stats['processed'] += 1
        
        try:
            logger.info(f"å¼€å§‹æå–ç¬”è®°å†…å®¹ï¼ŒHTMLå¤§å°: {len(html_content)} å­—ç¬¦")
            
            # æŒ‰ä¼˜å…ˆçº§æå–æ•°æ®
            all_data = {}
            
            # 1. å°è¯•JSONæ•°æ®æå–
            json_data = self._extract_from_json(html_content)
            if json_data:
                all_data.update(json_data)
                logger.info(f"JSONæå–æˆåŠŸ: {len(json_data)} ä¸ªå­—æ®µ")
            
            # 2. HTMLç»“æ„è§£æ
            html_data = self._extract_from_html_structure(html_content)
            if html_data:
                # åˆå¹¶æ•°æ®ï¼Œå·²æœ‰çš„ä¸è¦†ç›–
                for key, value in html_data.items():
                    if key not in all_data or not all_data[key]:
                        all_data[key] = value
                logger.info(f"HTMLè§£ææˆåŠŸ: {len(html_data)} ä¸ªå­—æ®µ")
            
            # 3. æ­£åˆ™è¡¨è¾¾å¼æå–
            regex_data = self._extract_with_regex(html_content)
            if regex_data:
                for key, value in regex_data.items():
                    if key not in all_data or not all_data[key]:
                        all_data[key] = value
                logger.info(f"æ­£åˆ™æå–æˆåŠŸ: {len(regex_data)} ä¸ªå­—æ®µ")
            
            # æ•°æ®æ¸…æ´—å’ŒéªŒè¯
            cleaned_data = self._clean_and_validate_data(all_data)
            
            # æ·»åŠ å…ƒæ•°æ®
            cleaned_data['extraction_info'] = {
                'source_file': source_file,
                'extracted_at': datetime.now().isoformat(),
                'extractor_version': '1.0.0',
                'fields_count': len(cleaned_data) - 1,  # å‡å»extraction_info
            }
            
            self.stats['success'] += 1
            logger.info(f"âœ… ç¬”è®°å†…å®¹æå–æˆåŠŸï¼Œå…±æå– {len(cleaned_data)-1} ä¸ªå­—æ®µ")
            
            return cleaned_data
            
        except Exception as e:
            self.stats['failed'] += 1
            logger.error(f"âŒ ç¬”è®°å†…å®¹æå–å¤±è´¥: {e}")
            return None
    
    def _extract_from_json(self, html_content: str) -> Dict[str, Any]:
        """ä»JSONæ•°æ®æå–ä¿¡æ¯"""
        data = {}
        
        try:
            # æŸ¥æ‰¾window.__INITIAL_STATE__
            pattern = r'window\.__INITIAL_STATE__\s*=\s*({.*?});'
            match = re.search(pattern, html_content, re.DOTALL)
            
            if match:
                json_str = match.group(1)
                try:
                    initial_state = json.loads(json_str)
                    data.update(self._parse_json_structure(initial_state))
                except json.JSONDecodeError:
                    # å°è¯•ä¿®å¤JSON
                    fixed_data = self._try_fix_json(json_str)
                    if fixed_data:
                        data.update(fixed_data)
            
            # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„JSONæ•°æ®
            other_patterns = [
                r'window\.__NUXT__\s*=\s*({.*?});',
                r'window\.initialData\s*=\s*({.*?});',
                r'window\.pageData\s*=\s*({.*?});',
            ]
            
            for pattern in other_patterns:
                match = re.search(pattern, html_content, re.DOTALL)
                if match:
                    try:
                        json_data = json.loads(match.group(1))
                        data.update(self._parse_json_structure(json_data))
                    except:
                        continue
                        
        except Exception as e:
            logger.warning(f"JSONæ•°æ®æå–å¤±è´¥: {e}")
        
        return data
    
    def _parse_json_structure(self, json_data: Any) -> Dict[str, Any]:
        """è§£æJSONæ•°æ®ç»“æ„"""
        data = {}
        
        try:
            # é€’å½’æœç´¢å…³é”®å­—æ®µ
            def search_fields(obj, path=""):
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡å­—æ®µ
                        for target_field, possible_keys in self.json_fields_mapping.items():
                            if key in possible_keys and value:
                                data[target_field] = value
                        
                        # é€’å½’æœç´¢
                        if isinstance(value, (dict, list)):
                            search_fields(value, f"{path}.{key}" if path else key)
                            
                elif isinstance(obj, list):
                    for i, item in enumerate(obj):
                        if isinstance(item, dict):
                            search_fields(item, f"{path}[{i}]")
            
            search_fields(json_data)
            
        except Exception as e:
            logger.warning(f"JSONç»“æ„è§£æå¤±è´¥: {e}")
        
        return data
    
    def _try_fix_json(self, json_str: str) -> Optional[Dict[str, Any]]:
        """å°è¯•ä¿®å¤æŸåçš„JSON"""
        fixes = [
            lambda s: re.sub(r'[,;]\s*$', '', s.strip()),
            lambda s: s.replace("'", '"'),
            lambda s: re.sub(r',\s*}', '}', s),
            lambda s: re.sub(r',\s*]', ']', s),
        ]
        
        for fix_func in fixes:
            try:
                fixed_json = fix_func(json_str)
                data = json.loads(fixed_json)
                return self._parse_json_structure(data)
            except:
                continue
        
        return None
    
    def _extract_from_html_structure(self, html_content: str) -> Dict[str, Any]:
        """ä»HTMLç»“æ„æå–æ•°æ®"""
        data = {}
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ–‡æœ¬æ•°æ®
            for field, selectors in self.html_selectors.items():
                for selector in selectors:
                    elements = soup.select(selector)
                    if elements:
                        text = elements[0].get_text(strip=True)
                        if text:
                            data[field] = text
                            break
            
            # æå–å›¾ç‰‡
            images = self._extract_images(soup)
            if images:
                data['images'] = images
            
            # æå–Metaæ ‡ç­¾
            meta_data = self._extract_meta_tags(soup)
            data.update(meta_data)
            
            # æå–æ ‡ç­¾ä¿¡æ¯
            tags = self._extract_tags(soup)
            if tags:
                data['tags'] = tags
            
        except Exception as e:
            logger.warning(f"HTMLç»“æ„è§£æå¤±è´¥: {e}")
        
        return data
    
    def _extract_images(self, soup: BeautifulSoup) -> List[str]:
        """æå–å›¾ç‰‡URL"""
        images = []
        
        selectors = [
            '.swiper-slide img',
            '.media-container img', 
            'img[src*="xhscdn"]',
            '[style*="background-image"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for element in elements:
                # è·å–å›¾ç‰‡URL
                src = element.get('src') or element.get('data-src')
                
                # ä»styleå±æ€§ä¸­æå–background-image
                if not src and element.get('style'):
                    style = element.get('style')
                    bg_match = re.search(r'background-image:\s*url\(["\']?([^"\'()]+)["\']?\)', style)
                    if bg_match:
                        src = bg_match.group(1)
                
                if src and 'xhscdn' in src and src not in images:
                    images.append(src)
        
        # å»é‡å¹¶è¿‡æ»¤å¤´åƒå›¾ç‰‡
        filtered_images = []
        for img in images:
            # è¿‡æ»¤æ‰å¤´åƒå›¾ç‰‡ï¼ˆé€šå¸¸åŒ…å«avatarå…³é”®è¯ï¼‰
            if 'avatar' not in img.lower() and len(filtered_images) < 20:
                filtered_images.append(img)
        
        return filtered_images
    
    def _extract_meta_tags(self, soup: BeautifulSoup) -> Dict[str, str]:
        """æå–Metaæ ‡ç­¾ä¿¡æ¯"""
        data = {}
        
        for meta_name, field_name in self.meta_mappings.items():
            selectors = [
                f'meta[property="{meta_name}"]',
                f'meta[name="{meta_name}"]',
            ]
            
            for selector in selectors:
                meta_tag = soup.select_one(selector)
                if meta_tag:
                    content = meta_tag.get('content')
                    if content:
                        data[field_name] = content
                        break
        
        return data
    
    def _extract_tags(self, soup: BeautifulSoup) -> List[str]:
        """æå–æ ‡ç­¾ä¿¡æ¯"""
        tags = []
        
        tag_selectors = [
            '.tag', '[class*="tag"]', '.topic', '[class*="topic"]'
        ]
        
        for selector in tag_selectors:
            elements = soup.select(selector)
            for element in elements:
                tag_text = element.get_text(strip=True)
                if tag_text and tag_text.startswith('#') and tag_text not in tags:
                    tags.append(tag_text)
        
        return tags[:10]  # é™åˆ¶æ ‡ç­¾æ•°é‡
    
    def _extract_with_regex(self, html_content: str) -> Dict[str, Any]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ•°æ®"""
        data = {}
        
        try:
            for field, patterns in self.regex_patterns.items():
                for pattern in patterns:
                    matches = re.findall(pattern, html_content, re.IGNORECASE)
                    if matches:
                        if field in ['image_urls']:
                            # å»é‡å¹¶é™åˆ¶æ•°é‡
                            unique_matches = list(set(matches))[:15]
                            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å†…å®¹å›¾ç‰‡çš„URL
                            if field == 'image_urls':
                                filtered = [url for url in unique_matches 
                                          if 'xhscdn' in url and 'avatar' not in url.lower()]
                                if filtered:
                                    data[field] = filtered
                        else:
                            data[field] = matches[0]
                        break
                        
        except Exception as e:
            logger.warning(f"æ­£åˆ™è¡¨è¾¾å¼æå–å¤±è´¥: {e}")
        
        return data
    
    def _clean_and_validate_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—å’ŒéªŒè¯æ•°æ®"""
        cleaned_data = {}
        
        for key, value in raw_data.items():
            try:
                if key in ['like_count', 'collect_count', 'comment_count']:
                    # æ¸…ç†æ•°å­—å­—æ®µ
                    if isinstance(value, str):
                        # æå–æ•°å­—
                        number_match = re.search(r'(\d+)', value)
                        if number_match:
                            cleaned_data[key] = int(number_match.group(1))
                    elif isinstance(value, (int, float)):
                        cleaned_data[key] = int(value)
                        
                elif key in ['title', 'content', 'author_name']:
                    # æ¸…ç†æ–‡æœ¬å­—æ®µ
                    if isinstance(value, str):
                        cleaned_text = value.strip()
                        # ç§»é™¤HTMLæ ‡ç­¾
                        cleaned_text = re.sub(r'<[^>]+>', '', cleaned_text)
                        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
                        if cleaned_text:
                            cleaned_data[key] = cleaned_text
                            
                elif key in ['images', 'tags', 'image_urls']:
                    # æ¸…ç†åˆ—è¡¨å­—æ®µ
                    if isinstance(value, list):
                        cleaned_list = [item for item in value if item and str(item).strip()]
                        if cleaned_list:
                            cleaned_data[key] = cleaned_list
                            
                else:
                    # å…¶ä»–å­—æ®µç›´æ¥èµ‹å€¼
                    if value:
                        cleaned_data[key] = value
                        
            except Exception as e:
                logger.warning(f"æ¸…æ´—å­—æ®µ {key} å¤±è´¥: {e}")
                continue
        
        return cleaned_data
    
    def save_extracted_data(self, note_data: Dict[str, Any], note_id: str = None) -> str:
        """
        ä¿å­˜æå–çš„æ•°æ®åˆ°JSONæ–‡ä»¶
        
        Args:
            note_data: ç¬”è®°æ•°æ®
            note_id: ç¬”è®°IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            if note_id:
                filename = f"{note_id}_extracted.json"
            else:
                # ä½¿ç”¨æ•°æ®å“ˆå¸Œä½œä¸ºæ–‡ä»¶å
                data_hash = hashlib.md5(json.dumps(note_data, sort_keys=True).encode()).hexdigest()
                filename = f"note_{data_hash}_extracted.json"
            
            file_path = self.cache_dir / filename
            
            # ä¿å­˜æ•°æ®
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(note_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
            return str(file_path)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            raise
    
    def batch_extract_from_cache(self, pattern: str = "*.html") -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†ç¼“å­˜ç›®å½•ä¸­çš„HTMLæ–‡ä»¶
        
        Args:
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            æå–ç»“æœåˆ—è¡¨
        """
        self.stats['start_time'] = datetime.now()
        results = []
        
        try:
            html_files = list(self.cache_dir.glob(pattern))
            logger.info(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶å¾…å¤„ç†")
            
            for html_file in html_files:
                logger.info(f"å¤„ç†æ–‡ä»¶: {html_file.name}")
                
                extracted_data = self.extract_from_html_file(str(html_file))
                if extracted_data:
                    # ä¿å­˜æå–çš„æ•°æ®
                    note_id = extracted_data.get('note_id')
                    saved_path = self.save_extracted_data(extracted_data, note_id)
                    
                    extracted_data['saved_path'] = saved_path
                    results.append(extracted_data)
                    
                    logger.info(f"âœ… æˆåŠŸå¤„ç†: {html_file.name}")
                else:
                    logger.warning(f"âš ï¸ å¤„ç†å¤±è´¥: {html_file.name}")
                    
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        self._print_stats()
        
        return results
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        if self.stats['start_time']:
            duration = datetime.now() - self.stats['start_time']
            logger.info("\n" + "="*50)
            logger.info("ğŸ“Š å¤„ç†ç»Ÿè®¡")
            logger.info("="*50)
            logger.info(f"æ€»å¤„ç†æ•°: {self.stats['processed']}")
            logger.info(f"æˆåŠŸæ•°: {self.stats['success']}")
            logger.info(f"å¤±è´¥æ•°: {self.stats['failed']}")
            logger.info(f"æˆåŠŸç‡: {self.stats['success']/max(self.stats['processed'], 1)*100:.1f}%")
            logger.info(f"å¤„ç†æ—¶é—´: {duration.total_seconds():.1f} ç§’")
            logger.info("="*50)

def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import sys
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    extractor = NoteContentExtractor()
    
    if len(sys.argv) > 1:
        # å¤„ç†æŒ‡å®šæ–‡ä»¶
        html_file = sys.argv[1]
        logger.info(f"å¤„ç†æ–‡ä»¶: {html_file}")
        
        extracted_data = extractor.extract_from_html_file(html_file)
        if extracted_data:
            note_id = extracted_data.get('note_id')
            saved_path = extractor.save_extracted_data(extracted_data, note_id)
            print(f"âœ… æå–æˆåŠŸï¼Œä¿å­˜åˆ°: {saved_path}")
        else:
            print("âŒ æå–å¤±è´¥")
    else:
        # æ‰¹é‡å¤„ç†
        logger.info("å¼€å§‹æ‰¹é‡å¤„ç†ç¼“å­˜ç›®å½•ä¸­çš„HTMLæ–‡ä»¶")
        results = extractor.batch_extract_from_cache()
        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ–‡ä»¶")

if __name__ == "__main__":
    main() 