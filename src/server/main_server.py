#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å°çº¢ä¹¦æœç´¢æœåŠ¡å™¨æ¨¡å—
æä¾›Web APIå’Œé™æ€æ–‡ä»¶æœåŠ¡ï¼Œæ”¯æŒæœç´¢ã€ç»“æœå±•ç¤ºç­‰åŠŸèƒ½

ä¸»è¦åŠŸèƒ½ï¼š
1. Web APIæœåŠ¡ - æœç´¢ã€ç¬”è®°è¯¦æƒ…ã€çƒ­é—¨å…³é”®è¯
2. é™æ€æ–‡ä»¶æœåŠ¡ - HTMLã€CSSã€JSã€å›¾ç‰‡
3. HTMLç»“æœé¡µé¢æœåŠ¡ - æ–‡ä»¶å½¢å¼å’ŒAPIå½¢å¼
4. ç”¨æˆ·ç™»å½•æ”¯æŒ
"""

import sys
import os
import logging
import time
import hashlib
import traceback
import json
import threading
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from flask import Flask, request, jsonify, send_from_directory, redirect, url_for
from flask_cors import CORS
from src.crawler.XHS_crawler import XiaoHongShuCrawler
from src.server.debug_manager import debug_manager
# from src.server.note_generator import NoteContentGenerator  # å·²åˆ é™¤ï¼ŒåŠŸèƒ½å·²æ•´åˆ
from src.server.note_content_extractor import NoteContentExtractor

# ==================== é…ç½®å’Œåˆå§‹åŒ– ====================

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__, static_folder='../../static')
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# ==================== å…¨å±€å˜é‡ ====================

# Cookieæ–‡ä»¶è·¯å¾„
COOKIES_FILE = os.path.join('cache', 'cookies', 'xiaohongshu_cookies.json')

# å…¨å±€çˆ¬è™«å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
crawler = None

# ç¬”è®°å†…å®¹ç”Ÿæˆå™¨å®ä¾‹ï¼ˆå·²åˆ é™¤ï¼ŒåŠŸèƒ½å·²æ•´åˆåˆ°ç»Ÿä¸€æå–å™¨ï¼‰
# note_generator = NoteContentGenerator()

# ç¬”è®°å†…å®¹æå–å™¨å®ä¾‹
note_extractor = NoteContentExtractor()

# ç»Ÿä¸€æ•°æ®æå–å™¨å®ä¾‹
from src.server.unified_extractor import UnifiedExtractor
unified_extractor = UnifiedExtractor()

# HTMLç»“æœå†…å­˜ç¼“å­˜ï¼ˆé¿å…æ–‡ä»¶è·¯å¾„é—®é¢˜ï¼‰
html_results_cache = {}

# ==================== å·¥å…·å‡½æ•° ====================

def store_html_result(html_hash, html_content):
    """
    å­˜å‚¨HTMLç»“æœåˆ°å†…å­˜ç¼“å­˜
    
    Args:
        html_hash: HTMLå†…å®¹çš„MD5å“ˆå¸Œå€¼
        html_content: HTMLå†…å®¹
    """
    global html_results_cache
    html_results_cache[html_hash] = html_content
    logger.info(f"HTMLå†…å®¹å·²å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜: {html_hash}")

def init_crawler():
    """
    å»¶è¿Ÿåˆå§‹åŒ–çˆ¬è™«å®ä¾‹
    åªåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–ï¼Œé¿å…å¯åŠ¨æ—¶çš„æ€§èƒ½å¼€é”€
    
    Returns:
        bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
    """
    global crawler
    if crawler is None:
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–å°çº¢ä¹¦çˆ¬è™«...")
            crawler = XiaoHongShuCrawler(
                use_selenium=True, 
                headless=True, 
                cookies_file=COOKIES_FILE
            )
            # è®¾ç½®HTMLå­˜å‚¨å›è°ƒå‡½æ•°
            crawler.set_html_callback(store_html_result)
            logger.info("å°çº¢ä¹¦çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"å°çº¢ä¹¦çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            crawler = None
            return False
    return True

def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•è·¯å¾„"""
    return os.path.dirname(os.path.dirname(os.path.dirname(__file__)))

def start_backend_extraction(search_results, session_id):
    """
    å¯åŠ¨åå°ç¬”è®°å†…å®¹æå–ä»»åŠ¡
    
    Args:
        search_results: æœç´¢ç»“æœåˆ—è¡¨
        session_id: ä¼šè¯ID
    """
    try:
        # å¯¼å…¥åå°çˆ¬è™«æ¨¡å—
        from src.crawler.backend_XHS_crawler import start_backend_crawl
        
        # è§„èŒƒåŒ–æœç´¢ç»“æœæ ¼å¼
        if isinstance(search_results, dict) and 'data' in search_results:
            notes_data = search_results['data']
        else:
            notes_data = search_results if isinstance(search_results, list) else []
        
        if not notes_data:
            logger.warning(f"åå°æå–ä»»åŠ¡å–æ¶ˆï¼šæ²¡æœ‰æœ‰æ•ˆçš„ç¬”è®°æ•°æ® (session: {session_id})")
            return
        
        logger.info(f"ğŸš€ å¯åŠ¨åå°ç¬”è®°å†…å®¹æå–ä»»åŠ¡")
        logger.info(f"ğŸ“Š ä¼šè¯ID: {session_id}")
        logger.info(f"ğŸ“ å¾…æå–ç¬”è®°æ•°é‡: {len(notes_data)}")
        
        # è®°å½•åˆ°debugç®¡ç†å™¨
        debug_manager.store_debug_info(session_id, f"ğŸš€ åå°çˆ¬è™«å¼€å§‹æå– {len(notes_data)} ç¯‡ç¬”è®°çš„è¯¦ç»†å†…å®¹", "INFO")
        
        # å¯åŠ¨åå°çˆ¬å–ä»»åŠ¡
        backend_session_id = f"{session_id}_backend"
        result = start_backend_crawl(notes_data, backend_session_id)
        
        # è®°å½•å®ŒæˆçŠ¶æ€
        success_count = result.get('success_count', 0)
        failed_count = result.get('failed_count', 0)
        success_rate = result.get('success_rate', 0)
        duration = result.get('duration_seconds', 0)
        
        logger.info(f"ğŸ‰ åå°ç¬”è®°å†…å®¹æå–ä»»åŠ¡å®Œæˆ!")
        logger.info(f"ğŸ“Š æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}, æˆåŠŸç‡: {success_rate:.1f}%, è€—æ—¶: {duration:.1f}ç§’")
        
        debug_manager.store_debug_info(
            session_id, 
            f"âœ… åå°çˆ¬è™«å®Œæˆï¼æˆåŠŸ: {success_count}/{len(notes_data)} ç¯‡ ({success_rate:.1f}%), è€—æ—¶: {duration:.1f}ç§’", 
            "INFO"
        )
        
    except Exception as e:
        error_msg = f"åå°ç¬”è®°å†…å®¹æå–ä»»åŠ¡å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        logger.error(traceback.format_exc())
        debug_manager.store_debug_info(session_id, f"âŒ {error_msg}", "ERROR")





# ==================== é™æ€æ–‡ä»¶è·¯ç”± ====================

@app.route('/')
def index():
    """ä¸»é¡µ - è¿”å›æœç´¢ç•Œé¢"""
    return send_from_directory('../../static', 'index.html')

@app.route('/css/<path:path>')
def serve_css(path):
    """æä¾›CSSæ–‡ä»¶æœåŠ¡"""
    return send_from_directory('../../static/css', path)

@app.route('/js/<path:path>')
def serve_js(path):
    """æä¾›JavaScriptæ–‡ä»¶æœåŠ¡"""
    return send_from_directory('../../static/js', path)

@app.route('/img/<path:path>')
def serve_img(path):
    """æä¾›å›¾ç‰‡æ–‡ä»¶æœåŠ¡"""
    return send_from_directory('../../static/images', path)

# ==================== ç”¨æˆ·ç™»å½•è·¯ç”± ====================

@app.route('/login')
def login():
    """
    ç”¨æˆ·ç™»å½•é¡µé¢
    æ‰“å¼€æµè§ˆå™¨è®©ç”¨æˆ·æ‰‹åŠ¨ç™»å½•å°çº¢ä¹¦ï¼Œè·å–cookie
    """
    try:
        # åˆ›å»ºä¸“é—¨ç”¨äºç™»å½•çš„çˆ¬è™«å®ä¾‹ï¼ˆéæ— å¤´æ¨¡å¼ï¼‰
        login_crawler = XiaoHongShuCrawler(use_selenium=True, headless=False)
        success = login_crawler.login()
        login_crawler.close()
        
        if success:
            # ç™»å½•æˆåŠŸåé‡ç½®ä¸»çˆ¬è™«å®ä¾‹ï¼Œä»¥ä½¿ç”¨æ–°çš„cookie
            global crawler
            crawler = None
            return redirect(url_for('index'))
        else:
            return jsonify({"error": "ç™»å½•å¤±è´¥ï¼Œè¯·é‡è¯•"}), 500
    except Exception as e:
        logger.error(f"ç™»å½•è¿‡ç¨‹å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({"error": f"ç™»å½•è¿‡ç¨‹å‡ºé”™: {str(e)}"}), 500

# ==================== APIè·¯ç”± ====================

@app.route('/api/search')
def search():
    """
    æœç´¢API
    æ ¹æ®å…³é”®è¯æœç´¢å°çº¢ä¹¦ç¬”è®°
    
    å‚æ•°:
        keyword: æœç´¢å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰
        max_results: æœ€å¤§ç»“æœæ•°é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤21ï¼‰
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆå¯é€‰ï¼Œé»˜è®¤trueï¼‰
        session_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼Œç”¨äºdebugä¿¡æ¯ï¼‰
    
    è¿”å›:
        JSONæ ¼å¼çš„æœç´¢ç»“æœï¼ŒåŒ…å«ç¬”è®°åˆ—è¡¨å’ŒHTMLé¡µé¢URL
    """
    # åˆå§‹åŒ–çˆ¬è™«
    if not init_crawler():
        return jsonify({"error": "çˆ¬è™«åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒChromeæµè§ˆå™¨"}), 500
        
    # è·å–å‚æ•°
    keyword = request.args.get('keyword', '').strip()
    session_id = request.args.get('session_id', f"search_{int(time.time())}")
    
    if not keyword:
        return jsonify({"error": "ç¼ºå°‘å…³é”®è¯å‚æ•°"}), 400
    
    try:
        # è§£æå‚æ•°
        max_results = int(request.args.get('max_results', 21))
        use_cache = request.args.get('use_cache', 'true').lower() == 'true'
        
        # è®°å½•å¼€å§‹æœç´¢
        debug_manager.store_debug_info(session_id, f"ğŸ” å¼€å§‹æœç´¢å…³é”®è¯: {keyword}", "INFO")
        debug_manager.store_debug_info(session_id, f"ğŸ“Š æœ€å¤§ç»“æœæ•°: {max_results}, ä½¿ç”¨ç¼“å­˜: {use_cache}", "INFO")
        
        # è®¾ç½®çˆ¬è™«çš„debugå›è°ƒ
        debug_callback = debug_manager.create_debug_callback(session_id)
        
        # å¦‚æœçˆ¬è™«æ”¯æŒdebugå›è°ƒï¼Œè®¾ç½®å®ƒ
        if hasattr(crawler, 'set_debug_callback'):
            crawler.set_debug_callback(debug_callback)
        
        # ğŸ”§ ä¿®å¤1ï¼šè®°å½•HTMLç”ŸæˆçŠ¶æ€çš„æ ‡å¿—
        html_generation_started = False
        html_hash = hashlib.md5(keyword.encode()).hexdigest()
        
        # è®¾ç½®HTMLç”ŸæˆçŠ¶æ€è¿½è¸ªå›è°ƒ
        def html_status_callback(hash_key, html_content):
            nonlocal html_generation_started
            html_generation_started = True
            debug_manager.store_debug_info(session_id, f"ğŸ“„ HTMLç»“æœé¡µé¢ç”Ÿæˆå®Œæˆ: {hash_key}", "INFO")
            # è°ƒç”¨åŸå§‹çš„å­˜å‚¨å›è°ƒ
            store_html_result(hash_key, html_content)
        
        # è®¾ç½®å¢å¼ºçš„HTMLå›è°ƒ
        if hasattr(crawler, 'set_html_callback'):
            crawler.set_html_callback(html_status_callback)
        
        # æ‰§è¡Œæœç´¢
        debug_manager.store_debug_info(session_id, "ğŸš€ æ­£åœ¨æ‰§è¡Œæœç´¢...", "INFO")
        search_results = crawler.search(keyword, max_results=max_results, use_cache=use_cache)
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥æœç´¢ç»“æœ
        logger.info(f"æœç´¢ç»“æœç±»å‹: {type(search_results)}")
        logger.info(f"æœç´¢ç»“æœé•¿åº¦: {len(search_results) if search_results else 0}")
        if search_results:
            logger.info(f"ç¬¬ä¸€æ¡ç»“æœ: {search_results[0] if len(search_results) > 0 else 'N/A'}")
        
        # å¦‚æœæœç´¢ç»“æœä¸ºç©ºï¼Œå°è¯•ä»ç¼“å­˜ç›´æ¥è¯»å–
        if not search_results or len(search_results) == 0:
            logger.warning("çˆ¬è™«æœç´¢ç»“æœä¸ºç©ºï¼Œå°è¯•ä»ç¼“å­˜ç›´æ¥è¯»å–...")
            try:
                cache_filename = f"search_{hashlib.md5(keyword.encode()).hexdigest()}.json"
                cache_path = os.path.join(get_project_root(), 'cache', 'temp', cache_filename)
                if os.path.exists(cache_path):
                    with open(cache_path, 'r', encoding='utf-8') as cache_file:
                        cache_data = json.load(cache_file)
                        search_results = cache_data.get('data', [])
                        logger.info(f"ä»ç¼“å­˜æ¢å¤äº† {len(search_results)} æ¡ç»“æœ")
                        debug_manager.store_debug_info(session_id, f"âœ… ä»ç¼“å­˜æ¢å¤äº† {len(search_results)} æ¡ç»“æœ", "INFO")
            except Exception as cache_error:
                logger.error(f"ä»ç¼“å­˜æ¢å¤å¤±è´¥: {cache_error}")
        
        # ğŸ”§ ä¿®å¤2ï¼šç­‰å¾…HTMLç”Ÿæˆå®Œæˆï¼ˆå¦‚æœæœ‰æ•°æ®çš„è¯ï¼‰
        html_ready = False
        if search_results and len(search_results) > 0:
            # ç­‰å¾…HTMLç”Ÿæˆï¼ˆæœ€å¤š10ç§’ï¼‰
            debug_manager.store_debug_info(session_id, "â³ ç­‰å¾…HTMLé¡µé¢ç”Ÿæˆ...", "INFO")
            wait_start = time.time()
            max_wait = 10  # æœ€å¤§ç­‰å¾…10ç§’
            
            while time.time() - wait_start < max_wait:
                if html_generation_started or html_hash in html_results_cache:
                    html_ready = True
                    debug_manager.store_debug_info(session_id, "âœ… HTMLé¡µé¢ç”Ÿæˆå®Œæˆ", "INFO")
                    break
                time.sleep(0.5)  # æ¯500msæ£€æŸ¥ä¸€æ¬¡
            
            if not html_ready:
                debug_manager.store_debug_info(session_id, "âš ï¸ HTMLé¡µé¢ç”Ÿæˆè¶…æ—¶ï¼Œä½†æœç´¢æ•°æ®æœ‰æ•ˆ", "WARNING")
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯åŠ¨åå°çˆ¬è™«æå–è¯¦ç»†å†…å®¹
        if search_results and len(search_results) > 0:
            # è·å–é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ï¼‰
            enable_backend_extraction = os.environ.get('ENABLE_BACKEND_EXTRACTION', 'false').lower() == 'true'
            
            if enable_backend_extraction:
                debug_manager.store_debug_info(session_id, "ğŸ” å¯åŠ¨åå°çˆ¬è™«æå–ç¬”è®°è¯¦ç»†å†…å®¹...", "INFO")
                threading.Thread(
                    target=start_backend_extraction,
                    args=(search_results, session_id),
                    daemon=True
                ).start()
            else:
                debug_manager.store_debug_info(session_id, "âš ï¸ åå°ç¬”è®°å†…å®¹æå–å·²ç¦ç”¨", "INFO")
        
        # è§„èŒƒåŒ–æœç´¢ç»“æœæ ¼å¼
        if isinstance(search_results, dict) and 'data' in search_results:
            notes = search_results['data']
        else:
            notes = search_results if isinstance(search_results, list) else []
        
        debug_manager.store_debug_info(session_id, f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(notes)} æ¡ç¬”è®°", "INFO")
        
        # ğŸ”§ ä¿®å¤3ï¼šåªæœ‰åœ¨æœ‰æœ‰æ•ˆç¬”è®°æ•°æ®ä¸”HTMLç¡®å®ç”Ÿæˆæ—¶æ‰è¿”å›HTML URL
        if notes and len(notes) > 0:
            # éªŒè¯ç¬”è®°æ•°æ®çš„æœ‰æ•ˆæ€§
            valid_notes = [note for note in notes if note.get('title') or note.get('desc')]
            
            if valid_notes:
                # æ„å»ºå“åº”æ•°æ®
                response_data = {
                    "keyword": keyword,
                    "session_id": session_id,
                    "timestamp": int(time.time()),
                    "count": len(valid_notes),
                    "notes": valid_notes,
                    "html_generation_status": "completed" if html_ready else "pending"
                }
                
                # ğŸ”§ ä¿®å¤ï¼šåªæœ‰HTMLç¡®å®å‡†å¤‡å¥½æ—¶æ‰æä¾›URL
                if html_ready:
                    html_url = f"/results/search_{html_hash}.html"           # æ–‡ä»¶å½¢å¼
                    html_api_url = f"/api/result-html/{html_hash}"           # APIå½¢å¼ï¼ˆæ¨èï¼‰
                    response_data["html_url"] = html_url
                    response_data["html_api_url"] = html_api_url
                    debug_manager.store_debug_info(session_id, f"ğŸ“„ HTMLé¡µé¢å·²å‡†å¤‡å°±ç»ª: {html_api_url}", "INFO")
                else:
                    # æä¾›HTMLçŠ¶æ€æŸ¥è¯¢ç«¯ç‚¹
                    response_data["html_status_url"] = f"/api/html-status/{html_hash}"
                    debug_manager.store_debug_info(session_id, f"ğŸ“„ HTMLé¡µé¢ç”Ÿæˆä¸­ï¼Œæä¾›çŠ¶æ€æŸ¥è¯¢: {response_data['html_status_url']}", "INFO")
                
                return jsonify(response_data)
            else:
                debug_manager.store_debug_info(session_id, "âš ï¸ ç¬”è®°æ•°æ®æ— æ•ˆï¼Œæ²¡æœ‰æ ‡é¢˜æˆ–æè¿°", "WARNING")
        
        # ğŸ”§ ä¿®å¤ï¼šæ²¡æœ‰æœ‰æ•ˆæ•°æ®æ—¶ä¸è¿”å›HTML URL
        debug_manager.store_debug_info(session_id, "âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç¬”è®°æ•°æ®", "WARNING")
        return jsonify({
            "keyword": keyword,
            "session_id": session_id,
            "timestamp": int(time.time()),
            "count": 0,
            "notes": [],
            "html_generation_status": "no_data",
            "message": "æœªæ‰¾åˆ°ç›¸å…³ç¬”è®°"
        })
    except Exception as e:
        logger.error(f"æœç´¢å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        debug_manager.store_debug_info(session_id, f"âŒ æœç´¢å¤±è´¥: {str(e)}", "ERROR")
        return jsonify({"error": "æœç´¢å¤±è´¥", "message": str(e), "session_id": session_id}), 500

@app.route('/api/note/<note_id>')
def get_note(note_id):
    """
    è·å–ç¬”è®°è¯¦æƒ…API
    
    å‚æ•°:
        note_id: ç¬”è®°ID
    
    è¿”å›:
        JSONæ ¼å¼çš„ç¬”è®°è¯¦æƒ…
    """
    if not init_crawler():
        return jsonify({"error": "çˆ¬è™«åˆå§‹åŒ–å¤±è´¥"}), 500
        
    if not note_id:
        return jsonify({"error": "ç¼ºå°‘ç¬”è®°IDå‚æ•°"}), 400
    
    try:
        note = crawler.get_note_detail(note_id)
        
        if note:
            return jsonify({"note": note})
        else:
            return jsonify({"error": "æœªæ‰¾åˆ°è¯¥ç¬”è®°"}), 404
    except Exception as e:
        logger.error(f"è·å–ç¬”è®°è¯¦æƒ…å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({"error": "è·å–ç¬”è®°è¯¦æƒ…å¤±è´¥", "message": str(e)}), 500

@app.route('/api/hot-keywords')
def hot_keywords():
    """
    è·å–çƒ­é—¨å…³é”®è¯API
    
    è¿”å›:
        JSONæ ¼å¼çš„çƒ­é—¨å…³é”®è¯åˆ—è¡¨
    """
    if not init_crawler():
        return jsonify({"error": "çˆ¬è™«åˆå§‹åŒ–å¤±è´¥"}), 500
        
    try:
        keywords = crawler.get_hot_keywords()
        return jsonify({"keywords": keywords})
    except Exception as e:
        logger.error(f"è·å–çƒ­é—¨å…³é”®è¯å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({"error": "è·å–çƒ­é—¨å…³é”®è¯å¤±è´¥", "message": str(e)}), 500

@app.route('/api/debug/<session_id>')
def get_debug_info(session_id):
    """
    è·å–debugä¿¡æ¯API
    
    å‚æ•°:
        session_id: ä¼šè¯ID
        since: è·å–æŒ‡å®šæ—¶é—´æˆ³ä¹‹åçš„ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
        JSONæ ¼å¼çš„debugä¿¡æ¯åˆ—è¡¨
    """
    since = request.args.get('since', type=float, default=0)
    return jsonify(debug_manager.get_debug_info(session_id, since))

@app.route('/api/html-status/<html_hash>')
def get_html_status(html_hash):
    """
    è·å–HTMLç”ŸæˆçŠ¶æ€API
    
    å‚æ•°:
        html_hash: HTMLå†…å®¹çš„MD5å“ˆå¸Œå€¼
    
    è¿”å›:
        JSONæ ¼å¼çš„HTMLçŠ¶æ€ä¿¡æ¯
    """
    global html_results_cache
    
    try:
        # æ£€æŸ¥å†…å­˜ç¼“å­˜
        if html_hash in html_results_cache:
            html_url = f"/results/search_{html_hash}.html"           
            html_api_url = f"/api/result-html/{html_hash}"           
            return jsonify({
                "status": "ready",
                "html_url": html_url,
                "html_api_url": html_api_url,
                "message": "HTMLé¡µé¢å·²ç”Ÿæˆå®Œæˆ"
            })
        
        # æ£€æŸ¥æ–‡ä»¶ç³»ç»Ÿ
        results_dir = os.path.join(get_project_root(), 'cache', 'results')
        html_file = os.path.join(results_dir, f"search_{html_hash}.html")
        
        if os.path.exists(html_file):
            # æ–‡ä»¶å­˜åœ¨ï¼ŒåŠ è½½åˆ°å†…å­˜ç¼“å­˜
            try:
                with open(html_file, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                html_results_cache[html_hash] = html_content
                
                html_url = f"/results/search_{html_hash}.html"           
                html_api_url = f"/api/result-html/{html_hash}"           
                return jsonify({
                    "status": "ready",
                    "html_url": html_url,
                    "html_api_url": html_api_url,
                    "message": "HTMLé¡µé¢å·²ç”Ÿæˆå®Œæˆï¼ˆä»æ–‡ä»¶åŠ è½½ï¼‰"
                })
            except Exception as e:
                logger.error(f"åŠ è½½HTMLæ–‡ä»¶åˆ°ç¼“å­˜å¤±è´¥: {str(e)}")
                return jsonify({
                    "status": "error",
                    "message": f"HTMLæ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}"
                }), 500
        
        # HTMLè¿˜æœªç”Ÿæˆ
        return jsonify({
            "status": "pending",
            "message": "HTMLé¡µé¢æ­£åœ¨ç”Ÿæˆä¸­..."
        })
        
    except Exception as e:
        logger.error(f"è·å–HTMLçŠ¶æ€å¤±è´¥: {str(e)}")
        return jsonify({
            "status": "error",
            "message": f"è·å–HTMLçŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/create-similar-note/<note_id>', methods=['POST'])
def create_similar_note(note_id):
    """
    åˆ›å»ºåŒç±»ç¬”è®°API
    
    å‚æ•°:
        note_id: åŸç¬”è®°ID
    
    è¿”å›:
        JSONæ ¼å¼çš„ç”Ÿæˆç¬”è®°å†…å®¹
    """
    if not note_id:
        return jsonify({"success": False, "message": "ç¼ºå°‘ç¬”è®°IDå‚æ•°"}), 400
    
    try:
        # åˆå§‹åŒ–çˆ¬è™«
        if not init_crawler():
            return jsonify({"success": False, "message": "ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥"}), 500
        
        # è·å–åŸç¬”è®°è¯¦æƒ…
        logger.info(f"æ­£åœ¨è·å–åŸç¬”è®°è¯¦æƒ…: {note_id}")
        original_note = crawler.get_note_detail(note_id)
        
        if not original_note:
            return jsonify({"success": False, "message": "æ— æ³•è·å–åŸç¬”è®°å†…å®¹"}), 404
        
        # æ·»åŠ note_idåˆ°åŸç¬”è®°ä¿¡æ¯ä¸­
        original_note['note_id'] = note_id
        
        # ç¬”è®°ç”ŸæˆåŠŸèƒ½å·²ç§»é™¤ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        logger.warning(f"ç¬”è®°ç”ŸæˆåŠŸèƒ½å·²ç§»é™¤ï¼Œæ— æ³•ç”ŸæˆåŸºäºç¬”è®°: {note_id} çš„åŒç±»ç¬”è®°")
        return jsonify({
            "success": False,
            "message": "ç¬”è®°ç”ŸæˆåŠŸèƒ½å·²ç§»é™¤ï¼Œè¯·ä½¿ç”¨å…¶ä»–æ–¹å¼"
        }), 501
        
        # è¿™éƒ¨åˆ†ä»£ç ä¸ä¼šæ‰§è¡Œåˆ°ï¼Œå› ä¸ºä¸Šé¢å·²ç»è¿”å›äº†é”™è¯¯
        
    except Exception as e:
        logger.error(f"åˆ›å»ºåŒç±»ç¬”è®°å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            "success": False, 
            "message": f"åˆ›å»ºåŒç±»ç¬”è®°å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/note-generation-debug/<session_id>')
def get_note_generation_debug(session_id):
    """
    è·å–ç¬”è®°ç”Ÿæˆdebugä¿¡æ¯API
    
    å‚æ•°:
        session_id: ç¬”è®°ç”Ÿæˆä¼šè¯ID
    
    è¿”å›:
        JSONæ ¼å¼çš„debugä¿¡æ¯
    """
    try:
        # ç¬”è®°ç”ŸæˆåŠŸèƒ½å·²ç§»é™¤
        return jsonify({
            "success": False,
            "message": "ç¬”è®°ç”ŸæˆåŠŸèƒ½å·²ç§»é™¤",
            "session_id": session_id
        }), 501
    except Exception as e:
        logger.error(f"è·å–ç¬”è®°ç”Ÿæˆdebugä¿¡æ¯å¤±è´¥: {str(e)}")
        return jsonify({
            "success": False,
            "message": str(e)
        }), 500

@app.route('/api/backend-crawl-status/<session_id>')
def get_backend_crawl_status(session_id):
    """
    è·å–åå°çˆ¬è™«çŠ¶æ€API
    
    å‚æ•°:
        session_id: ä¼šè¯ID
    
    è¿”å›:
        JSONæ ¼å¼çš„åå°çˆ¬è™«çŠ¶æ€ä¿¡æ¯
    """
    try:
        backend_session_id = f"{session_id}_backend"
        
        # æ£€æŸ¥åå°çˆ¬è™«ç»“æœç›®å½•
        temp_notes_dir = os.path.join(get_project_root(), 'temp', 'notes')
        batch_dir = os.path.join(temp_notes_dir, f"batch_{backend_session_id}")
        
        if not os.path.exists(batch_dir):
            return jsonify({
                'success': False,
                'status': 'not_started',
                'message': 'åå°çˆ¬è™«ä»»åŠ¡å°šæœªå¼€å§‹'
            })
        
        # è¯»å–ä»»åŠ¡ä¿¡æ¯
        task_file = os.path.join(batch_dir, 'task_info.json')
        stats_file = os.path.join(batch_dir, 'crawl_stats.json')
        
        result = {
            'success': True,
            'session_id': session_id,
            'backend_session_id': backend_session_id,
            'batch_dir': batch_dir
        }
        
        # è¯»å–ä»»åŠ¡ä¿¡æ¯
        if os.path.exists(task_file):
            with open(task_file, 'r', encoding='utf-8') as f:
                task_info = json.load(f)
                result['task_info'] = task_info
        
        # è¯»å–ç»Ÿè®¡ä¿¡æ¯
        if os.path.exists(stats_file):
            with open(stats_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)
                result['stats'] = stats
                result['status'] = 'completed'
        else:
            result['status'] = 'running'
        
        # ç»Ÿè®¡å·²å®Œæˆçš„æ–‡ä»¶
        files = os.listdir(batch_dir) if os.path.exists(batch_dir) else []
        source_files = [f for f in files if f.endswith('_source.html')]
        detail_files = [f for f in files if f.endswith('_detail.json')]
        image_dirs = [f for f in files if f.endswith('_images') and os.path.isdir(os.path.join(batch_dir, f))]
        
        result['file_counts'] = {
            'source_files': len(source_files),
            'detail_files': len(detail_files),
            'image_dirs': len(image_dirs),
            'total_files': len(files)
        }
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"è·å–åå°çˆ¬è™«çŠ¶æ€å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/cache/notes/<path:filename>')
def serve_note_files(filename):
    """
    æä¾›ç¬”è®°ç›¸å…³æ–‡ä»¶çš„é™æ€è®¿é—®
    åŒ…æ‹¬é¡µé¢æºç ã€å›¾ç‰‡ç­‰æ–‡ä»¶
    
    å‚æ•°:
        filename: æ–‡ä»¶è·¯å¾„ï¼ˆå¯èƒ½åŒ…å«å­ç›®å½•ï¼‰
    
    è¿”å›:
        é™æ€æ–‡ä»¶å†…å®¹
    """
    try:
        cache_notes_dir = os.path.join(get_project_root(), 'cache', 'notes')
        logger.debug(f"æä¾›ç¬”è®°æ–‡ä»¶æœåŠ¡: {filename} from {cache_notes_dir}")
        return send_from_directory(cache_notes_dir, filename)
    except Exception as e:
        logger.error(f"æä¾›ç¬”è®°æ–‡ä»¶æœåŠ¡å¤±è´¥: {str(e)}")
        return "æ–‡ä»¶ä¸å­˜åœ¨", 404

@app.route('/api/note-data')
def get_note_data():
    """
    è·å–ç¬”è®°æå–æ•°æ®çš„APIæ¥å£
    æ”¯æŒé€šè¿‡æ–‡ä»¶è·¯å¾„åŠ è½½å·²æå–çš„JSONæ•°æ®
    """
    try:
        file_path = request.args.get('file')
        if not file_path:
            return jsonify({'error': 'ç¼ºå°‘æ–‡ä»¶è·¯å¾„å‚æ•°'}), 400
        
        logger.info(f"è¯·æ±‚ç¬”è®°æ•°æ®: {file_path}")
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶è·¯å¾„åœ¨å…è®¸çš„èŒƒå›´å†…
        cache_dir = os.path.join(get_project_root(), 'cache', 'notes')
        full_path = os.path.join(cache_dir, file_path)
        
        # è§„èŒƒåŒ–è·¯å¾„ï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»
        full_path = os.path.normpath(full_path)
        cache_dir = os.path.normpath(cache_dir)
        
        if not full_path.startswith(cache_dir):
            return jsonify({'error': 'éæ³•çš„æ–‡ä»¶è·¯å¾„'}), 403
        
        # å¦‚æœæ˜¯HTMLæ–‡ä»¶ï¼Œä½¿ç”¨ç»Ÿä¸€æå–å™¨æå–æ•°æ®
        if file_path.endswith('.html'):
            if os.path.exists(full_path):
                logger.info(f"ä½¿ç”¨ç»Ÿä¸€æå–å™¨ä»HTMLæ–‡ä»¶æå–æ•°æ®: {full_path}")
                extracted_data = unified_extractor.extract_from_html_file_hybrid(full_path)
                
                if extracted_data:
                    # å°è¯•ä¿å­˜æå–çš„æ•°æ®
                    try:
                        keyword = "extracted"
                        saved_path = unified_extractor.save_unified_results(extracted_data.get('notes', []), keyword)
                        logger.info(f"å·²ä¿å­˜ç»Ÿä¸€æå–æ•°æ®åˆ°: {saved_path}")
                    except Exception as save_error:
                        logger.warning(f"ä¿å­˜ç»Ÿä¸€æå–æ•°æ®å¤±è´¥: {save_error}")
                    
                    return jsonify(extracted_data)
                else:
                    return jsonify({'error': 'æ— æ³•ä»HTMLæ–‡ä»¶ä¸­æå–æ•°æ®'}), 422
            else:
                return jsonify({'error': 'HTMLæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # å¦‚æœæ˜¯JSONæ–‡ä»¶ï¼Œç›´æ¥è¯»å–
        elif file_path.endswith('.json'):
            if os.path.exists(full_path):
                try:
                    with open(full_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    logger.info(f"æˆåŠŸè¯»å–JSONæ–‡ä»¶: {full_path}")
                    return jsonify(data)
                except json.JSONDecodeError as e:
                    logger.error(f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
                    return jsonify({'error': 'JSONæ–‡ä»¶æ ¼å¼é”™è¯¯'}), 422
            else:
                return jsonify({'error': 'JSONæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        else:
            return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œä»…æ”¯æŒ.htmlå’Œ.jsonæ–‡ä»¶'}), 400
            
    except Exception as e:
        logger.error(f"è·å–ç¬”è®°æ•°æ®å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯'}), 500

@app.route('/note-detail.html')
def note_detail_page():
    """ç¬”è®°è¯¦æƒ…é¡µé¢"""
    return send_from_directory('../../static', 'note_detail.html')

@app.route('/api/unified-extract', methods=['POST'])
def unified_extract():
    """
    ç»Ÿä¸€æ•°æ®æå–API
    æ‰¹é‡å¤„ç†ç¼“å­˜ä¸­çš„HTMLæ–‡ä»¶ï¼Œä½¿ç”¨ç»Ÿä¸€æå–å™¨
    """
    try:
        # è·å–å‚æ•°
        data = request.get_json() or {}
        keyword = data.get('keyword', 'batch_extract')
        max_files = data.get('max_files', None)
        pattern = data.get('pattern', '*.html')
        
        logger.info(f"å¯åŠ¨ç»Ÿä¸€æ•°æ®æå–: keyword={keyword}, max_files={max_files}, pattern={pattern}")
        
        # æ‰§è¡Œæ‰¹é‡æå–
        results = unified_extractor.extract_from_html_files(pattern=pattern, max_files=max_files)
        
        if results:
            # ä¿å­˜ç»Ÿä¸€ç»“æœ
            saved_path = unified_extractor.save_unified_results(results, keyword)
            
            # ç”ŸæˆHTMLé¢„è§ˆ
            html_file = unified_extractor._generate_html_preview(results, keyword)
            
            return jsonify({
                'success': True,
                'count': len(results),
                'notes': results,
                'saved_path': saved_path,
                'html_preview': html_file,
                'keyword': keyword,
                'extraction_info': {
                    'extractor_version': '2.0.0',
                    'processed_at': time.time(),
                    'strategies_used': list(set(note.get('method', 'unknown') for note in results))
                },
                'message': f'æˆåŠŸæå– {len(results)} æ¡ç¬”è®°æ•°æ®'
            })
        else:
            return jsonify({
                'success': False,
                'count': 0,
                'notes': [],
                'message': 'æœªæå–åˆ°ä»»ä½•æ•°æ®'
            })
            
    except Exception as e:
        logger.error(f"ç»Ÿä¸€æ•°æ®æå–å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'ç»Ÿä¸€æ•°æ®æå–å¤±è´¥'
        }), 500

@app.route('/api/config/intelligent-search')
def get_intelligent_search_config():
    """
    è·å–æ™ºèƒ½æœç´¢é…ç½®API
    
    è¿”å›:
        JSONæ ¼å¼çš„æ™ºèƒ½æœç´¢é…ç½®
    """
    try:
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        import os
        import json
        
        config_str = os.environ.get('INTELLIGENT_SEARCH_CONFIG')
        if config_str:
            try:
                config = json.loads(config_str)
                logger.info(f"ğŸ“‹ è¿”å›æ™ºèƒ½æœç´¢é…ç½®: {config}")
                return jsonify({
                    'success': True,
                    'config': config
                })
            except json.JSONDecodeError:
                pass
        
        # é»˜è®¤é…ç½®
        default_config = {
            'enable_cache_search': False,
            'enable_html_extraction': True,
            'enable_realtime_search': False,
            'wait_for_html_save': True,
            'html_save_timeout': 30,
            'extraction_timeout': 60
        }
        
        logger.info(f"ğŸ“‹ è¿”å›é»˜è®¤æ™ºèƒ½æœç´¢é…ç½®: {default_config}")
        return jsonify({
            'success': True,
            'config': default_config
        })
        
    except Exception as e:
        logger.error(f"è·å–æ™ºèƒ½æœç´¢é…ç½®å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'message': 'è·å–æ™ºèƒ½æœç´¢é…ç½®å¤±è´¥'
        }), 500

# ==================== HTMLç»“æœé¡µé¢è·¯ç”± ====================

@app.route('/results/<path:filename>')
def serve_results(filename):
    """
    æä¾›HTMLç»“æœé¡µé¢æœåŠ¡ï¼ˆæ–‡ä»¶å½¢å¼ï¼‰
    ä»æ–‡ä»¶ç³»ç»Ÿæä¾›é¢„ç”Ÿæˆçš„HTMLç»“æœé¡µé¢
    """
    results_dir = os.path.join(get_project_root(), 'cache', 'results')
    logger.debug(f"æ–‡ä»¶æœåŠ¡ç›®å½•: {results_dir}")
    return send_from_directory(results_dir, filename)

@app.route('/api/result-html/<html_hash>')
def get_result_html(html_hash):
    """
    ç›´æ¥è¿”å›HTMLç»“æœé¡µé¢å†…å®¹ï¼ˆAPIå½¢å¼ï¼Œæ¨èï¼‰
    ä¼˜å…ˆä»å†…å­˜ç¼“å­˜è¿”å›ï¼Œé¿å…æ–‡ä»¶è·¯å¾„é—®é¢˜
    
    å‚æ•°:
        html_hash: HTMLå†…å®¹çš„MD5å“ˆå¸Œå€¼
    
    è¿”å›:
        HTMLé¡µé¢å†…å®¹
    """
    global html_results_cache
    
    # ä¼˜å…ˆä»å†…å­˜ç¼“å­˜è·å–
    if html_hash in html_results_cache:
        html_content = html_results_cache[html_hash]
        logger.info(f"ä»å†…å­˜ç¼“å­˜è¿”å›HTMLå†…å®¹: {html_hash}")
        return html_content, 200, {'Content-Type': 'text/html; charset=utf-8'}
    
    # å›é€€ï¼šå°è¯•ä»æ–‡ä»¶è¯»å–
    try:
        html_filename = f"search_{html_hash}.html"
        results_dir = os.path.join(get_project_root(), 'cache', 'results')
        html_path = os.path.join(results_dir, html_filename)
        
        if os.path.exists(html_path):
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            # å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜ä¸­
            html_results_cache[html_hash] = html_content
            logger.info(f"ä»æ–‡ä»¶è¯»å–å¹¶ç¼“å­˜HTMLå†…å®¹: {html_path}")
            return html_content, 200, {'Content-Type': 'text/html; charset=utf-8'}
        else:
            logger.warning(f"HTMLæ–‡ä»¶ä¸å­˜åœ¨: {html_path}")
            return jsonify({"error": "HTMLç»“æœé¡µé¢ä¸å­˜åœ¨"}), 404
    except Exception as e:
        logger.error(f"è¯»å–HTMLæ–‡ä»¶å¤±è´¥: {str(e)}")
        return jsonify({"error": "æ— æ³•è¯»å–HTMLæ–‡ä»¶"}), 500



# ==================== é”™è¯¯å¤„ç† ====================

@app.errorhandler(404)
def not_found(e):
    """å¤„ç†404é”™è¯¯"""
    return jsonify({"error": "èµ„æºä¸å­˜åœ¨"}), 404

@app.errorhandler(500)
def server_error(e):
    """å¤„ç†500é”™è¯¯"""
    return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500

# ==================== æ¸…ç†å‡½æ•° ====================

def cleanup():
    """åº”ç”¨é€€å‡ºæ—¶çš„æ¸…ç†å·¥ä½œ"""
    global crawler
    if crawler:
        crawler.close()
        crawler = None

# ==================== ä¸»ç¨‹åºå…¥å£ ====================

if __name__ == '__main__':
    import atexit
    
    try:
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        project_root = get_project_root()
        essential_dirs = [
            'cache/cookies',
            'cache/temp', 
            'cache/logs',
            'cache/results',
            'static/images'
        ]
        for dir_path in essential_dirs:
            full_path = os.path.join(project_root, dir_path)
            os.makedirs(full_path, exist_ok=True)
        
        # æ³¨å†Œæ¸…ç†å‡½æ•°
        atexit.register(cleanup)
        
        logger.info("å°çº¢ä¹¦æœç´¢æœåŠ¡å¯åŠ¨ä¸­...")
        logger.info("è®¿é—®åœ°å€: http://localhost:8080")
        logger.info("å¦‚éœ€ç™»å½•ï¼Œè¯·è®¿é—®: http://localhost:8080/login")
        
        # å¯åŠ¨æœåŠ¡
        app.run(debug=False, host='0.0.0.0', port=8080)
    except KeyboardInterrupt:
        logger.info("æœåŠ¡å·²åœæ­¢")
        cleanup()
    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        cleanup() 