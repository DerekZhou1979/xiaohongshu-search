#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å°çº¢ä¹¦æœç´¢æœåŠ¡å™¨æ¨¡å—
æä¾›Web APIå’Œé™æ€æ–‡ä»¶æœåŠ¡ï¼Œæ”¯æŒæœç´¢ã€ç»“æœå±•ç¤ºç­‰åŠŸèƒ½

ä¸»è¦åŠŸèƒ½ï¼š
1. Web APIæœåŠ¡ - æœç´¢ã€ç¬”è®°è¯¦æƒ…ã€çƒ­é—¨å…³é”®è¯
2. é™æ€æ–‡ä»¶æœåŠ¡ - HTMLã€CSSã€JSã€å›¾ç‰‡
3. HTMLç»“æœé¡µé¢æœåŠ¡ - æ–‡ä»¶å½¢å¼å’ŒAPIå½¢å¼
4. ç”¨æˆ·ç™»å½•æ”¯æŒ
"""

import sys
import os
import logging
import time
import hashlib
import traceback
import json
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from flask import Flask, request, jsonify, send_from_directory, redirect, url_for
from flask_cors import CORS
from src.crawler.xiaohongshu_crawler import XiaoHongShuCrawler

# ==================== é…ç½®å’Œåˆå§‹åŒ– ====================

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__, static_folder='../../static')
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# ==================== å…¨å±€å˜é‡ ====================

# Cookieæ–‡ä»¶è·¯å¾„
COOKIES_FILE = os.path.join('cache', 'cookies', 'xiaohongshu_cookies.json')

# å…¨å±€çˆ¬è™«å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
crawler = None

# HTMLç»“æœå†…å­˜ç¼“å­˜ï¼ˆé¿å…æ–‡ä»¶è·¯å¾„é—®é¢˜ï¼‰
html_results_cache = {}

# Debugä¿¡æ¯å­˜å‚¨
debug_info_store = {}

# ==================== å·¥å…·å‡½æ•° ====================

def store_html_result(html_hash, html_content):
    """
    å­˜å‚¨HTMLç»“æœåˆ°å†…å­˜ç¼“å­˜
    
    Args:
        html_hash: HTMLå†…å®¹çš„MD5å“ˆå¸Œå€¼
        html_content: HTMLå†…å®¹
    """
    global html_results_cache
    html_results_cache[html_hash] = html_content
    logger.info(f"HTMLå†…å®¹å·²å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜: {html_hash}")

def init_crawler():
    """
    å»¶è¿Ÿåˆå§‹åŒ–çˆ¬è™«å®ä¾‹
    åªåœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–ï¼Œé¿å…å¯åŠ¨æ—¶çš„æ€§èƒ½å¼€é”€
    
    Returns:
        bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
    """
    global crawler
    if crawler is None:
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ–å°çº¢ä¹¦çˆ¬è™«...")
            crawler = XiaoHongShuCrawler(
                use_selenium=True, 
                headless=True, 
                cookies_file=COOKIES_FILE
            )
            # è®¾ç½®HTMLå­˜å‚¨å›è°ƒå‡½æ•°
            crawler.set_html_callback(store_html_result)
            logger.info("å°çº¢ä¹¦çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"å°çº¢ä¹¦çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            crawler = None
            return False
    return True

def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•è·¯å¾„"""
    return os.path.dirname(os.path.dirname(os.path.dirname(__file__)))

def create_login_required_page(decoded_url):
    """åˆ›å»ºéœ€è¦ç™»å½•çš„é¡µé¢"""
    return '''
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>éœ€è¦ç™»å½•</title>
        <style>
            body { 
                font-family: -apple-system, BlinkMacSystemFont, sans-serif; 
                text-align: center; 
                padding: 50px; 
                background: #f8f9fa;
            }
            .container { 
                max-width: 600px; 
                margin: 0 auto; 
                background: white; 
                padding: 40px; 
                border-radius: 10px; 
                box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            }
            .warning { color: #ff9800; font-size: 18px; margin-bottom: 20px; }
            .info { color: #666; margin-bottom: 20px; line-height: 1.6; }
            .url-info { 
                background: #f5f5f5; 
                padding: 15px; 
                border-radius: 5px; 
                margin: 20px 0; 
                word-break: break-all; 
                font-family: monospace;
                font-size: 12px;
            }
            .back-btn {
                display: inline-block;
                background: #ff6b6b;
                color: white;
                padding: 10px 20px;
                border-radius: 5px;
                text-decoration: none;
                margin-top: 20px;
            }
            .back-btn:hover { background: #ff5252; }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="warning">ğŸ” è¯¥ç¬”è®°éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®</div>
            <div class="info">
                xsec_tokenå’Œxsec_sourceå‚æ•°å·²æ·»åŠ ï¼Œä½†è¯¥ç¬”è®°ä»éœ€è¦æœ‰æ•ˆçš„ç™»å½•çŠ¶æ€ã€‚<br>
                è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š<br>
                â€¢ ç¬”è®°è®¾ç½®ä¸ºç§å¯†æˆ–ä»…å¥½å‹å¯è§<br>
                â€¢ éœ€è¦æ›´æ–°çš„ç™»å½•å‡­è¯<br>
                â€¢ ç¬”è®°å·²è¢«åˆ é™¤æˆ–é™åˆ¶è®¿é—®
            </div>
            <div class="url-info">è®¿é—®URL: ''' + decoded_url + '''</div>
            <a href="javascript:history.back()" class="back-btn">â† è¿”å›æœç´¢ç»“æœ</a>
        </div>
    </body>
    </html>
    ''', 200, {'Content-Type': 'text/html; charset=utf-8'}

def fix_proxy_content(content, original_url):
    """ä¿®å¤ä»£ç†å†…å®¹ä¸­çš„é“¾æ¥å’Œèµ„æºå¼•ç”¨"""
    try:
        import re
        from urllib.parse import urljoin, urlparse
        
        # è·å–åŸºç¡€URL
        parsed_url = urlparse(original_url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        
        # ä¿®å¤ç›¸å¯¹é“¾æ¥
        content = re.sub(r'href="(/[^"]*)"', rf'href="{base_url}\1"', content)
        content = re.sub(r"href='(/[^']*)'", rf"href='{base_url}\1'", content)
        
        # ä¿®å¤ç›¸å¯¹èµ„æºé“¾æ¥ï¼ˆCSS, JS, å›¾ç‰‡ç­‰ï¼‰
        content = re.sub(r'src="(/[^"]*)"', rf'src="{base_url}\1"', content)
        content = re.sub(r"src='(/[^']*)'", rf"src='{base_url}\1'", content)
        
        # ä¿®å¤CSSä¸­çš„ç›¸å¯¹é“¾æ¥
        content = re.sub(r'url\((/[^)]*)\)', rf'url({base_url}\1)', content)
        content = re.sub(r'url\("(/[^"]*)"\)', rf'url("{base_url}\1")', content)
        content = re.sub(r"url\('(/[^']*)'\)", rf"url('{base_url}\1')", content)
        
        # æ·»åŠ ä»£ç†æç¤ºæ ·å¼å’Œå®‰å…¨ç­–ç•¥
        if '<head>' in content:
            style_injection = '''
            <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
            <meta http-equiv="X-Content-Type-Options" content="nosniff">
            <style>
                .proxy-notice { 
                    background: linear-gradient(45deg, #ff6b6b, #ff8e8e);
                    color: white; 
                    padding: 12px; 
                    text-align: center; 
                    position: fixed; 
                    top: 0; 
                    left: 0; 
                    right: 0; 
                    z-index: 9999;
                    font-size: 14px;
                    font-weight: 500;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.2);
                }
                body { padding-top: 50px !important; }
                .proxy-notice a {
                    color: white;
                    text-decoration: underline;
                    margin-left: 10px;
                }
            </style>
            '''
            content = content.replace('<head>', f'<head>{style_injection}')
        
        # æ·»åŠ ä»£ç†æç¤º
        if '<body>' in content:
            proxy_notice = '''
            <div class="proxy-notice">
                ğŸ”— é€šè¿‡è±«å›­è‚¡ä»½ä»£ç†æœåŠ¡è®¿é—® - å·²è‡ªåŠ¨å¤„ç†ç™»å½•è®¤è¯
                <a href="javascript:history.back()">â† è¿”å›æœç´¢ç»“æœ</a>
            </div>
            '''
            content = content.replace('<body>', f'<body>{proxy_notice}')
        
        # ä¿®å¤å¯èƒ½çš„åè®®é—®é¢˜
        content = content.replace('http://www.xiaohongshu.com', 'https://www.xiaohongshu.com')
        
        return content
        
    except Exception as e:
        logger.error(f"ä¿®å¤ä»£ç†å†…å®¹å¤±è´¥: {str(e)}")
        return content

def store_debug_info(session_id, message, level="INFO"):
    """
    å­˜å‚¨debugä¿¡æ¯
    
    Args:
        session_id: ä¼šè¯ID
        message: debugæ¶ˆæ¯
        level: æ—¥å¿—çº§åˆ«
    """
    if session_id not in debug_info_store:
        debug_info_store[session_id] = []
    
    debug_info_store[session_id].append({
        'timestamp': time.time(),
        'level': level,
        'message': message,
        'time_str': time.strftime('%H:%M:%S', time.localtime())
    })
    
    # é™åˆ¶æ¯ä¸ªä¼šè¯æœ€å¤šå­˜å‚¨100æ¡debugä¿¡æ¯
    if len(debug_info_store[session_id]) > 100:
        debug_info_store[session_id] = debug_info_store[session_id][-100:]

# ==================== é™æ€æ–‡ä»¶è·¯ç”± ====================

@app.route('/')
def index():
    """ä¸»é¡µ - è¿”å›æœç´¢ç•Œé¢"""
    return send_from_directory('../../static', 'index.html')

@app.route('/css/<path:path>')
def serve_css(path):
    """æä¾›CSSæ–‡ä»¶æœåŠ¡"""
    return send_from_directory('../../static/css', path)

@app.route('/js/<path:path>')
def serve_js(path):
    """æä¾›JavaScriptæ–‡ä»¶æœåŠ¡"""
    return send_from_directory('../../static/js', path)

@app.route('/img/<path:path>')
def serve_img(path):
    """æä¾›å›¾ç‰‡æ–‡ä»¶æœåŠ¡"""
    return send_from_directory('../../static/images', path)

# ==================== ç”¨æˆ·ç™»å½•è·¯ç”± ====================

@app.route('/login')
def login():
    """
    ç”¨æˆ·ç™»å½•é¡µé¢
    æ‰“å¼€æµè§ˆå™¨è®©ç”¨æˆ·æ‰‹åŠ¨ç™»å½•å°çº¢ä¹¦ï¼Œè·å–cookie
    """
    try:
        # åˆ›å»ºä¸“é—¨ç”¨äºç™»å½•çš„çˆ¬è™«å®ä¾‹ï¼ˆéæ— å¤´æ¨¡å¼ï¼‰
        login_crawler = XiaoHongShuCrawler(use_selenium=True, headless=False)
        success = login_crawler.login()
        login_crawler.close()
        
        if success:
            # ç™»å½•æˆåŠŸåé‡ç½®ä¸»çˆ¬è™«å®ä¾‹ï¼Œä»¥ä½¿ç”¨æ–°çš„cookie
            global crawler
            crawler = None
            return redirect(url_for('index'))
        else:
            return jsonify({"error": "ç™»å½•å¤±è´¥ï¼Œè¯·é‡è¯•"}), 500
    except Exception as e:
        logger.error(f"ç™»å½•è¿‡ç¨‹å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({"error": f"ç™»å½•è¿‡ç¨‹å‡ºé”™: {str(e)}"}), 500

# ==================== APIè·¯ç”± ====================

@app.route('/api/search')
def search():
    """
    æœç´¢API
    æ ¹æ®å…³é”®è¯æœç´¢å°çº¢ä¹¦ç¬”è®°
    
    å‚æ•°:
        keyword: æœç´¢å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰
        max_results: æœ€å¤§ç»“æœæ•°é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤21ï¼‰
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆå¯é€‰ï¼Œé»˜è®¤trueï¼‰
        session_id: ä¼šè¯IDï¼ˆå¯é€‰ï¼Œç”¨äºdebugä¿¡æ¯ï¼‰
    
    è¿”å›:
        JSONæ ¼å¼çš„æœç´¢ç»“æœï¼ŒåŒ…å«ç¬”è®°åˆ—è¡¨å’ŒHTMLé¡µé¢URL
    """
    # åˆå§‹åŒ–çˆ¬è™«
    if not init_crawler():
        return jsonify({"error": "çˆ¬è™«åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒChromeæµè§ˆå™¨"}), 500
        
    # è·å–å‚æ•°
    keyword = request.args.get('keyword', '').strip()
    session_id = request.args.get('session_id', f"search_{int(time.time())}")
    
    if not keyword:
        return jsonify({"error": "ç¼ºå°‘å…³é”®è¯å‚æ•°"}), 400
    
    try:
        # è§£æå‚æ•°
        max_results = int(request.args.get('max_results', 21))
        use_cache = request.args.get('use_cache', 'true').lower() == 'true'
        
        # è®°å½•å¼€å§‹æœç´¢
        store_debug_info(session_id, f"ğŸ” å¼€å§‹æœç´¢å…³é”®è¯: {keyword}", "INFO")
        store_debug_info(session_id, f"ğŸ“Š æœ€å¤§ç»“æœæ•°: {max_results}, ä½¿ç”¨ç¼“å­˜: {use_cache}", "INFO")
        
        # è®¾ç½®çˆ¬è™«çš„debugå›è°ƒ
        def debug_callback(message, level="INFO"):
            store_debug_info(session_id, message, level)
        
        # å¦‚æœçˆ¬è™«æ”¯æŒdebugå›è°ƒï¼Œè®¾ç½®å®ƒ
        if hasattr(crawler, 'set_debug_callback'):
            crawler.set_debug_callback(debug_callback)
        
        # æ‰§è¡Œæœç´¢
        store_debug_info(session_id, "ğŸš€ æ­£åœ¨æ‰§è¡Œæœç´¢...", "INFO")
        search_results = crawler.search(keyword, max_results=max_results, use_cache=use_cache)
        
        # è§„èŒƒåŒ–æœç´¢ç»“æœæ ¼å¼
        if isinstance(search_results, dict) and 'data' in search_results:
            notes = search_results['data']
        else:
            notes = search_results if isinstance(search_results, list) else []
        
        store_debug_info(session_id, f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(notes)} æ¡ç¬”è®°", "INFO")
        
        # ç”ŸæˆHTMLé¡µé¢URL
        html_hash = hashlib.md5(keyword.encode()).hexdigest()
        html_url = f"/results/search_{html_hash}.html"           # æ–‡ä»¶å½¢å¼
        html_api_url = f"/api/result-html/{html_hash}"           # APIå½¢å¼ï¼ˆæ¨èï¼‰
        
        store_debug_info(session_id, f"ğŸ“„ ç”ŸæˆHTMLé¡µé¢: {html_api_url}", "INFO")
        
        return jsonify({
            "keyword": keyword,
            "session_id": session_id,
            "timestamp": int(time.time()),
            "count": len(notes),
            "notes": notes,
            "html_url": html_url,
            "html_api_url": html_api_url
        })
    except Exception as e:
        logger.error(f"æœç´¢å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        store_debug_info(session_id, f"âŒ æœç´¢å¤±è´¥: {str(e)}", "ERROR")
        return jsonify({"error": "æœç´¢å¤±è´¥", "message": str(e), "session_id": session_id}), 500

@app.route('/api/note/<note_id>')
def get_note(note_id):
    """
    è·å–ç¬”è®°è¯¦æƒ…API
    
    å‚æ•°:
        note_id: ç¬”è®°ID
    
    è¿”å›:
        JSONæ ¼å¼çš„ç¬”è®°è¯¦æƒ…
    """
    if not init_crawler():
        return jsonify({"error": "çˆ¬è™«åˆå§‹åŒ–å¤±è´¥"}), 500
        
    if not note_id:
        return jsonify({"error": "ç¼ºå°‘ç¬”è®°IDå‚æ•°"}), 400
    
    try:
        note = crawler.get_note_detail(note_id)
        
        if note:
            return jsonify({"note": note})
        else:
            return jsonify({"error": "æœªæ‰¾åˆ°è¯¥ç¬”è®°"}), 404
    except Exception as e:
        logger.error(f"è·å–ç¬”è®°è¯¦æƒ…å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({"error": "è·å–ç¬”è®°è¯¦æƒ…å¤±è´¥", "message": str(e)}), 500

@app.route('/api/hot-keywords')
def hot_keywords():
    """
    è·å–çƒ­é—¨å…³é”®è¯API
    
    è¿”å›:
        JSONæ ¼å¼çš„çƒ­é—¨å…³é”®è¯åˆ—è¡¨
    """
    if not init_crawler():
        return jsonify({"error": "çˆ¬è™«åˆå§‹åŒ–å¤±è´¥"}), 500
        
    try:
        keywords = crawler.get_hot_keywords()
        return jsonify({"keywords": keywords})
    except Exception as e:
        logger.error(f"è·å–çƒ­é—¨å…³é”®è¯å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({"error": "è·å–çƒ­é—¨å…³é”®è¯å¤±è´¥", "message": str(e)}), 500

@app.route('/api/debug/<session_id>')
def get_debug_info(session_id):
    """
    è·å–debugä¿¡æ¯API
    
    å‚æ•°:
        session_id: ä¼šè¯ID
        since: è·å–æŒ‡å®šæ—¶é—´æˆ³ä¹‹åçš„ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    
    è¿”å›:
        JSONæ ¼å¼çš„debugä¿¡æ¯åˆ—è¡¨
    """
    since = request.args.get('since', type=float, default=0)
    
    if session_id not in debug_info_store:
        return jsonify({"debug_info": []})
    
    debug_info = debug_info_store[session_id]
    
    # å¦‚æœæŒ‡å®šäº†sinceå‚æ•°ï¼Œåªè¿”å›è¯¥æ—¶é—´æˆ³ä¹‹åçš„ä¿¡æ¯
    if since > 0:
        debug_info = [info for info in debug_info if info['timestamp'] > since]
    
    return jsonify({
        "session_id": session_id,
        "debug_info": debug_info,
        "last_timestamp": debug_info[-1]['timestamp'] if debug_info else 0
    })

# ==================== HTMLç»“æœé¡µé¢è·¯ç”± ====================

@app.route('/results/<path:filename>')
def serve_results(filename):
    """
    æä¾›HTMLç»“æœé¡µé¢æœåŠ¡ï¼ˆæ–‡ä»¶å½¢å¼ï¼‰
    ä»æ–‡ä»¶ç³»ç»Ÿæä¾›é¢„ç”Ÿæˆçš„HTMLç»“æœé¡µé¢
    """
    results_dir = os.path.join(get_project_root(), 'cache', 'results')
    logger.debug(f"æ–‡ä»¶æœåŠ¡ç›®å½•: {results_dir}")
    return send_from_directory(results_dir, filename)

@app.route('/api/result-html/<html_hash>')
def get_result_html(html_hash):
    """
    ç›´æ¥è¿”å›HTMLç»“æœé¡µé¢å†…å®¹ï¼ˆAPIå½¢å¼ï¼Œæ¨èï¼‰
    ä¼˜å…ˆä»å†…å­˜ç¼“å­˜è¿”å›ï¼Œé¿å…æ–‡ä»¶è·¯å¾„é—®é¢˜
    
    å‚æ•°:
        html_hash: HTMLå†…å®¹çš„MD5å“ˆå¸Œå€¼
    
    è¿”å›:
        HTMLé¡µé¢å†…å®¹
    """
    global html_results_cache
    
    # ä¼˜å…ˆä»å†…å­˜ç¼“å­˜è·å–
    if html_hash in html_results_cache:
        html_content = html_results_cache[html_hash]
        logger.info(f"ä»å†…å­˜ç¼“å­˜è¿”å›HTMLå†…å®¹: {html_hash}")
        return html_content, 200, {'Content-Type': 'text/html; charset=utf-8'}
    
    # å›é€€ï¼šå°è¯•ä»æ–‡ä»¶è¯»å–
    try:
        html_filename = f"search_{html_hash}.html"
        results_dir = os.path.join(get_project_root(), 'cache', 'results')
        html_path = os.path.join(results_dir, html_filename)
        
        if os.path.exists(html_path):
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            # å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜ä¸­
            html_results_cache[html_hash] = html_content
            logger.info(f"ä»æ–‡ä»¶è¯»å–å¹¶ç¼“å­˜HTMLå†…å®¹: {html_path}")
            return html_content, 200, {'Content-Type': 'text/html; charset=utf-8'}
        else:
            logger.warning(f"HTMLæ–‡ä»¶ä¸å­˜åœ¨: {html_path}")
            return jsonify({"error": "HTMLç»“æœé¡µé¢ä¸å­˜åœ¨"}), 404
    except Exception as e:
        logger.error(f"è¯»å–HTMLæ–‡ä»¶å¤±è´¥: {str(e)}")
        return jsonify({"error": "æ— æ³•è¯»å–HTMLæ–‡ä»¶"}), 500

# ==================== ä»£ç†è·¯ç”± ====================

@app.route('/proxy/note/<path:note_url>')
def proxy_note(note_url):
    """
    ä»£ç†å°çº¢ä¹¦ç¬”è®°é“¾æ¥ï¼Œä½¿ç”¨cookiesè¿›è¡Œè®¤è¯
    
    Args:
        note_url: å°çº¢ä¹¦ç¬”è®°URLï¼ˆå·²ç¼–ç ï¼‰
    """
    try:
        import urllib.parse
        import requests
        
        # è§£ç URL
        decoded_url = urllib.parse.unquote(note_url)
        
        # ç¡®ä¿URLæ˜¯å°çº¢ä¹¦çš„é“¾æ¥
        if not decoded_url.startswith('https://www.xiaohongshu.com/'):
            return jsonify({"error": "æ— æ•ˆçš„é“¾æ¥"}), 400
        
        # åŠ è½½cookies
        cookies_dict = {}
        if os.path.exists(COOKIES_FILE):
            try:
                with open(COOKIES_FILE, 'r', encoding='utf-8') as f:
                    cookies_list = json.load(f)
                for cookie in cookies_list:
                    cookies_dict[cookie['name']] = cookie['value']
            except Exception as e:
                logger.warning(f"åŠ è½½cookieså¤±è´¥: {str(e)}")
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.xiaohongshu.com/',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # è®°å½•è®¿é—®æ—¥å¿—
        logger.info(f"ä»£ç†è®¿é—®URL: {decoded_url}")
        
        # å‘èµ·è¯·æ±‚ï¼Œç¦ç”¨SSLéªŒè¯ä»¥é¿å…è¯ä¹¦é—®é¢˜
        response = requests.get(decoded_url, cookies=cookies_dict, headers=headers, timeout=15, verify=False)
        
        # è®°å½•å“åº”çŠ¶æ€
        logger.info(f"ä»£ç†å“åº”çŠ¶æ€: {response.status_code}")
        
        if response.status_code == 200:
            # éªŒè¯æ˜¯å¦æˆåŠŸè®¿é—®ï¼ˆæ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•ï¼‰
            content_preview = response.text[:1000].lower()
            if 'ç™»å½•' in content_preview or 'login' in content_preview or 'éªŒè¯' in content_preview:
                logger.warning(f"å¯èƒ½éœ€è¦ç™»å½•è®¤è¯: {decoded_url}")
                return create_login_required_page(decoded_url)
            
            logger.info(f"æˆåŠŸè®¿é—®ç¬”è®°: {decoded_url}")
            
            # å¤„ç†å“åº”å†…å®¹
            content = response.text
            
            # ä¿®å¤å†…å®¹ä¸­çš„å„ç§é“¾æ¥å’Œèµ„æºå¼•ç”¨
            content = fix_proxy_content(content, decoded_url)
            
            # è®¾ç½®æ­£ç¡®çš„å“åº”å¤´
            response_headers = {
                'Content-Type': 'text/html; charset=utf-8',
                'Cache-Control': 'no-cache, no-store, must-revalidate',
                'Pragma': 'no-cache',
                'Expires': '0',
                'X-Frame-Options': 'SAMEORIGIN',
                'X-Content-Type-Options': 'nosniff',
                'Referrer-Policy': 'strict-origin-when-cross-origin',
                'Content-Security-Policy': "default-src 'self' 'unsafe-inline' 'unsafe-eval' data: blob: https: wss:; img-src 'self' data: blob: https: http:; media-src 'self' data: blob: https: http:; connect-src 'self' https: wss:; font-src 'self' data: https:; style-src 'self' 'unsafe-inline' https:; script-src 'self' 'unsafe-inline' 'unsafe-eval' https:; upgrade-insecure-requests;"
            }
            
            return content, 200, response_headers
        else:
            logger.warning(f"ä»£ç†è¯·æ±‚å¤±è´¥: {response.status_code}")
            return '''
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="UTF-8">
                <title>è®¿é—®å¤±è´¥</title>
                <style>
                    body { font-family: -apple-system, BlinkMacSystemFont, sans-serif; text-align: center; padding: 50px; }
                    .error { color: #ff6b6b; font-size: 18px; margin-bottom: 20px; }
                    .retry { color: #666; }
                </style>
            </head>
            <body>
                <div class="error">âš ï¸ æ— æ³•è®¿é—®è¯¥ç¬”è®°</div>
                <div class="retry">å¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜æˆ–ç¬”è®°å·²è¢«åˆ é™¤</div>
                <div style="margin-top: 20px;">
                    <a href="javascript:history.back()" style="color: #ff6b6b; text-decoration: none;">â† è¿”å›æœç´¢ç»“æœ</a>
                </div>
            </body>
            </html>
            ''', 200, {'Content-Type': 'text/html; charset=utf-8'}
            
    except Exception as e:
        logger.error(f"ä»£ç†ç¬”è®°é“¾æ¥å¤±è´¥: {str(e)}")
        return jsonify({"error": f"ä»£ç†å¤±è´¥: {str(e)}"}), 500

# ==================== é”™è¯¯å¤„ç† ====================

@app.errorhandler(404)
def not_found(e):
    """å¤„ç†404é”™è¯¯"""
    return jsonify({"error": "èµ„æºä¸å­˜åœ¨"}), 404

@app.errorhandler(500)
def server_error(e):
    """å¤„ç†500é”™è¯¯"""
    return jsonify({"error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"}), 500

# ==================== æ¸…ç†å‡½æ•° ====================

def cleanup():
    """åº”ç”¨é€€å‡ºæ—¶çš„æ¸…ç†å·¥ä½œ"""
    global crawler
    if crawler:
        crawler.close()
        crawler = None

# ==================== ä¸»ç¨‹åºå…¥å£ ====================

if __name__ == '__main__':
    import atexit
    
    try:
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        project_root = get_project_root()
        essential_dirs = [
            'cache/cookies',
            'cache/temp', 
            'cache/logs',
            'cache/results',
            'static/images'
        ]
        for dir_path in essential_dirs:
            full_path = os.path.join(project_root, dir_path)
            os.makedirs(full_path, exist_ok=True)
        
        # æ³¨å†Œæ¸…ç†å‡½æ•°
        atexit.register(cleanup)
        
        logger.info("å°çº¢ä¹¦æœç´¢æœåŠ¡å¯åŠ¨ä¸­...")
        logger.info("è®¿é—®åœ°å€: http://localhost:8080")
        logger.info("å¦‚éœ€ç™»å½•ï¼Œè¯·è®¿é—®: http://localhost:8080/login")
        
        # å¯åŠ¨æœåŠ¡
        app.run(debug=False, host='0.0.0.0', port=8080)
    except KeyboardInterrupt:
        logger.info("æœåŠ¡å·²åœæ­¢")
        cleanup()
    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        cleanup() 