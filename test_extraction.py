#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æµ‹è¯•æ”¹è¿›çš„æ•°æ®æå–åŠŸèƒ½
"""

import sys
import os
import json
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(__file__))

from src.crawler.xiaohongshu_crawler import XiaoHongShuCrawler

def test_search_with_improved_extraction():
    """æµ‹è¯•æ”¹è¿›çš„æ•°æ®æå–åŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ”¹è¿›çš„æ•°æ®æå–åŠŸèƒ½...")
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = XiaoHongShuCrawler(use_selenium=True, headless=False)
    
    try:
        # æœç´¢å…³é”®è¯
        keyword = "æŠ¤è‚¤å“"
        print(f"ğŸ” æœç´¢å…³é”®è¯: {keyword}")
        
        # æ‰§è¡Œæœç´¢
        results = crawler.search(keyword, max_results=5, use_cache=False)
        
        if results:
            print(f"âœ… æˆåŠŸè·å– {len(results)} æ¡ç»“æœ")
            
            # æ˜¾ç¤ºç»“æœ
            for i, note in enumerate(results, 1):
                print(f"\nğŸ“ ç¬”è®° {i}:")
                print(f"   ID: {note['id']}")
                print(f"   æ ‡é¢˜: {note['title'][:50]}...")
                print(f"   æè¿°: {note['desc'][:80]}...")
                print(f"   ä½œè€…: {note['author']}")
                print(f"   å°é¢: {note['cover'][:50]}...")
                print(f"   URL: {note['url'][:50]}...")
                print(f"   äº’åŠ¨æ•°æ®: ğŸ‘{note['likes']} ğŸ’¬{note['comments']} â­{note['collects']} ğŸ”„{note['shares']}")
        else:
            print("âŒ æ²¡æœ‰è·å–åˆ°ç»“æœ")
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
    
    finally:
        # å…³é—­çˆ¬è™«
        crawler.close()
        print("ğŸ”š æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_search_with_improved_extraction() 